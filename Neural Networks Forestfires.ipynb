{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "05253720",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/SAURABHMASLEKAR/xyz/blob/main/neural_networks_forestfires_csv.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5ea296e8",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5ea296e8",
    "outputId": "648d4b78-1e18-4229-f16e-8bce594cf9a4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: keras in /usr/local/lib/python3.7/dist-packages (2.4.3)\n",
      "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.7/dist-packages (from keras) (1.19.5)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from keras) (3.13)\n",
      "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.7/dist-packages (from keras) (1.4.1)\n",
      "Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from keras) (2.10.0)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from h5py->keras) (1.15.0)\n",
      "Requirement already satisfied: tensorflow in /usr/local/lib/python3.7/dist-packages (2.4.1)\n",
      "Requirement already satisfied: wrapt~=1.12.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.12.1)\n",
      "Requirement already satisfied: six~=1.15.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.15.0)\n",
      "Requirement already satisfied: termcolor~=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.1.0)\n",
      "Requirement already satisfied: gast==0.3.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (0.3.3)\n",
      "Requirement already satisfied: grpcio~=1.32.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.32.0)\n",
      "Requirement already satisfied: astunparse~=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: numpy~=1.19.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.19.5)\n",
      "Requirement already satisfied: flatbuffers~=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.12)\n",
      "Requirement already satisfied: tensorflow-estimator<2.5.0,>=2.4.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (2.4.0)\n",
      "Requirement already satisfied: opt-einsum~=3.3.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (3.3.0)\n",
      "Requirement already satisfied: absl-py~=0.10 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (0.12.0)\n",
      "Requirement already satisfied: typing-extensions~=3.7.4 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (3.7.4.3)\n",
      "Requirement already satisfied: wheel~=0.35 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (0.36.2)\n",
      "Requirement already satisfied: tensorboard~=2.4 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (2.4.1)\n",
      "Requirement already satisfied: google-pasta~=0.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (3.12.4)\n",
      "Requirement already satisfied: h5py~=2.10.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (2.10.0)\n",
      "Requirement already satisfied: keras-preprocessing~=1.1.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.1.2)\n",
      "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow) (1.30.0)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow) (0.4.4)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow) (3.3.4)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow) (2.0.0)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow) (1.8.0)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow) (56.1.0)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow) (2.23.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow) (0.2.8)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3.6\" in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow) (4.7.2)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow) (4.2.2)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.4->tensorflow) (1.3.0)\n",
      "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard~=2.4->tensorflow) (4.0.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow) (2020.12.5)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow) (3.0.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow) (2.10)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow) (1.24.3)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.4->tensorflow) (3.1.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard~=2.4->tensorflow) (3.4.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install keras\n",
    "!pip install tensorflow\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e91a0b51",
   "metadata": {
    "id": "e91a0b51"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1e7b11d0",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 439
    },
    "id": "1e7b11d0",
    "outputId": "a25e2da9-60e0-4bce-c4f3-ee460bffb382"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>month</th>\n",
       "      <th>day</th>\n",
       "      <th>FFMC</th>\n",
       "      <th>DMC</th>\n",
       "      <th>DC</th>\n",
       "      <th>ISI</th>\n",
       "      <th>temp</th>\n",
       "      <th>RH</th>\n",
       "      <th>wind</th>\n",
       "      <th>rain</th>\n",
       "      <th>area</th>\n",
       "      <th>dayfri</th>\n",
       "      <th>daymon</th>\n",
       "      <th>daysat</th>\n",
       "      <th>daysun</th>\n",
       "      <th>daythu</th>\n",
       "      <th>daytue</th>\n",
       "      <th>daywed</th>\n",
       "      <th>monthapr</th>\n",
       "      <th>monthaug</th>\n",
       "      <th>monthdec</th>\n",
       "      <th>monthfeb</th>\n",
       "      <th>monthjan</th>\n",
       "      <th>monthjul</th>\n",
       "      <th>monthjun</th>\n",
       "      <th>monthmar</th>\n",
       "      <th>monthmay</th>\n",
       "      <th>monthnov</th>\n",
       "      <th>monthoct</th>\n",
       "      <th>monthsep</th>\n",
       "      <th>size_category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>mar</td>\n",
       "      <td>fri</td>\n",
       "      <td>86.2</td>\n",
       "      <td>26.2</td>\n",
       "      <td>94.3</td>\n",
       "      <td>5.1</td>\n",
       "      <td>8.2</td>\n",
       "      <td>51</td>\n",
       "      <td>6.7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>small</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>oct</td>\n",
       "      <td>tue</td>\n",
       "      <td>90.6</td>\n",
       "      <td>35.4</td>\n",
       "      <td>669.1</td>\n",
       "      <td>6.7</td>\n",
       "      <td>18.0</td>\n",
       "      <td>33</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>small</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>oct</td>\n",
       "      <td>sat</td>\n",
       "      <td>90.6</td>\n",
       "      <td>43.7</td>\n",
       "      <td>686.9</td>\n",
       "      <td>6.7</td>\n",
       "      <td>14.6</td>\n",
       "      <td>33</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>small</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>mar</td>\n",
       "      <td>fri</td>\n",
       "      <td>91.7</td>\n",
       "      <td>33.3</td>\n",
       "      <td>77.5</td>\n",
       "      <td>9.0</td>\n",
       "      <td>8.3</td>\n",
       "      <td>97</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>small</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>mar</td>\n",
       "      <td>sun</td>\n",
       "      <td>89.3</td>\n",
       "      <td>51.3</td>\n",
       "      <td>102.2</td>\n",
       "      <td>9.6</td>\n",
       "      <td>11.4</td>\n",
       "      <td>99</td>\n",
       "      <td>1.8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>small</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>512</th>\n",
       "      <td>aug</td>\n",
       "      <td>sun</td>\n",
       "      <td>81.6</td>\n",
       "      <td>56.7</td>\n",
       "      <td>665.6</td>\n",
       "      <td>1.9</td>\n",
       "      <td>27.8</td>\n",
       "      <td>32</td>\n",
       "      <td>2.7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.44</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>large</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>513</th>\n",
       "      <td>aug</td>\n",
       "      <td>sun</td>\n",
       "      <td>81.6</td>\n",
       "      <td>56.7</td>\n",
       "      <td>665.6</td>\n",
       "      <td>1.9</td>\n",
       "      <td>21.9</td>\n",
       "      <td>71</td>\n",
       "      <td>5.8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>54.29</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>large</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>514</th>\n",
       "      <td>aug</td>\n",
       "      <td>sun</td>\n",
       "      <td>81.6</td>\n",
       "      <td>56.7</td>\n",
       "      <td>665.6</td>\n",
       "      <td>1.9</td>\n",
       "      <td>21.2</td>\n",
       "      <td>70</td>\n",
       "      <td>6.7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.16</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>large</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>515</th>\n",
       "      <td>aug</td>\n",
       "      <td>sat</td>\n",
       "      <td>94.4</td>\n",
       "      <td>146.0</td>\n",
       "      <td>614.7</td>\n",
       "      <td>11.3</td>\n",
       "      <td>25.6</td>\n",
       "      <td>42</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>small</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>516</th>\n",
       "      <td>nov</td>\n",
       "      <td>tue</td>\n",
       "      <td>79.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>106.7</td>\n",
       "      <td>1.1</td>\n",
       "      <td>11.8</td>\n",
       "      <td>31</td>\n",
       "      <td>4.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>small</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>517 rows Ã— 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    month  day  FFMC    DMC  ...  monthnov  monthoct  monthsep  size_category\n",
       "0     mar  fri  86.2   26.2  ...         0         0         0          small\n",
       "1     oct  tue  90.6   35.4  ...         0         1         0          small\n",
       "2     oct  sat  90.6   43.7  ...         0         1         0          small\n",
       "3     mar  fri  91.7   33.3  ...         0         0         0          small\n",
       "4     mar  sun  89.3   51.3  ...         0         0         0          small\n",
       "..    ...  ...   ...    ...  ...       ...       ...       ...            ...\n",
       "512   aug  sun  81.6   56.7  ...         0         0         0          large\n",
       "513   aug  sun  81.6   56.7  ...         0         0         0          large\n",
       "514   aug  sun  81.6   56.7  ...         0         0         0          large\n",
       "515   aug  sat  94.4  146.0  ...         0         0         0          small\n",
       "516   nov  tue  79.5    3.0  ...         1         0         0          small\n",
       "\n",
       "[517 rows x 31 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=pd.read_csv('forestfires.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3692ed95",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3692ed95",
    "outputId": "03340013-f6bf-457f-e90b-a42eaa4087ba"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-8.05959472e-01, -1.32332557e+00, -1.83047676e+00, ...,\n",
       "        -4.40225453e-02, -1.72859706e-01, -7.06081245e-01],\n",
       "       [-8.10203395e-03, -1.17954077e+00,  4.88890915e-01, ...,\n",
       "        -4.40225453e-02,  5.78503817e+00, -7.06081245e-01],\n",
       "       [-8.10203395e-03, -1.04982188e+00,  5.60715454e-01, ...,\n",
       "        -4.40225453e-02,  5.78503817e+00, -7.06081245e-01],\n",
       "       ...,\n",
       "       [-1.64008316e+00, -8.46647711e-01,  4.74768113e-01, ...,\n",
       "        -4.40225453e-02, -1.72859706e-01, -7.06081245e-01],\n",
       "       [ 6.80956663e-01,  5.49002541e-01,  2.69382214e-01, ...,\n",
       "        -4.40225453e-02, -1.72859706e-01, -7.06081245e-01],\n",
       "       [-2.02087875e+00, -1.68591332e+00, -1.78044169e+00, ...,\n",
       "         2.27156334e+01, -1.72859706e-01, -7.06081245e-01]])"
      ]
     },
     "execution_count": 5,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#scaling the numerical data( leaving the target variable )\n",
    "df1=df.iloc[:,2:30]\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc=StandardScaler()\n",
    "df_norm=sc.fit_transform(df1)\n",
    "df_norm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58e6819f",
   "metadata": {
    "id": "58e6819f"
   },
   "source": [
    "# PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b1de1b29",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b1de1b29",
    "outputId": "d27e6af9-5d46-48a5-e0bd-9a3ab792e1a9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 3.76670947e+00, -1.32025451e+00, -8.43971398e-01, ...,\n",
       "        -6.53345819e-02, -6.05082538e-15, -1.58743875e-16],\n",
       "       [ 3.90786263e-01,  8.31061522e-01, -1.10136513e+00, ...,\n",
       "         3.42618601e-02, -2.67236885e-15, -6.92610536e-16],\n",
       "       [ 6.90415596e-01,  1.17774562e+00, -1.22199841e+00, ...,\n",
       "         2.63235187e-02,  5.92028990e-15,  8.36530871e-16],\n",
       "       ...,\n",
       "       [ 9.21634000e-01, -2.64543072e-01,  2.71921606e+00, ...,\n",
       "        -2.97865814e-01, -6.98934052e-16,  4.03200598e-18],\n",
       "       [-1.62054896e+00, -9.78838231e-01,  3.31987355e-01, ...,\n",
       "         3.91949863e-02,  5.57925976e-16, -3.39227990e-17],\n",
       "       [ 4.07590654e+00, -3.67440726e-01, -2.47151775e-01, ...,\n",
       "        -2.50420726e-02,  6.17289277e-17, -8.31075187e-17]])"
      ]
     },
     "execution_count": 6,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca=PCA(n_components=28)\n",
    "pca_values=pca.fit_transform(df_norm)\n",
    "pca_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c6e6c4f7",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c6e6c4f7",
    "outputId": "90e09cfb-1afa-4aa7-fe90-35ef3d5c3f0c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.35522746e-01, 6.85788793e-02, 6.23572652e-02, 5.32713255e-02,\n",
       "       4.75942360e-02, 4.68009902e-02, 4.37490015e-02, 4.28025164e-02,\n",
       "       4.08875728e-02, 4.01633268e-02, 3.92926854e-02, 3.83232321e-02,\n",
       "       3.64221503e-02, 3.63217289e-02, 3.57856782e-02, 3.50087806e-02,\n",
       "       3.35447704e-02, 3.24777366e-02, 3.04490902e-02, 3.00246758e-02,\n",
       "       2.37167400e-02, 2.08329788e-02, 1.18357869e-02, 8.88449559e-03,\n",
       "       4.55347471e-03, 7.98135931e-04, 2.67271490e-32, 3.42850975e-33])"
      ]
     },
     "execution_count": 7,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var=pca.explained_variance_ratio_\n",
    "var\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b275d007",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b275d007",
    "outputId": "ba944408-d65a-49d7-8d41-2ec8965b8036"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([13.55, 20.41, 26.65, 31.98, 36.74, 41.42, 45.79, 50.07, 54.16,\n",
       "       58.18, 62.11, 65.94, 69.58, 73.21, 76.79, 80.29, 83.64, 86.89,\n",
       "       89.93, 92.93, 95.3 , 97.38, 98.56, 99.45, 99.91, 99.99, 99.99,\n",
       "       99.99])"
      ]
     },
     "execution_count": 8,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var1=np.cumsum(np.round(var,decimals=4)*100)\n",
    "var1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9a8a291b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 500
    },
    "id": "9a8a291b",
    "outputId": "4afcf8fe-d097-4a65-8313-ad5271b74a63"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f904b46ebd0>]"
      ]
     },
     "execution_count": 9,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light",
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(12,8))\n",
    "plt.plot(var1,color='red')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "179116e8",
   "metadata": {
    "id": "179116e8"
   },
   "outputs": [],
   "source": [
    "# hence here we will choose 24 pcs outoff 28 for further procedure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ecbbcd7b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 439
    },
    "id": "ecbbcd7b",
    "outputId": "c990ae84-9320-4914-d943-fe15c832fb33"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pc1</th>\n",
       "      <th>pc2</th>\n",
       "      <th>pc3</th>\n",
       "      <th>pc4</th>\n",
       "      <th>pc5</th>\n",
       "      <th>pc6</th>\n",
       "      <th>pc7</th>\n",
       "      <th>pc8</th>\n",
       "      <th>pc9</th>\n",
       "      <th>pc10</th>\n",
       "      <th>pc11</th>\n",
       "      <th>pc12</th>\n",
       "      <th>pc13</th>\n",
       "      <th>pc14</th>\n",
       "      <th>pc15</th>\n",
       "      <th>pc16</th>\n",
       "      <th>pc17</th>\n",
       "      <th>pc18</th>\n",
       "      <th>pc19</th>\n",
       "      <th>pc20</th>\n",
       "      <th>pc21</th>\n",
       "      <th>pc22</th>\n",
       "      <th>pc23</th>\n",
       "      <th>pc24</th>\n",
       "      <th>size_category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3.766709</td>\n",
       "      <td>-1.320255</td>\n",
       "      <td>-0.843971</td>\n",
       "      <td>-1.994738</td>\n",
       "      <td>-1.453359</td>\n",
       "      <td>0.693985</td>\n",
       "      <td>0.308104</td>\n",
       "      <td>-0.019764</td>\n",
       "      <td>0.010161</td>\n",
       "      <td>-0.437314</td>\n",
       "      <td>-0.536738</td>\n",
       "      <td>1.234550</td>\n",
       "      <td>0.276198</td>\n",
       "      <td>-0.671216</td>\n",
       "      <td>-0.529599</td>\n",
       "      <td>-0.197543</td>\n",
       "      <td>-0.021839</td>\n",
       "      <td>0.688958</td>\n",
       "      <td>0.563603</td>\n",
       "      <td>-0.439596</td>\n",
       "      <td>-0.926619</td>\n",
       "      <td>-0.405425</td>\n",
       "      <td>-0.118719</td>\n",
       "      <td>-0.017933</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.390786</td>\n",
       "      <td>0.831062</td>\n",
       "      <td>-1.101365</td>\n",
       "      <td>1.400671</td>\n",
       "      <td>2.869388</td>\n",
       "      <td>0.965898</td>\n",
       "      <td>-2.795574</td>\n",
       "      <td>0.041095</td>\n",
       "      <td>-0.548879</td>\n",
       "      <td>0.104500</td>\n",
       "      <td>-2.876498</td>\n",
       "      <td>-0.568255</td>\n",
       "      <td>2.095225</td>\n",
       "      <td>1.417634</td>\n",
       "      <td>-0.879983</td>\n",
       "      <td>-2.503167</td>\n",
       "      <td>0.499649</td>\n",
       "      <td>0.563706</td>\n",
       "      <td>-0.703319</td>\n",
       "      <td>-1.535718</td>\n",
       "      <td>-0.892995</td>\n",
       "      <td>0.836590</td>\n",
       "      <td>0.204975</td>\n",
       "      <td>0.290771</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.690416</td>\n",
       "      <td>1.177746</td>\n",
       "      <td>-1.221998</td>\n",
       "      <td>2.442038</td>\n",
       "      <td>1.090630</td>\n",
       "      <td>0.390801</td>\n",
       "      <td>-1.586675</td>\n",
       "      <td>-2.159336</td>\n",
       "      <td>-0.090580</td>\n",
       "      <td>0.260888</td>\n",
       "      <td>-3.236229</td>\n",
       "      <td>-0.601439</td>\n",
       "      <td>1.998004</td>\n",
       "      <td>1.477351</td>\n",
       "      <td>-0.946682</td>\n",
       "      <td>-2.545144</td>\n",
       "      <td>-0.658411</td>\n",
       "      <td>-0.423618</td>\n",
       "      <td>0.860550</td>\n",
       "      <td>-1.195230</td>\n",
       "      <td>-0.297870</td>\n",
       "      <td>0.743648</td>\n",
       "      <td>0.081757</td>\n",
       "      <td>0.345915</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.359951</td>\n",
       "      <td>-1.161443</td>\n",
       "      <td>0.385728</td>\n",
       "      <td>-2.118328</td>\n",
       "      <td>-1.949601</td>\n",
       "      <td>1.027664</td>\n",
       "      <td>-0.179422</td>\n",
       "      <td>-0.250227</td>\n",
       "      <td>-0.620329</td>\n",
       "      <td>-1.343189</td>\n",
       "      <td>-0.145846</td>\n",
       "      <td>1.019492</td>\n",
       "      <td>0.576990</td>\n",
       "      <td>-0.752744</td>\n",
       "      <td>0.349346</td>\n",
       "      <td>-0.040887</td>\n",
       "      <td>0.017843</td>\n",
       "      <td>0.332572</td>\n",
       "      <td>1.164745</td>\n",
       "      <td>-1.632741</td>\n",
       "      <td>-0.817618</td>\n",
       "      <td>1.523710</td>\n",
       "      <td>-0.342302</td>\n",
       "      <td>-0.378420</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.974329</td>\n",
       "      <td>-0.842626</td>\n",
       "      <td>1.327788</td>\n",
       "      <td>0.038086</td>\n",
       "      <td>-1.124763</td>\n",
       "      <td>-0.574676</td>\n",
       "      <td>-0.777155</td>\n",
       "      <td>0.303635</td>\n",
       "      <td>0.861126</td>\n",
       "      <td>-2.024719</td>\n",
       "      <td>-0.467108</td>\n",
       "      <td>1.131879</td>\n",
       "      <td>-0.137990</td>\n",
       "      <td>-0.823316</td>\n",
       "      <td>0.402298</td>\n",
       "      <td>0.844431</td>\n",
       "      <td>1.014944</td>\n",
       "      <td>-0.618231</td>\n",
       "      <td>0.822853</td>\n",
       "      <td>-1.794109</td>\n",
       "      <td>-0.723371</td>\n",
       "      <td>2.020419</td>\n",
       "      <td>-0.545591</td>\n",
       "      <td>0.161735</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>512</th>\n",
       "      <td>-0.087560</td>\n",
       "      <td>0.153964</td>\n",
       "      <td>1.241810</td>\n",
       "      <td>1.536581</td>\n",
       "      <td>0.372425</td>\n",
       "      <td>-1.133422</td>\n",
       "      <td>-0.362287</td>\n",
       "      <td>0.766946</td>\n",
       "      <td>0.818745</td>\n",
       "      <td>-0.289632</td>\n",
       "      <td>-0.933909</td>\n",
       "      <td>0.161275</td>\n",
       "      <td>-0.398215</td>\n",
       "      <td>0.197490</td>\n",
       "      <td>-0.801640</td>\n",
       "      <td>0.300522</td>\n",
       "      <td>0.513876</td>\n",
       "      <td>0.539642</td>\n",
       "      <td>-0.052958</td>\n",
       "      <td>1.898628</td>\n",
       "      <td>-1.441786</td>\n",
       "      <td>-0.821192</td>\n",
       "      <td>-1.205707</td>\n",
       "      <td>-0.698666</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>513</th>\n",
       "      <td>0.794366</td>\n",
       "      <td>-0.083966</td>\n",
       "      <td>2.670485</td>\n",
       "      <td>0.284995</td>\n",
       "      <td>0.223323</td>\n",
       "      <td>-0.904232</td>\n",
       "      <td>-0.014849</td>\n",
       "      <td>0.107226</td>\n",
       "      <td>1.340049</td>\n",
       "      <td>-0.147246</td>\n",
       "      <td>-0.652051</td>\n",
       "      <td>-0.132893</td>\n",
       "      <td>-0.518732</td>\n",
       "      <td>0.162358</td>\n",
       "      <td>-0.274733</td>\n",
       "      <td>0.342367</td>\n",
       "      <td>0.485571</td>\n",
       "      <td>0.580150</td>\n",
       "      <td>0.384984</td>\n",
       "      <td>0.086251</td>\n",
       "      <td>-0.970693</td>\n",
       "      <td>-1.353365</td>\n",
       "      <td>-1.254890</td>\n",
       "      <td>-1.212175</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>514</th>\n",
       "      <td>0.921634</td>\n",
       "      <td>-0.264543</td>\n",
       "      <td>2.719216</td>\n",
       "      <td>-0.019643</td>\n",
       "      <td>0.242195</td>\n",
       "      <td>-0.966939</td>\n",
       "      <td>-0.118080</td>\n",
       "      <td>0.123010</td>\n",
       "      <td>1.290364</td>\n",
       "      <td>-0.177553</td>\n",
       "      <td>-0.657663</td>\n",
       "      <td>-0.083060</td>\n",
       "      <td>-0.285899</td>\n",
       "      <td>0.062647</td>\n",
       "      <td>-0.494765</td>\n",
       "      <td>0.332816</td>\n",
       "      <td>0.344047</td>\n",
       "      <td>0.122409</td>\n",
       "      <td>0.313948</td>\n",
       "      <td>0.211157</td>\n",
       "      <td>-0.777731</td>\n",
       "      <td>-1.736711</td>\n",
       "      <td>-1.154127</td>\n",
       "      <td>-1.230040</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>515</th>\n",
       "      <td>-1.620549</td>\n",
       "      <td>-0.978838</td>\n",
       "      <td>0.331987</td>\n",
       "      <td>1.256638</td>\n",
       "      <td>-0.408164</td>\n",
       "      <td>0.735698</td>\n",
       "      <td>0.815510</td>\n",
       "      <td>-1.398344</td>\n",
       "      <td>0.076379</td>\n",
       "      <td>-0.005814</td>\n",
       "      <td>-0.503898</td>\n",
       "      <td>0.174276</td>\n",
       "      <td>-0.163149</td>\n",
       "      <td>0.246912</td>\n",
       "      <td>-0.147679</td>\n",
       "      <td>-0.011739</td>\n",
       "      <td>-1.035533</td>\n",
       "      <td>-0.774382</td>\n",
       "      <td>-0.216315</td>\n",
       "      <td>0.515791</td>\n",
       "      <td>0.080575</td>\n",
       "      <td>-0.055548</td>\n",
       "      <td>-0.067502</td>\n",
       "      <td>-0.311027</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>516</th>\n",
       "      <td>4.075907</td>\n",
       "      <td>-0.367441</td>\n",
       "      <td>-0.247152</td>\n",
       "      <td>0.979966</td>\n",
       "      <td>6.792273</td>\n",
       "      <td>5.943666</td>\n",
       "      <td>-1.639583</td>\n",
       "      <td>8.121827</td>\n",
       "      <td>-0.627980</td>\n",
       "      <td>4.953722</td>\n",
       "      <td>-1.411962</td>\n",
       "      <td>2.986327</td>\n",
       "      <td>-2.734589</td>\n",
       "      <td>6.584205</td>\n",
       "      <td>-6.010301</td>\n",
       "      <td>10.467443</td>\n",
       "      <td>-7.333036</td>\n",
       "      <td>0.377340</td>\n",
       "      <td>8.870354</td>\n",
       "      <td>-1.074288</td>\n",
       "      <td>2.382433</td>\n",
       "      <td>1.042850</td>\n",
       "      <td>0.296436</td>\n",
       "      <td>0.125099</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>517 rows Ã— 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          pc1       pc2       pc3  ...      pc23      pc24  size_category\n",
       "0    3.766709 -1.320255 -0.843971  ... -0.118719 -0.017933              0\n",
       "1    0.390786  0.831062 -1.101365  ...  0.204975  0.290771              0\n",
       "2    0.690416  1.177746 -1.221998  ...  0.081757  0.345915              0\n",
       "3    3.359951 -1.161443  0.385728  ... -0.342302 -0.378420              0\n",
       "4    2.974329 -0.842626  1.327788  ... -0.545591  0.161735              0\n",
       "..        ...       ...       ...  ...       ...       ...            ...\n",
       "512 -0.087560  0.153964  1.241810  ... -1.205707 -0.698666              1\n",
       "513  0.794366 -0.083966  2.670485  ... -1.254890 -1.212175              1\n",
       "514  0.921634 -0.264543  2.719216  ... -1.154127 -1.230040              1\n",
       "515 -1.620549 -0.978838  0.331987  ... -0.067502 -0.311027              0\n",
       "516  4.075907 -0.367441 -0.247152  ...  0.296436  0.125099              0\n",
       "\n",
       "[517 rows x 25 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "finaldf=pd.concat([pd.DataFrame(pca_values[:,0:24],columns=['pc1','pc2','pc3','pc4','pc5','pc6','pc7',\n",
    "                                                             'pc8','pc9','pc10','pc11','pc12','pc13','pc14',\n",
    "                                                             'pc15','pc16','pc17','pc18','pc19','pc20','pc21',\n",
    "                                                             'pc22','pc23','pc24']),\n",
    "                 df[['size_category']]], axis = 1)\n",
    "finaldf.size_category.replace(('large','small'),(1,0),inplace=True)\n",
    "finaldf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "97c32c75",
   "metadata": {
    "id": "97c32c75"
   },
   "outputs": [],
   "source": [
    "#split the data into x and y\n",
    "array=finaldf.values\n",
    "x=array[:,0:24]\n",
    "y=array[:,24]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8fad939",
   "metadata": {
    "id": "a8fad939"
   },
   "source": [
    "# iteration 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0f8016c6",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0f8016c6",
    "outputId": "c6890449-93c6-4257-a0f3-04bf7654adea"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "37/37 [==============================] - 1s 8ms/step - loss: 0.6585 - accuracy: 0.6364 - val_loss: 0.6798 - val_accuracy: 0.6346\n",
      "Epoch 2/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.6364 - accuracy: 0.7271 - val_loss: 0.6730 - val_accuracy: 0.6859\n",
      "Epoch 3/150\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.5872 - accuracy: 0.7609 - val_loss: 0.6745 - val_accuracy: 0.6731\n",
      "Epoch 4/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.5631 - accuracy: 0.7666 - val_loss: 0.6708 - val_accuracy: 0.6731\n",
      "Epoch 5/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.5263 - accuracy: 0.7929 - val_loss: 0.6738 - val_accuracy: 0.6731\n",
      "Epoch 6/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.5405 - accuracy: 0.7645 - val_loss: 0.6741 - val_accuracy: 0.6731\n",
      "Epoch 7/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.5290 - accuracy: 0.7660 - val_loss: 0.6719 - val_accuracy: 0.6731\n",
      "Epoch 8/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.5292 - accuracy: 0.7679 - val_loss: 0.6721 - val_accuracy: 0.6731\n",
      "Epoch 9/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.5259 - accuracy: 0.7645 - val_loss: 0.6706 - val_accuracy: 0.6731\n",
      "Epoch 10/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.4981 - accuracy: 0.7799 - val_loss: 0.6619 - val_accuracy: 0.6731\n",
      "Epoch 11/150\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.5067 - accuracy: 0.7796 - val_loss: 0.6668 - val_accuracy: 0.6731\n",
      "Epoch 12/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.4993 - accuracy: 0.7808 - val_loss: 0.6697 - val_accuracy: 0.6731\n",
      "Epoch 13/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.4794 - accuracy: 0.7865 - val_loss: 0.6694 - val_accuracy: 0.6795\n",
      "Epoch 14/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.4949 - accuracy: 0.7625 - val_loss: 0.6675 - val_accuracy: 0.6795\n",
      "Epoch 15/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.5083 - accuracy: 0.7585 - val_loss: 0.6691 - val_accuracy: 0.6859\n",
      "Epoch 16/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.4909 - accuracy: 0.7715 - val_loss: 0.6693 - val_accuracy: 0.6859\n",
      "Epoch 17/150\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.4578 - accuracy: 0.8160 - val_loss: 0.6671 - val_accuracy: 0.6859\n",
      "Epoch 18/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.4589 - accuracy: 0.8035 - val_loss: 0.6673 - val_accuracy: 0.6923\n",
      "Epoch 19/150\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.4881 - accuracy: 0.7796 - val_loss: 0.6695 - val_accuracy: 0.6923\n",
      "Epoch 20/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.4375 - accuracy: 0.8142 - val_loss: 0.6679 - val_accuracy: 0.6923\n",
      "Epoch 21/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.4399 - accuracy: 0.8125 - val_loss: 0.6674 - val_accuracy: 0.7051\n",
      "Epoch 22/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.4600 - accuracy: 0.7886 - val_loss: 0.6686 - val_accuracy: 0.7051\n",
      "Epoch 23/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.4535 - accuracy: 0.7924 - val_loss: 0.6707 - val_accuracy: 0.7051\n",
      "Epoch 24/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.4638 - accuracy: 0.7923 - val_loss: 0.6693 - val_accuracy: 0.7051\n",
      "Epoch 25/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.4451 - accuracy: 0.7940 - val_loss: 0.6679 - val_accuracy: 0.7051\n",
      "Epoch 26/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.4341 - accuracy: 0.8020 - val_loss: 0.6755 - val_accuracy: 0.7051\n",
      "Epoch 27/150\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.3884 - accuracy: 0.8347 - val_loss: 0.6774 - val_accuracy: 0.7115\n",
      "Epoch 28/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.3990 - accuracy: 0.8137 - val_loss: 0.6796 - val_accuracy: 0.6987\n",
      "Epoch 29/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.3871 - accuracy: 0.8299 - val_loss: 0.6814 - val_accuracy: 0.6987\n",
      "Epoch 30/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.4010 - accuracy: 0.8147 - val_loss: 0.6902 - val_accuracy: 0.6987\n",
      "Epoch 31/150\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.3822 - accuracy: 0.8310 - val_loss: 0.6941 - val_accuracy: 0.6987\n",
      "Epoch 32/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.3732 - accuracy: 0.8373 - val_loss: 0.6932 - val_accuracy: 0.6987\n",
      "Epoch 33/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.3508 - accuracy: 0.8482 - val_loss: 0.7007 - val_accuracy: 0.6987\n",
      "Epoch 34/150\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.3856 - accuracy: 0.8379 - val_loss: 0.7016 - val_accuracy: 0.6859\n",
      "Epoch 35/150\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.3820 - accuracy: 0.8339 - val_loss: 0.7074 - val_accuracy: 0.6923\n",
      "Epoch 36/150\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.3870 - accuracy: 0.8226 - val_loss: 0.7124 - val_accuracy: 0.6987\n",
      "Epoch 37/150\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.3451 - accuracy: 0.8601 - val_loss: 0.7159 - val_accuracy: 0.6859\n",
      "Epoch 38/150\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.3842 - accuracy: 0.8264 - val_loss: 0.7163 - val_accuracy: 0.6731\n",
      "Epoch 39/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.3104 - accuracy: 0.8761 - val_loss: 0.7214 - val_accuracy: 0.6795\n",
      "Epoch 40/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.3684 - accuracy: 0.8374 - val_loss: 0.7247 - val_accuracy: 0.6667\n",
      "Epoch 41/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.3365 - accuracy: 0.8568 - val_loss: 0.7351 - val_accuracy: 0.6538\n",
      "Epoch 42/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.3214 - accuracy: 0.8587 - val_loss: 0.7425 - val_accuracy: 0.6667\n",
      "Epoch 43/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.3567 - accuracy: 0.8267 - val_loss: 0.7588 - val_accuracy: 0.6603\n",
      "Epoch 44/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.3291 - accuracy: 0.8571 - val_loss: 0.7626 - val_accuracy: 0.6603\n",
      "Epoch 45/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.3490 - accuracy: 0.8453 - val_loss: 0.7685 - val_accuracy: 0.6603\n",
      "Epoch 46/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.3301 - accuracy: 0.8594 - val_loss: 0.7768 - val_accuracy: 0.6603\n",
      "Epoch 47/150\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.3166 - accuracy: 0.8756 - val_loss: 0.7817 - val_accuracy: 0.6474\n",
      "Epoch 48/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.3012 - accuracy: 0.8822 - val_loss: 0.7847 - val_accuracy: 0.6538\n",
      "Epoch 49/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.2963 - accuracy: 0.8778 - val_loss: 0.7974 - val_accuracy: 0.6538\n",
      "Epoch 50/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.3203 - accuracy: 0.8562 - val_loss: 0.7976 - val_accuracy: 0.6603\n",
      "Epoch 51/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.2708 - accuracy: 0.9002 - val_loss: 0.8052 - val_accuracy: 0.6603\n",
      "Epoch 52/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.3101 - accuracy: 0.8653 - val_loss: 0.8120 - val_accuracy: 0.6603\n",
      "Epoch 53/150\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.2850 - accuracy: 0.8788 - val_loss: 0.8223 - val_accuracy: 0.6538\n",
      "Epoch 54/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.3047 - accuracy: 0.8586 - val_loss: 0.8324 - val_accuracy: 0.6474\n",
      "Epoch 55/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.3065 - accuracy: 0.8695 - val_loss: 0.8337 - val_accuracy: 0.6603\n",
      "Epoch 56/150\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.3435 - accuracy: 0.8384 - val_loss: 0.8456 - val_accuracy: 0.6538\n",
      "Epoch 57/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.2722 - accuracy: 0.8845 - val_loss: 0.8525 - val_accuracy: 0.6538\n",
      "Epoch 58/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.2618 - accuracy: 0.8915 - val_loss: 0.8578 - val_accuracy: 0.6538\n",
      "Epoch 59/150\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.2786 - accuracy: 0.8866 - val_loss: 0.8701 - val_accuracy: 0.6538\n",
      "Epoch 60/150\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.2750 - accuracy: 0.8951 - val_loss: 0.8722 - val_accuracy: 0.6538\n",
      "Epoch 61/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.2730 - accuracy: 0.8907 - val_loss: 0.8822 - val_accuracy: 0.6603\n",
      "Epoch 62/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.2951 - accuracy: 0.8654 - val_loss: 0.8856 - val_accuracy: 0.6603\n",
      "Epoch 63/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.2756 - accuracy: 0.8813 - val_loss: 0.8902 - val_accuracy: 0.6603\n",
      "Epoch 64/150\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.2505 - accuracy: 0.8980 - val_loss: 0.9090 - val_accuracy: 0.6538\n",
      "Epoch 65/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.2740 - accuracy: 0.8777 - val_loss: 0.9149 - val_accuracy: 0.6667\n",
      "Epoch 66/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.2583 - accuracy: 0.8878 - val_loss: 0.9083 - val_accuracy: 0.6667\n",
      "Epoch 67/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.2354 - accuracy: 0.8940 - val_loss: 0.9235 - val_accuracy: 0.6603\n",
      "Epoch 68/150\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.2275 - accuracy: 0.8967 - val_loss: 0.9350 - val_accuracy: 0.6667\n",
      "Epoch 69/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.2429 - accuracy: 0.8891 - val_loss: 0.9395 - val_accuracy: 0.6667\n",
      "Epoch 70/150\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 0.2413 - accuracy: 0.8909 - val_loss: 0.9397 - val_accuracy: 0.6667\n",
      "Epoch 71/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.2452 - accuracy: 0.8998 - val_loss: 0.9616 - val_accuracy: 0.6731\n",
      "Epoch 72/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.2134 - accuracy: 0.9242 - val_loss: 0.9629 - val_accuracy: 0.6795\n",
      "Epoch 73/150\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.2268 - accuracy: 0.9137 - val_loss: 0.9720 - val_accuracy: 0.6795\n",
      "Epoch 74/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.2319 - accuracy: 0.8984 - val_loss: 0.9815 - val_accuracy: 0.6795\n",
      "Epoch 75/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.2387 - accuracy: 0.8899 - val_loss: 0.9824 - val_accuracy: 0.6795\n",
      "Epoch 76/150\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 0.2055 - accuracy: 0.9176 - val_loss: 0.9926 - val_accuracy: 0.6859\n",
      "Epoch 77/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.1927 - accuracy: 0.9277 - val_loss: 0.9746 - val_accuracy: 0.6731\n",
      "Epoch 78/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.2280 - accuracy: 0.8963 - val_loss: 0.9865 - val_accuracy: 0.6859\n",
      "Epoch 79/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.1984 - accuracy: 0.9205 - val_loss: 0.9841 - val_accuracy: 0.6795\n",
      "Epoch 80/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.2215 - accuracy: 0.8995 - val_loss: 0.9878 - val_accuracy: 0.6859\n",
      "Epoch 81/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.2252 - accuracy: 0.8990 - val_loss: 0.9910 - val_accuracy: 0.6795\n",
      "Epoch 82/150\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.2327 - accuracy: 0.8903 - val_loss: 0.9996 - val_accuracy: 0.6795\n",
      "Epoch 83/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.2022 - accuracy: 0.9049 - val_loss: 0.9911 - val_accuracy: 0.6859\n",
      "Epoch 84/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.2221 - accuracy: 0.8952 - val_loss: 1.0006 - val_accuracy: 0.6923\n",
      "Epoch 85/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.1965 - accuracy: 0.9039 - val_loss: 1.0125 - val_accuracy: 0.6795\n",
      "Epoch 86/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.2013 - accuracy: 0.8950 - val_loss: 1.0173 - val_accuracy: 0.6859\n",
      "Epoch 87/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.1684 - accuracy: 0.9264 - val_loss: 1.0421 - val_accuracy: 0.6603\n",
      "Epoch 88/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.1935 - accuracy: 0.9316 - val_loss: 1.0337 - val_accuracy: 0.6795\n",
      "Epoch 89/150\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.1863 - accuracy: 0.9206 - val_loss: 1.0524 - val_accuracy: 0.6731\n",
      "Epoch 90/150\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.2056 - accuracy: 0.9156 - val_loss: 1.0452 - val_accuracy: 0.6795\n",
      "Epoch 91/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.2111 - accuracy: 0.8973 - val_loss: 1.0429 - val_accuracy: 0.6859\n",
      "Epoch 92/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.1634 - accuracy: 0.9385 - val_loss: 1.0525 - val_accuracy: 0.6795\n",
      "Epoch 93/150\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.2132 - accuracy: 0.8873 - val_loss: 1.0525 - val_accuracy: 0.6795\n",
      "Epoch 94/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.1643 - accuracy: 0.9313 - val_loss: 1.0569 - val_accuracy: 0.6795\n",
      "Epoch 95/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.1784 - accuracy: 0.9219 - val_loss: 1.0685 - val_accuracy: 0.6795\n",
      "Epoch 96/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.1728 - accuracy: 0.9325 - val_loss: 1.0614 - val_accuracy: 0.6859\n",
      "Epoch 97/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.1615 - accuracy: 0.9329 - val_loss: 1.0677 - val_accuracy: 0.6795\n",
      "Epoch 98/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.1767 - accuracy: 0.9161 - val_loss: 1.0782 - val_accuracy: 0.6795\n",
      "Epoch 99/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.1605 - accuracy: 0.9421 - val_loss: 1.0752 - val_accuracy: 0.6923\n",
      "Epoch 100/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.1702 - accuracy: 0.9306 - val_loss: 1.0731 - val_accuracy: 0.6987\n",
      "Epoch 101/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.1592 - accuracy: 0.9312 - val_loss: 1.0907 - val_accuracy: 0.6923\n",
      "Epoch 102/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.1707 - accuracy: 0.9179 - val_loss: 1.0791 - val_accuracy: 0.6987\n",
      "Epoch 103/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.1455 - accuracy: 0.9451 - val_loss: 1.0978 - val_accuracy: 0.6923\n",
      "Epoch 104/150\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.1576 - accuracy: 0.9465 - val_loss: 1.0982 - val_accuracy: 0.7051\n",
      "Epoch 105/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.1952 - accuracy: 0.9175 - val_loss: 1.1084 - val_accuracy: 0.7115\n",
      "Epoch 106/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.1470 - accuracy: 0.9361 - val_loss: 1.1142 - val_accuracy: 0.7051\n",
      "Epoch 107/150\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.1703 - accuracy: 0.9385 - val_loss: 1.1106 - val_accuracy: 0.7051\n",
      "Epoch 108/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.1649 - accuracy: 0.9307 - val_loss: 1.1015 - val_accuracy: 0.7179\n",
      "Epoch 109/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.1520 - accuracy: 0.9424 - val_loss: 1.1379 - val_accuracy: 0.6859\n",
      "Epoch 110/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.1303 - accuracy: 0.9570 - val_loss: 1.1330 - val_accuracy: 0.7051\n",
      "Epoch 111/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.1349 - accuracy: 0.9678 - val_loss: 1.1094 - val_accuracy: 0.7179\n",
      "Epoch 112/150\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.1385 - accuracy: 0.9437 - val_loss: 1.1166 - val_accuracy: 0.7115\n",
      "Epoch 113/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.1433 - accuracy: 0.9623 - val_loss: 1.1023 - val_accuracy: 0.7179\n",
      "Epoch 114/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.1551 - accuracy: 0.9470 - val_loss: 1.0666 - val_accuracy: 0.7179\n",
      "Epoch 115/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.1372 - accuracy: 0.9515 - val_loss: 1.0698 - val_accuracy: 0.7179\n",
      "Epoch 116/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.1458 - accuracy: 0.9404 - val_loss: 1.0873 - val_accuracy: 0.7244\n",
      "Epoch 117/150\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.1343 - accuracy: 0.9476 - val_loss: 1.0899 - val_accuracy: 0.7308\n",
      "Epoch 118/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.1410 - accuracy: 0.9435 - val_loss: 1.0882 - val_accuracy: 0.7308\n",
      "Epoch 119/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.1205 - accuracy: 0.9696 - val_loss: 1.0926 - val_accuracy: 0.7308\n",
      "Epoch 120/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.1421 - accuracy: 0.9496 - val_loss: 1.1046 - val_accuracy: 0.7308\n",
      "Epoch 121/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.1196 - accuracy: 0.9614 - val_loss: 1.1030 - val_accuracy: 0.7308\n",
      "Epoch 122/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.1156 - accuracy: 0.9542 - val_loss: 1.1128 - val_accuracy: 0.7308\n",
      "Epoch 123/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.1270 - accuracy: 0.9613 - val_loss: 1.1106 - val_accuracy: 0.7372\n",
      "Epoch 124/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.1141 - accuracy: 0.9552 - val_loss: 1.0917 - val_accuracy: 0.7244\n",
      "Epoch 125/150\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.1220 - accuracy: 0.9448 - val_loss: 1.0943 - val_accuracy: 0.7372\n",
      "Epoch 126/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.1071 - accuracy: 0.9699 - val_loss: 1.1002 - val_accuracy: 0.7308\n",
      "Epoch 127/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.1097 - accuracy: 0.9642 - val_loss: 1.0899 - val_accuracy: 0.7372\n",
      "Epoch 128/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.1349 - accuracy: 0.9435 - val_loss: 1.0973 - val_accuracy: 0.7308\n",
      "Epoch 129/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.1034 - accuracy: 0.9679 - val_loss: 1.0948 - val_accuracy: 0.7308\n",
      "Epoch 130/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.0963 - accuracy: 0.9742 - val_loss: 1.0958 - val_accuracy: 0.7308\n",
      "Epoch 131/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.0997 - accuracy: 0.9760 - val_loss: 1.1047 - val_accuracy: 0.7308\n",
      "Epoch 132/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.0884 - accuracy: 0.9766 - val_loss: 1.1222 - val_accuracy: 0.7308\n",
      "Epoch 133/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.0948 - accuracy: 0.9675 - val_loss: 1.1075 - val_accuracy: 0.7308\n",
      "Epoch 134/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.0920 - accuracy: 0.9673 - val_loss: 1.1053 - val_accuracy: 0.7308\n",
      "Epoch 135/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.0953 - accuracy: 0.9719 - val_loss: 1.1190 - val_accuracy: 0.7308\n",
      "Epoch 136/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.0730 - accuracy: 0.9840 - val_loss: 1.1343 - val_accuracy: 0.7308\n",
      "Epoch 137/150\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.1049 - accuracy: 0.9553 - val_loss: 1.1162 - val_accuracy: 0.7308\n",
      "Epoch 138/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.0843 - accuracy: 0.9702 - val_loss: 1.1366 - val_accuracy: 0.7308\n",
      "Epoch 139/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.0835 - accuracy: 0.9757 - val_loss: 1.1095 - val_accuracy: 0.7308\n",
      "Epoch 140/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.0883 - accuracy: 0.9679 - val_loss: 1.1353 - val_accuracy: 0.7308\n",
      "Epoch 141/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.0857 - accuracy: 0.9733 - val_loss: 1.1481 - val_accuracy: 0.7308\n",
      "Epoch 142/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.0797 - accuracy: 0.9804 - val_loss: 1.1347 - val_accuracy: 0.7308\n",
      "Epoch 143/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.0882 - accuracy: 0.9771 - val_loss: 1.1458 - val_accuracy: 0.7308\n",
      "Epoch 144/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.0952 - accuracy: 0.9653 - val_loss: 1.1416 - val_accuracy: 0.7308\n",
      "Epoch 145/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.0661 - accuracy: 0.9767 - val_loss: 1.1562 - val_accuracy: 0.7308\n",
      "Epoch 146/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.0928 - accuracy: 0.9634 - val_loss: 1.1533 - val_accuracy: 0.7372\n",
      "Epoch 147/150\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.0782 - accuracy: 0.9651 - val_loss: 1.1719 - val_accuracy: 0.7308\n",
      "Epoch 148/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.0819 - accuracy: 0.9753 - val_loss: 1.1570 - val_accuracy: 0.7372\n",
      "Epoch 149/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.0631 - accuracy: 0.9848 - val_loss: 1.1692 - val_accuracy: 0.7308\n",
      "Epoch 150/150\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.0836 - accuracy: 0.9804 - val_loss: 1.1618 - val_accuracy: 0.7436\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f903c710d10>"
      ]
     },
     "execution_count": 12,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create model\n",
    "model=Sequential()\n",
    "model.add(Dense(12,input_dim=24,activation='relu'))\n",
    "model.add(Dense(8,activation='relu'))\n",
    "model.add(Dense(1,activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "model.fit(x,y, validation_split=0.3,epochs=150,batch_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "12f1152f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "12f1152f",
    "outputId": "91240696-1a55-4bb2-a4af-a89b6ece5223"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17/17 [==============================] - 0s 1ms/step - loss: 0.4006 - accuracy: 0.9072\n"
     ]
    }
   ],
   "source": [
    "#accuracy of model\n",
    "scores=model.evaluate(x,y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cff4b7a0",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cff4b7a0",
    "outputId": "f0174306-9e69-4dbf-9840-b709ebd64279"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 90.72%\n"
     ]
    }
   ],
   "source": [
    "print(\"%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d663a13",
   "metadata": {
    "id": "1d663a13"
   },
   "source": [
    "# iteration 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "722f8c8a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "722f8c8a",
    "outputId": "527c60fb-20b1-4176-fed6-f7a4f7264a0d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "25/25 [==============================] - 1s 9ms/step - loss: 1.3950 - accuracy: 0.7316 - val_loss: 0.6563 - val_accuracy: 0.6731\n",
      "Epoch 2/100\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.5704 - accuracy: 0.7670 - val_loss: 0.6396 - val_accuracy: 0.6731\n",
      "Epoch 3/100\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.5689 - accuracy: 0.7651 - val_loss: 0.6390 - val_accuracy: 0.6731\n",
      "Epoch 4/100\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.5847 - accuracy: 0.7488 - val_loss: 0.6395 - val_accuracy: 0.6731\n",
      "Epoch 5/100\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.5812 - accuracy: 0.7540 - val_loss: 0.6396 - val_accuracy: 0.6731\n",
      "Epoch 6/100\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.5841 - accuracy: 0.7382 - val_loss: 0.6371 - val_accuracy: 0.6731\n",
      "Epoch 7/100\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.5683 - accuracy: 0.7515 - val_loss: 0.6387 - val_accuracy: 0.6731\n",
      "Epoch 8/100\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.5187 - accuracy: 0.7718 - val_loss: 0.6397 - val_accuracy: 0.6731\n",
      "Epoch 9/100\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.5615 - accuracy: 0.7258 - val_loss: 0.6303 - val_accuracy: 0.6731\n",
      "Epoch 10/100\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.5152 - accuracy: 0.7751 - val_loss: 0.6442 - val_accuracy: 0.6731\n",
      "Epoch 11/100\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.5409 - accuracy: 0.7477 - val_loss: 0.6299 - val_accuracy: 0.6731\n",
      "Epoch 12/100\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.5260 - accuracy: 0.7512 - val_loss: 0.6240 - val_accuracy: 0.6731\n",
      "Epoch 13/100\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.4939 - accuracy: 0.7909 - val_loss: 0.6238 - val_accuracy: 0.6795\n",
      "Epoch 14/100\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.4767 - accuracy: 0.7922 - val_loss: 0.6356 - val_accuracy: 0.6795\n",
      "Epoch 15/100\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.5185 - accuracy: 0.7482 - val_loss: 0.6284 - val_accuracy: 0.6795\n",
      "Epoch 16/100\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.5150 - accuracy: 0.7387 - val_loss: 0.6356 - val_accuracy: 0.6795\n",
      "Epoch 17/100\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.5150 - accuracy: 0.7436 - val_loss: 0.6409 - val_accuracy: 0.6795\n",
      "Epoch 18/100\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.5053 - accuracy: 0.7542 - val_loss: 0.6355 - val_accuracy: 0.6859\n",
      "Epoch 19/100\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.4717 - accuracy: 0.7820 - val_loss: 0.6380 - val_accuracy: 0.6859\n",
      "Epoch 20/100\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.4992 - accuracy: 0.7764 - val_loss: 0.6338 - val_accuracy: 0.6859\n",
      "Epoch 21/100\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.4941 - accuracy: 0.7619 - val_loss: 0.6458 - val_accuracy: 0.6859\n",
      "Epoch 22/100\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.4867 - accuracy: 0.7809 - val_loss: 0.6219 - val_accuracy: 0.6923\n",
      "Epoch 23/100\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.4726 - accuracy: 0.7893 - val_loss: 0.7109 - val_accuracy: 0.6923\n",
      "Epoch 24/100\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.4272 - accuracy: 0.7978 - val_loss: 0.7341 - val_accuracy: 0.6923\n",
      "Epoch 25/100\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.4615 - accuracy: 0.7806 - val_loss: 0.7050 - val_accuracy: 0.6923\n",
      "Epoch 26/100\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.4828 - accuracy: 0.7690 - val_loss: 0.7260 - val_accuracy: 0.6923\n",
      "Epoch 27/100\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.4533 - accuracy: 0.7903 - val_loss: 0.7221 - val_accuracy: 0.7051\n",
      "Epoch 28/100\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.4112 - accuracy: 0.8167 - val_loss: 0.7181 - val_accuracy: 0.7051\n",
      "Epoch 29/100\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.3866 - accuracy: 0.8334 - val_loss: 0.7123 - val_accuracy: 0.7051\n",
      "Epoch 30/100\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.3875 - accuracy: 0.8245 - val_loss: 0.7108 - val_accuracy: 0.7051\n",
      "Epoch 31/100\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.4150 - accuracy: 0.8093 - val_loss: 0.7950 - val_accuracy: 0.7051\n",
      "Epoch 32/100\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.4229 - accuracy: 0.7961 - val_loss: 0.9195 - val_accuracy: 0.7051\n",
      "Epoch 33/100\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.4187 - accuracy: 0.7949 - val_loss: 0.9135 - val_accuracy: 0.7051\n",
      "Epoch 34/100\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.4172 - accuracy: 0.8093 - val_loss: 0.9062 - val_accuracy: 0.7051\n",
      "Epoch 35/100\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.4234 - accuracy: 0.7902 - val_loss: 0.9112 - val_accuracy: 0.7051\n",
      "Epoch 36/100\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.4194 - accuracy: 0.8029 - val_loss: 0.9153 - val_accuracy: 0.7051\n",
      "Epoch 37/100\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.4243 - accuracy: 0.7977 - val_loss: 0.9071 - val_accuracy: 0.7051\n",
      "Epoch 38/100\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.3936 - accuracy: 0.8006 - val_loss: 0.8935 - val_accuracy: 0.7051\n",
      "Epoch 39/100\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.3898 - accuracy: 0.8168 - val_loss: 0.8901 - val_accuracy: 0.7051\n",
      "Epoch 40/100\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.4147 - accuracy: 0.7934 - val_loss: 0.9140 - val_accuracy: 0.7051\n",
      "Epoch 41/100\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.3573 - accuracy: 0.8246 - val_loss: 0.8858 - val_accuracy: 0.7051\n",
      "Epoch 42/100\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.4145 - accuracy: 0.8004 - val_loss: 0.8834 - val_accuracy: 0.7244\n",
      "Epoch 43/100\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.3791 - accuracy: 0.8127 - val_loss: 0.9650 - val_accuracy: 0.7051\n",
      "Epoch 44/100\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.3828 - accuracy: 0.8158 - val_loss: 0.9607 - val_accuracy: 0.7179\n",
      "Epoch 45/100\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.3825 - accuracy: 0.8077 - val_loss: 0.8997 - val_accuracy: 0.7244\n",
      "Epoch 46/100\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.3302 - accuracy: 0.8446 - val_loss: 0.9021 - val_accuracy: 0.7244\n",
      "Epoch 47/100\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.4133 - accuracy: 0.7967 - val_loss: 0.9035 - val_accuracy: 0.7372\n",
      "Epoch 48/100\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.3890 - accuracy: 0.7977 - val_loss: 0.9407 - val_accuracy: 0.7372\n",
      "Epoch 49/100\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.3503 - accuracy: 0.8264 - val_loss: 0.9378 - val_accuracy: 0.7372\n",
      "Epoch 50/100\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.3251 - accuracy: 0.8422 - val_loss: 0.8632 - val_accuracy: 0.7500\n",
      "Epoch 51/100\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.3322 - accuracy: 0.8269 - val_loss: 0.9294 - val_accuracy: 0.7500\n",
      "Epoch 52/100\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.3455 - accuracy: 0.8246 - val_loss: 0.8555 - val_accuracy: 0.7564\n",
      "Epoch 53/100\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.3358 - accuracy: 0.8314 - val_loss: 0.9209 - val_accuracy: 0.7564\n",
      "Epoch 54/100\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.3380 - accuracy: 0.8353 - val_loss: 0.8419 - val_accuracy: 0.7628\n",
      "Epoch 55/100\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.3615 - accuracy: 0.8044 - val_loss: 0.8426 - val_accuracy: 0.7628\n",
      "Epoch 56/100\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.3302 - accuracy: 0.8190 - val_loss: 0.8389 - val_accuracy: 0.7628\n",
      "Epoch 57/100\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.3257 - accuracy: 0.8375 - val_loss: 0.9054 - val_accuracy: 0.7628\n",
      "Epoch 58/100\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.3231 - accuracy: 0.8276 - val_loss: 0.8296 - val_accuracy: 0.7692\n",
      "Epoch 59/100\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.3451 - accuracy: 0.8003 - val_loss: 0.9004 - val_accuracy: 0.7628\n",
      "Epoch 60/100\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.2886 - accuracy: 0.8483 - val_loss: 0.8967 - val_accuracy: 0.7692\n",
      "Epoch 61/100\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.3342 - accuracy: 0.8212 - val_loss: 0.8307 - val_accuracy: 0.7692\n",
      "Epoch 62/100\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.3014 - accuracy: 0.8497 - val_loss: 0.8266 - val_accuracy: 0.7692\n",
      "Epoch 63/100\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.3340 - accuracy: 0.8296 - val_loss: 0.8877 - val_accuracy: 0.7692\n",
      "Epoch 64/100\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.2917 - accuracy: 0.8575 - val_loss: 0.8165 - val_accuracy: 0.7756\n",
      "Epoch 65/100\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.2793 - accuracy: 0.8628 - val_loss: 0.8364 - val_accuracy: 0.7756\n",
      "Epoch 66/100\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.2961 - accuracy: 0.8536 - val_loss: 0.8786 - val_accuracy: 0.7756\n",
      "Epoch 67/100\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.3025 - accuracy: 0.8513 - val_loss: 0.8743 - val_accuracy: 0.7756\n",
      "Epoch 68/100\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.3098 - accuracy: 0.8469 - val_loss: 0.8072 - val_accuracy: 0.7756\n",
      "Epoch 69/100\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.2804 - accuracy: 0.8601 - val_loss: 0.8013 - val_accuracy: 0.7756\n",
      "Epoch 70/100\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.3285 - accuracy: 0.8170 - val_loss: 0.7972 - val_accuracy: 0.7821\n",
      "Epoch 71/100\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.2755 - accuracy: 0.8609 - val_loss: 0.8001 - val_accuracy: 0.7821\n",
      "Epoch 72/100\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.3046 - accuracy: 0.8422 - val_loss: 0.8087 - val_accuracy: 0.7821\n",
      "Epoch 73/100\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.2924 - accuracy: 0.8424 - val_loss: 0.7940 - val_accuracy: 0.7949\n",
      "Epoch 74/100\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.2574 - accuracy: 0.8716 - val_loss: 0.7956 - val_accuracy: 0.7885\n",
      "Epoch 75/100\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.2658 - accuracy: 0.8627 - val_loss: 0.8522 - val_accuracy: 0.7821\n",
      "Epoch 76/100\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.3009 - accuracy: 0.8337 - val_loss: 0.8481 - val_accuracy: 0.7885\n",
      "Epoch 77/100\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.2655 - accuracy: 0.8599 - val_loss: 0.8459 - val_accuracy: 0.7949\n",
      "Epoch 78/100\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.2391 - accuracy: 0.8790 - val_loss: 0.8440 - val_accuracy: 0.8013\n",
      "Epoch 79/100\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.2454 - accuracy: 0.8867 - val_loss: 0.8410 - val_accuracy: 0.7949\n",
      "Epoch 80/100\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.2475 - accuracy: 0.8669 - val_loss: 0.7660 - val_accuracy: 0.8077\n",
      "Epoch 81/100\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.2289 - accuracy: 0.8762 - val_loss: 0.7682 - val_accuracy: 0.8013\n",
      "Epoch 82/100\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.2598 - accuracy: 0.8629 - val_loss: 0.8306 - val_accuracy: 0.8013\n",
      "Epoch 83/100\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.2437 - accuracy: 0.8741 - val_loss: 0.8243 - val_accuracy: 0.8013\n",
      "Epoch 84/100\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.2427 - accuracy: 0.8616 - val_loss: 0.7602 - val_accuracy: 0.8141\n",
      "Epoch 85/100\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.2294 - accuracy: 0.8832 - val_loss: 0.7523 - val_accuracy: 0.8141\n",
      "Epoch 86/100\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.2296 - accuracy: 0.8803 - val_loss: 0.7441 - val_accuracy: 0.8269\n",
      "Epoch 87/100\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.2819 - accuracy: 0.8420 - val_loss: 0.7424 - val_accuracy: 0.8077\n",
      "Epoch 88/100\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.2698 - accuracy: 0.8429 - val_loss: 0.7358 - val_accuracy: 0.8269\n",
      "Epoch 89/100\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.2270 - accuracy: 0.8761 - val_loss: 0.7292 - val_accuracy: 0.8269\n",
      "Epoch 90/100\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.2420 - accuracy: 0.8678 - val_loss: 0.7287 - val_accuracy: 0.8205\n",
      "Epoch 91/100\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.2008 - accuracy: 0.8951 - val_loss: 0.7192 - val_accuracy: 0.8333\n",
      "Epoch 92/100\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.2766 - accuracy: 0.8429 - val_loss: 0.7123 - val_accuracy: 0.8333\n",
      "Epoch 93/100\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.1987 - accuracy: 0.8935 - val_loss: 0.7105 - val_accuracy: 0.8269\n",
      "Epoch 94/100\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.2333 - accuracy: 0.8588 - val_loss: 0.7097 - val_accuracy: 0.8526\n",
      "Epoch 95/100\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.1918 - accuracy: 0.8889 - val_loss: 0.7068 - val_accuracy: 0.8462\n",
      "Epoch 96/100\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.2363 - accuracy: 0.8636 - val_loss: 0.7062 - val_accuracy: 0.8526\n",
      "Epoch 97/100\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.2043 - accuracy: 0.8911 - val_loss: 0.7040 - val_accuracy: 0.8590\n",
      "Epoch 98/100\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.2278 - accuracy: 0.8747 - val_loss: 0.6966 - val_accuracy: 0.8526\n",
      "Epoch 99/100\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.2173 - accuracy: 0.8831 - val_loss: 0.6940 - val_accuracy: 0.8590\n",
      "Epoch 100/100\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.2210 - accuracy: 0.8747 - val_loss: 0.7104 - val_accuracy: 0.8590\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f9039ec7350>"
      ]
     },
     "execution_count": 15,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1=Sequential()\n",
    "model1.add(Dense(12,input_dim=24,activation='sigmoid'))\n",
    "model1.add(Dense(8,activation='sigmoid'))\n",
    "model1.add(Dense(1,activation='relu'))\n",
    "model1.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "model1.fit(x, y, validation_split=0.3, epochs=100, batch_size=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "19596f0b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "19596f0b",
    "outputId": "c3b01b2a-296a-4adf-a504-68e63e26f0b5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17/17 [==============================] - 0s 1ms/step - loss: 0.3627 - accuracy: 0.8781\n",
      "accuracy: 87.81%\n"
     ]
    }
   ],
   "source": [
    "#model accuracy\n",
    "scores1=model1.evaluate(x,y)\n",
    "print(\"%s: %.2f%%\" % (model1.metrics_names[1], scores1[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "512e4ee9",
   "metadata": {
    "id": "512e4ee9"
   },
   "source": [
    "# iteration 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "49fa6b0b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "49fa6b0b",
    "outputId": "2885f4c2-2597-4f1c-b6f2-c4d4c0d79f54"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "25/25 [==============================] - 1s 9ms/step - loss: 2.0258 - accuracy: 0.6960 - val_loss: 3.3340 - val_accuracy: 0.6474\n",
      "Epoch 2/100\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 2.0306 - accuracy: 0.6628 - val_loss: 3.2782 - val_accuracy: 0.6346\n",
      "Epoch 3/100\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 1.9884 - accuracy: 0.7113 - val_loss: 3.1643 - val_accuracy: 0.6410\n",
      "Epoch 4/100\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 2.1498 - accuracy: 0.6654 - val_loss: 3.1674 - val_accuracy: 0.6410\n",
      "Epoch 5/100\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 1.7181 - accuracy: 0.6990 - val_loss: 3.2313 - val_accuracy: 0.6410\n",
      "Epoch 6/100\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 1.7864 - accuracy: 0.7265 - val_loss: 3.2244 - val_accuracy: 0.6474\n",
      "Epoch 7/100\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 2.0918 - accuracy: 0.7432 - val_loss: 3.2270 - val_accuracy: 0.6474\n",
      "Epoch 8/100\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 2.0974 - accuracy: 0.7298 - val_loss: 3.1064 - val_accuracy: 0.6474\n",
      "Epoch 9/100\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 2.0905 - accuracy: 0.7540 - val_loss: 3.0993 - val_accuracy: 0.6474\n",
      "Epoch 10/100\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 1.6169 - accuracy: 0.7578 - val_loss: 3.0775 - val_accuracy: 0.6474\n",
      "Epoch 11/100\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 1.8414 - accuracy: 0.7405 - val_loss: 3.0025 - val_accuracy: 0.6474\n",
      "Epoch 12/100\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 1.7598 - accuracy: 0.7419 - val_loss: 3.1136 - val_accuracy: 0.6795\n",
      "Epoch 13/100\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 1.8615 - accuracy: 0.7328 - val_loss: 3.1102 - val_accuracy: 0.6795\n",
      "Epoch 14/100\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 1.9517 - accuracy: 0.7529 - val_loss: 3.0342 - val_accuracy: 0.6923\n",
      "Epoch 15/100\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 2.0476 - accuracy: 0.7492 - val_loss: 3.0290 - val_accuracy: 0.6923\n",
      "Epoch 16/100\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 1.9444 - accuracy: 0.7529 - val_loss: 3.0276 - val_accuracy: 0.6923\n",
      "Epoch 17/100\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 1.9515 - accuracy: 0.7667 - val_loss: 3.0928 - val_accuracy: 0.6987\n",
      "Epoch 18/100\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 1.8754 - accuracy: 0.7729 - val_loss: 3.0116 - val_accuracy: 0.6987\n",
      "Epoch 19/100\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 1.5217 - accuracy: 0.7653 - val_loss: 3.0177 - val_accuracy: 0.6987\n",
      "Epoch 20/100\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 1.9395 - accuracy: 0.7456 - val_loss: 3.0280 - val_accuracy: 0.6987\n",
      "Epoch 21/100\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 1.8200 - accuracy: 0.7590 - val_loss: 3.1083 - val_accuracy: 0.6987\n",
      "Epoch 22/100\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 2.0031 - accuracy: 0.7629 - val_loss: 3.0929 - val_accuracy: 0.6987\n",
      "Epoch 23/100\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 1.7920 - accuracy: 0.7870 - val_loss: 3.0970 - val_accuracy: 0.6987\n",
      "Epoch 24/100\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 1.6251 - accuracy: 0.7757 - val_loss: 3.0922 - val_accuracy: 0.6987\n",
      "Epoch 25/100\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 1.8906 - accuracy: 0.7855 - val_loss: 3.0903 - val_accuracy: 0.6987\n",
      "Epoch 26/100\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 1.8200 - accuracy: 0.7839 - val_loss: 3.0692 - val_accuracy: 0.6923\n",
      "Epoch 27/100\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 2.0677 - accuracy: 0.7830 - val_loss: 2.4448 - val_accuracy: 0.6795\n",
      "Epoch 28/100\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 1.2818 - accuracy: 0.7697 - val_loss: 1.9937 - val_accuracy: 0.6667\n",
      "Epoch 29/100\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 1.0409 - accuracy: 0.7818 - val_loss: 1.7949 - val_accuracy: 0.6795\n",
      "Epoch 30/100\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.8638 - accuracy: 0.8066 - val_loss: 1.8644 - val_accuracy: 0.6667\n",
      "Epoch 31/100\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 1.2497 - accuracy: 0.7828 - val_loss: 1.8786 - val_accuracy: 0.6731\n",
      "Epoch 32/100\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.9246 - accuracy: 0.7941 - val_loss: 1.8795 - val_accuracy: 0.6859\n",
      "Epoch 33/100\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 1.1158 - accuracy: 0.7760 - val_loss: 1.8874 - val_accuracy: 0.6923\n",
      "Epoch 34/100\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 1.1779 - accuracy: 0.7525 - val_loss: 1.8901 - val_accuracy: 0.6859\n",
      "Epoch 35/100\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 1.1792 - accuracy: 0.7638 - val_loss: 1.8936 - val_accuracy: 0.6859\n",
      "Epoch 36/100\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 1.2822 - accuracy: 0.7838 - val_loss: 1.8170 - val_accuracy: 0.6923\n",
      "Epoch 37/100\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 1.0219 - accuracy: 0.7775 - val_loss: 1.8146 - val_accuracy: 0.6987\n",
      "Epoch 38/100\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 1.2221 - accuracy: 0.7736 - val_loss: 1.8278 - val_accuracy: 0.6923\n",
      "Epoch 39/100\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 1.0266 - accuracy: 0.7958 - val_loss: 1.8616 - val_accuracy: 0.6923\n",
      "Epoch 40/100\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.9264 - accuracy: 0.7797 - val_loss: 2.1178 - val_accuracy: 0.6923\n",
      "Epoch 41/100\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.9303 - accuracy: 0.8004 - val_loss: 2.0517 - val_accuracy: 0.6859\n",
      "Epoch 42/100\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 1.3634 - accuracy: 0.7678 - val_loss: 1.9895 - val_accuracy: 0.6859\n",
      "Epoch 43/100\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 1.2362 - accuracy: 0.7780 - val_loss: 2.1322 - val_accuracy: 0.6795\n",
      "Epoch 44/100\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 1.0022 - accuracy: 0.8042 - val_loss: 2.1231 - val_accuracy: 0.6859\n",
      "Epoch 45/100\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 1.2100 - accuracy: 0.7893 - val_loss: 2.1251 - val_accuracy: 0.6859\n",
      "Epoch 46/100\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.9872 - accuracy: 0.8028 - val_loss: 2.1257 - val_accuracy: 0.6859\n",
      "Epoch 47/100\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 1.0148 - accuracy: 0.7947 - val_loss: 2.2030 - val_accuracy: 0.6859\n",
      "Epoch 48/100\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 1.2720 - accuracy: 0.7868 - val_loss: 2.2645 - val_accuracy: 0.6859\n",
      "Epoch 49/100\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 1.0982 - accuracy: 0.7988 - val_loss: 2.2642 - val_accuracy: 0.6859\n",
      "Epoch 50/100\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 1.1200 - accuracy: 0.7472 - val_loss: 2.2655 - val_accuracy: 0.6859\n",
      "Epoch 51/100\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 1.1294 - accuracy: 0.7772 - val_loss: 2.2668 - val_accuracy: 0.6795\n",
      "Epoch 52/100\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.9381 - accuracy: 0.8234 - val_loss: 2.2650 - val_accuracy: 0.6795\n",
      "Epoch 53/100\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 1.0059 - accuracy: 0.7961 - val_loss: 2.1136 - val_accuracy: 0.6923\n",
      "Epoch 54/100\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.8446 - accuracy: 0.8145 - val_loss: 2.1116 - val_accuracy: 0.7051\n",
      "Epoch 55/100\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.9888 - accuracy: 0.7981 - val_loss: 2.0510 - val_accuracy: 0.7051\n",
      "Epoch 56/100\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 1.0183 - accuracy: 0.8201 - val_loss: 2.1112 - val_accuracy: 0.7051\n",
      "Epoch 57/100\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.9540 - accuracy: 0.7929 - val_loss: 2.1107 - val_accuracy: 0.7051\n",
      "Epoch 58/100\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 1.1646 - accuracy: 0.7997 - val_loss: 2.1158 - val_accuracy: 0.7051\n",
      "Epoch 59/100\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.8889 - accuracy: 0.8382 - val_loss: 2.1103 - val_accuracy: 0.7051\n",
      "Epoch 60/100\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.8376 - accuracy: 0.8247 - val_loss: 2.0310 - val_accuracy: 0.7051\n",
      "Epoch 61/100\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.8955 - accuracy: 0.8057 - val_loss: 2.0243 - val_accuracy: 0.7115\n",
      "Epoch 62/100\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 1.0505 - accuracy: 0.7963 - val_loss: 2.0244 - val_accuracy: 0.7115\n",
      "Epoch 63/100\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.9999 - accuracy: 0.8378 - val_loss: 2.0123 - val_accuracy: 0.6987\n",
      "Epoch 64/100\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 1.0290 - accuracy: 0.8052 - val_loss: 1.8607 - val_accuracy: 0.6923\n",
      "Epoch 65/100\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 1.1177 - accuracy: 0.8119 - val_loss: 2.0031 - val_accuracy: 0.6987\n",
      "Epoch 66/100\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 1.0139 - accuracy: 0.8179 - val_loss: 2.0095 - val_accuracy: 0.7051\n",
      "Epoch 67/100\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.9857 - accuracy: 0.8070 - val_loss: 2.0162 - val_accuracy: 0.6987\n",
      "Epoch 68/100\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 1.1847 - accuracy: 0.8135 - val_loss: 2.0779 - val_accuracy: 0.6923\n",
      "Epoch 69/100\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 1.0822 - accuracy: 0.8175 - val_loss: 2.0755 - val_accuracy: 0.6987\n",
      "Epoch 70/100\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.8213 - accuracy: 0.8358 - val_loss: 2.0034 - val_accuracy: 0.6923\n",
      "Epoch 71/100\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 1.1260 - accuracy: 0.8126 - val_loss: 1.9967 - val_accuracy: 0.6667\n",
      "Epoch 72/100\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 1.1594 - accuracy: 0.8098 - val_loss: 1.9939 - val_accuracy: 0.6795\n",
      "Epoch 73/100\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 1.1972 - accuracy: 0.7688 - val_loss: 1.9924 - val_accuracy: 0.6667\n",
      "Epoch 74/100\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.9149 - accuracy: 0.8133 - val_loss: 1.9920 - val_accuracy: 0.6731\n",
      "Epoch 75/100\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.8574 - accuracy: 0.8321 - val_loss: 1.9865 - val_accuracy: 0.6731\n",
      "Epoch 76/100\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.9102 - accuracy: 0.8212 - val_loss: 1.9856 - val_accuracy: 0.6731\n",
      "Epoch 77/100\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.7544 - accuracy: 0.8420 - val_loss: 1.9882 - val_accuracy: 0.6731\n",
      "Epoch 78/100\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.9193 - accuracy: 0.8357 - val_loss: 1.9844 - val_accuracy: 0.6731\n",
      "Epoch 79/100\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 1.0292 - accuracy: 0.8393 - val_loss: 1.9845 - val_accuracy: 0.6731\n",
      "Epoch 80/100\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.9105 - accuracy: 0.8413 - val_loss: 1.9773 - val_accuracy: 0.6795\n",
      "Epoch 81/100\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.9410 - accuracy: 0.8388 - val_loss: 1.9788 - val_accuracy: 0.6795\n",
      "Epoch 82/100\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 1.0559 - accuracy: 0.8325 - val_loss: 1.9798 - val_accuracy: 0.6731\n",
      "Epoch 83/100\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 1.1141 - accuracy: 0.8423 - val_loss: 1.9798 - val_accuracy: 0.6795\n",
      "Epoch 84/100\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.9272 - accuracy: 0.8241 - val_loss: 1.9824 - val_accuracy: 0.6795\n",
      "Epoch 85/100\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 1.0876 - accuracy: 0.8507 - val_loss: 1.9819 - val_accuracy: 0.6731\n",
      "Epoch 86/100\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.9555 - accuracy: 0.8341 - val_loss: 1.9801 - val_accuracy: 0.6731\n",
      "Epoch 87/100\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.9499 - accuracy: 0.8325 - val_loss: 1.9876 - val_accuracy: 0.6795\n",
      "Epoch 88/100\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 1.3963 - accuracy: 0.8284 - val_loss: 1.9845 - val_accuracy: 0.6923\n",
      "Epoch 89/100\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 1.2736 - accuracy: 0.8469 - val_loss: 1.9873 - val_accuracy: 0.6923\n",
      "Epoch 90/100\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 1.0212 - accuracy: 0.8516 - val_loss: 1.9930 - val_accuracy: 0.6923\n",
      "Epoch 91/100\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 1.1029 - accuracy: 0.8323 - val_loss: 1.9644 - val_accuracy: 0.6923\n",
      "Epoch 92/100\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.8141 - accuracy: 0.8599 - val_loss: 2.0000 - val_accuracy: 0.6923\n",
      "Epoch 93/100\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.9252 - accuracy: 0.8482 - val_loss: 2.0277 - val_accuracy: 0.6923\n",
      "Epoch 94/100\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 1.0760 - accuracy: 0.8429 - val_loss: 2.0732 - val_accuracy: 0.6923\n",
      "Epoch 95/100\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 1.1470 - accuracy: 0.8338 - val_loss: 2.0139 - val_accuracy: 0.7051\n",
      "Epoch 96/100\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 1.1479 - accuracy: 0.8485 - val_loss: 2.0034 - val_accuracy: 0.6923\n",
      "Epoch 97/100\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.8743 - accuracy: 0.8724 - val_loss: 1.7518 - val_accuracy: 0.6603\n",
      "Epoch 98/100\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.8607 - accuracy: 0.8628 - val_loss: 1.8143 - val_accuracy: 0.6731\n",
      "Epoch 99/100\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 1.0212 - accuracy: 0.8592 - val_loss: 1.8855 - val_accuracy: 0.6795\n",
      "Epoch 100/100\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 1.1635 - accuracy: 0.8433 - val_loss: 1.8918 - val_accuracy: 0.6667\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f9037ce9150>"
      ]
     },
     "execution_count": 17,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2=Sequential()\n",
    "model2.add(Dense(12,input_dim=24,activation='relu'))\n",
    "model2.add(Dense(8,activation='relu'))\n",
    "model2.add(Dense(1,activation='relu'))\n",
    "model2.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "model2.fit(x,y,epochs=100, validation_split=0.3,batch_size=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4570ae38",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4570ae38",
    "outputId": "b20a4e46-7168-4ab2-b4fc-0c0a14748689"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17/17 [==============================] - 0s 1ms/step - loss: 1.2237 - accuracy: 0.8066\n",
      "accuracy: 80.66%\n"
     ]
    }
   ],
   "source": [
    "#model accuracy\n",
    "scores2=model2.evaluate(x,y)\n",
    "print(\"%s: %.2f%%\" % (model2.metrics_names[1], scores2[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f009635",
   "metadata": {
    "id": "4f009635"
   },
   "source": [
    "# iteration 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "036d92f2",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "036d92f2",
    "outputId": "fcbc8e4c-5593-4bcb-9c37-aed5b62b4e65"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "37/37 [==============================] - 1s 7ms/step - loss: 2.4914 - accuracy: 0.7534 - val_loss: 3.2539 - val_accuracy: 0.6859\n",
      "Epoch 2/150\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 2.6817 - accuracy: 0.7162 - val_loss: 3.1116 - val_accuracy: 0.6731\n",
      "Epoch 3/150\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 1.6127 - accuracy: 0.7541 - val_loss: 2.8821 - val_accuracy: 0.6474\n",
      "Epoch 4/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 2.5043 - accuracy: 0.6969 - val_loss: 3.0944 - val_accuracy: 0.6731\n",
      "Epoch 5/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 2.3126 - accuracy: 0.7714 - val_loss: 3.0934 - val_accuracy: 0.6859\n",
      "Epoch 6/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 2.1228 - accuracy: 0.7424 - val_loss: 3.1746 - val_accuracy: 0.6731\n",
      "Epoch 7/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 2.3228 - accuracy: 0.7568 - val_loss: 3.0972 - val_accuracy: 0.6731\n",
      "Epoch 8/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 1.6779 - accuracy: 0.8122 - val_loss: 2.9403 - val_accuracy: 0.6731\n",
      "Epoch 9/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 2.3696 - accuracy: 0.7654 - val_loss: 2.9432 - val_accuracy: 0.6731\n",
      "Epoch 10/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 1.9338 - accuracy: 0.7933 - val_loss: 2.9520 - val_accuracy: 0.6731\n",
      "Epoch 11/150\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 2.1396 - accuracy: 0.7483 - val_loss: 2.9981 - val_accuracy: 0.6667\n",
      "Epoch 12/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 1.9295 - accuracy: 0.7914 - val_loss: 2.9946 - val_accuracy: 0.6667\n",
      "Epoch 13/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 1.8584 - accuracy: 0.7656 - val_loss: 2.9054 - val_accuracy: 0.6667\n",
      "Epoch 14/150\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 2.2389 - accuracy: 0.7527 - val_loss: 2.8143 - val_accuracy: 0.6667\n",
      "Epoch 15/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 2.1093 - accuracy: 0.7631 - val_loss: 2.6345 - val_accuracy: 0.6731\n",
      "Epoch 16/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 1.7017 - accuracy: 0.7710 - val_loss: 2.1045 - val_accuracy: 0.6667\n",
      "Epoch 17/150\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 1.7667 - accuracy: 0.7844 - val_loss: 2.0968 - val_accuracy: 0.6667\n",
      "Epoch 18/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 1.5144 - accuracy: 0.7843 - val_loss: 2.1638 - val_accuracy: 0.6731\n",
      "Epoch 19/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 1.6163 - accuracy: 0.7729 - val_loss: 2.0911 - val_accuracy: 0.6731\n",
      "Epoch 20/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 1.3315 - accuracy: 0.7863 - val_loss: 1.6495 - val_accuracy: 0.6603\n",
      "Epoch 21/150\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 1.3554 - accuracy: 0.8256 - val_loss: 1.3245 - val_accuracy: 0.6731\n",
      "Epoch 22/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 1.2066 - accuracy: 0.8261 - val_loss: 1.3742 - val_accuracy: 0.7051\n",
      "Epoch 23/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.9440 - accuracy: 0.7945 - val_loss: 1.3792 - val_accuracy: 0.7115\n",
      "Epoch 24/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 1.2060 - accuracy: 0.8127 - val_loss: 1.3796 - val_accuracy: 0.7051\n",
      "Epoch 25/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 1.0927 - accuracy: 0.8098 - val_loss: 1.4560 - val_accuracy: 0.7051\n",
      "Epoch 26/150\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 1.2786 - accuracy: 0.8226 - val_loss: 1.4478 - val_accuracy: 0.7115\n",
      "Epoch 27/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.6082 - accuracy: 0.8769 - val_loss: 1.5157 - val_accuracy: 0.7179\n",
      "Epoch 28/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.8434 - accuracy: 0.8244 - val_loss: 1.5083 - val_accuracy: 0.7179\n",
      "Epoch 29/150\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 1.1953 - accuracy: 0.8169 - val_loss: 1.5001 - val_accuracy: 0.7179\n",
      "Epoch 30/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.8426 - accuracy: 0.8421 - val_loss: 1.5049 - val_accuracy: 0.7051\n",
      "Epoch 31/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.7804 - accuracy: 0.8562 - val_loss: 1.4931 - val_accuracy: 0.7244\n",
      "Epoch 32/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.9718 - accuracy: 0.8407 - val_loss: 1.4932 - val_accuracy: 0.6987\n",
      "Epoch 33/150\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.7321 - accuracy: 0.8442 - val_loss: 1.4889 - val_accuracy: 0.6987\n",
      "Epoch 34/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.9768 - accuracy: 0.8153 - val_loss: 1.4892 - val_accuracy: 0.7051\n",
      "Epoch 35/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.8567 - accuracy: 0.8491 - val_loss: 1.4852 - val_accuracy: 0.7115\n",
      "Epoch 36/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.8778 - accuracy: 0.8502 - val_loss: 1.4824 - val_accuracy: 0.7051\n",
      "Epoch 37/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 1.0561 - accuracy: 0.8373 - val_loss: 1.4768 - val_accuracy: 0.7115\n",
      "Epoch 38/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.8666 - accuracy: 0.8640 - val_loss: 1.4660 - val_accuracy: 0.7115\n",
      "Epoch 39/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.8208 - accuracy: 0.8649 - val_loss: 1.4666 - val_accuracy: 0.7115\n",
      "Epoch 40/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.9251 - accuracy: 0.8796 - val_loss: 1.4697 - val_accuracy: 0.7051\n",
      "Epoch 41/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.9073 - accuracy: 0.8450 - val_loss: 1.4654 - val_accuracy: 0.7051\n",
      "Epoch 42/150\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 1.0916 - accuracy: 0.8250 - val_loss: 1.4614 - val_accuracy: 0.7115\n",
      "Epoch 43/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.8347 - accuracy: 0.8613 - val_loss: 1.4534 - val_accuracy: 0.6987\n",
      "Epoch 44/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 1.0534 - accuracy: 0.8471 - val_loss: 1.4577 - val_accuracy: 0.7051\n",
      "Epoch 45/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 1.0422 - accuracy: 0.8346 - val_loss: 1.4578 - val_accuracy: 0.7051\n",
      "Epoch 46/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.8047 - accuracy: 0.8662 - val_loss: 1.4474 - val_accuracy: 0.7051\n",
      "Epoch 47/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.8654 - accuracy: 0.8469 - val_loss: 1.3798 - val_accuracy: 0.6987\n",
      "Epoch 48/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 1.0542 - accuracy: 0.8407 - val_loss: 1.4495 - val_accuracy: 0.7115\n",
      "Epoch 49/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.8001 - accuracy: 0.8700 - val_loss: 1.4424 - val_accuracy: 0.7179\n",
      "Epoch 50/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.8533 - accuracy: 0.8720 - val_loss: 1.4455 - val_accuracy: 0.7115\n",
      "Epoch 51/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.9482 - accuracy: 0.8566 - val_loss: 1.4640 - val_accuracy: 0.7115\n",
      "Epoch 52/150\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 1.0283 - accuracy: 0.8678 - val_loss: 1.3880 - val_accuracy: 0.7051\n",
      "Epoch 53/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.7410 - accuracy: 0.8995 - val_loss: 1.4940 - val_accuracy: 0.7115\n",
      "Epoch 54/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.8547 - accuracy: 0.8993 - val_loss: 1.4553 - val_accuracy: 0.7051\n",
      "Epoch 55/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 1.0101 - accuracy: 0.8712 - val_loss: 1.4385 - val_accuracy: 0.7372\n",
      "Epoch 56/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.8404 - accuracy: 0.8971 - val_loss: 1.4529 - val_accuracy: 0.7115\n",
      "Epoch 57/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 1.1234 - accuracy: 0.8695 - val_loss: 1.4425 - val_accuracy: 0.7051\n",
      "Epoch 58/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.9004 - accuracy: 0.8770 - val_loss: 1.3612 - val_accuracy: 0.7500\n",
      "Epoch 59/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.7552 - accuracy: 0.8949 - val_loss: 1.4346 - val_accuracy: 0.7308\n",
      "Epoch 60/150\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.6865 - accuracy: 0.9151 - val_loss: 1.4192 - val_accuracy: 0.7564\n",
      "Epoch 61/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 1.1141 - accuracy: 0.8707 - val_loss: 1.4256 - val_accuracy: 0.7564\n",
      "Epoch 62/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.8337 - accuracy: 0.8898 - val_loss: 1.3694 - val_accuracy: 0.7564\n",
      "Epoch 63/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 1.0325 - accuracy: 0.8775 - val_loss: 1.4427 - val_accuracy: 0.7564\n",
      "Epoch 64/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.9060 - accuracy: 0.8865 - val_loss: 1.4335 - val_accuracy: 0.7436\n",
      "Epoch 65/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.9139 - accuracy: 0.8768 - val_loss: 1.4381 - val_accuracy: 0.7500\n",
      "Epoch 66/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.8561 - accuracy: 0.8915 - val_loss: 1.4396 - val_accuracy: 0.7756\n",
      "Epoch 67/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.8577 - accuracy: 0.8950 - val_loss: 1.4475 - val_accuracy: 0.7756\n",
      "Epoch 68/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.7243 - accuracy: 0.9136 - val_loss: 1.5220 - val_accuracy: 0.7436\n",
      "Epoch 69/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 1.0506 - accuracy: 0.9027 - val_loss: 1.5371 - val_accuracy: 0.7308\n",
      "Epoch 70/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.8870 - accuracy: 0.9082 - val_loss: 1.5352 - val_accuracy: 0.7436\n",
      "Epoch 71/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 1.1479 - accuracy: 0.8847 - val_loss: 1.5298 - val_accuracy: 0.7436\n",
      "Epoch 72/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 1.0192 - accuracy: 0.8828 - val_loss: 1.5370 - val_accuracy: 0.7885\n",
      "Epoch 73/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.7887 - accuracy: 0.9055 - val_loss: 1.6038 - val_accuracy: 0.7436\n",
      "Epoch 74/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.9203 - accuracy: 0.8909 - val_loss: 1.5736 - val_accuracy: 0.7564\n",
      "Epoch 75/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 1.0331 - accuracy: 0.8843 - val_loss: 1.7003 - val_accuracy: 0.7564\n",
      "Epoch 76/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.8065 - accuracy: 0.8877 - val_loss: 1.9572 - val_accuracy: 0.7436\n",
      "Epoch 77/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 1.1672 - accuracy: 0.8806 - val_loss: 1.8580 - val_accuracy: 0.7564\n",
      "Epoch 78/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.6061 - accuracy: 0.9198 - val_loss: 1.7940 - val_accuracy: 0.7756\n",
      "Epoch 79/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.7511 - accuracy: 0.9060 - val_loss: 1.7988 - val_accuracy: 0.7756\n",
      "Epoch 80/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.8259 - accuracy: 0.9219 - val_loss: 1.9503 - val_accuracy: 0.7436\n",
      "Epoch 81/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.9042 - accuracy: 0.8944 - val_loss: 1.9550 - val_accuracy: 0.7500\n",
      "Epoch 82/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.6964 - accuracy: 0.9261 - val_loss: 2.0324 - val_accuracy: 0.7436\n",
      "Epoch 83/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 1.1180 - accuracy: 0.8843 - val_loss: 1.9540 - val_accuracy: 0.7756\n",
      "Epoch 84/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.7471 - accuracy: 0.9143 - val_loss: 1.9669 - val_accuracy: 0.7500\n",
      "Epoch 85/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.8479 - accuracy: 0.9165 - val_loss: 2.0401 - val_accuracy: 0.7564\n",
      "Epoch 86/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.9136 - accuracy: 0.9184 - val_loss: 2.0747 - val_accuracy: 0.7821\n",
      "Epoch 87/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.7169 - accuracy: 0.9233 - val_loss: 1.9845 - val_accuracy: 0.7756\n",
      "Epoch 88/150\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.5962 - accuracy: 0.9198 - val_loss: 2.0323 - val_accuracy: 0.7756\n",
      "Epoch 89/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.7429 - accuracy: 0.9177 - val_loss: 1.9566 - val_accuracy: 0.7692\n",
      "Epoch 90/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.9630 - accuracy: 0.8995 - val_loss: 1.9838 - val_accuracy: 0.7692\n",
      "Epoch 91/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 1.2217 - accuracy: 0.8754 - val_loss: 1.9000 - val_accuracy: 0.7564\n",
      "Epoch 92/150\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 1.2986 - accuracy: 0.8882 - val_loss: 2.0445 - val_accuracy: 0.7628\n",
      "Epoch 93/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.5229 - accuracy: 0.9353 - val_loss: 2.2038 - val_accuracy: 0.7692\n",
      "Epoch 94/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.6317 - accuracy: 0.9279 - val_loss: 1.9977 - val_accuracy: 0.7500\n",
      "Epoch 95/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.6105 - accuracy: 0.9302 - val_loss: 2.1876 - val_accuracy: 0.7885\n",
      "Epoch 96/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.6811 - accuracy: 0.9116 - val_loss: 2.1368 - val_accuracy: 0.7564\n",
      "Epoch 97/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.9456 - accuracy: 0.9143 - val_loss: 2.1329 - val_accuracy: 0.7628\n",
      "Epoch 98/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.9037 - accuracy: 0.9177 - val_loss: 2.1915 - val_accuracy: 0.7564\n",
      "Epoch 99/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.8732 - accuracy: 0.9119 - val_loss: 2.0389 - val_accuracy: 0.7885\n",
      "Epoch 100/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.9364 - accuracy: 0.9147 - val_loss: 2.0407 - val_accuracy: 0.7564\n",
      "Epoch 101/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 1.0814 - accuracy: 0.9002 - val_loss: 1.9555 - val_accuracy: 0.7692\n",
      "Epoch 102/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.6221 - accuracy: 0.9339 - val_loss: 2.1014 - val_accuracy: 0.7500\n",
      "Epoch 103/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.7744 - accuracy: 0.9369 - val_loss: 2.1007 - val_accuracy: 0.7436\n",
      "Epoch 104/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.7170 - accuracy: 0.9462 - val_loss: 2.0180 - val_accuracy: 0.7628\n",
      "Epoch 105/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 1.1193 - accuracy: 0.9009 - val_loss: 2.0753 - val_accuracy: 0.7821\n",
      "Epoch 106/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.6056 - accuracy: 0.9295 - val_loss: 2.0485 - val_accuracy: 0.7756\n",
      "Epoch 107/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.8133 - accuracy: 0.9179 - val_loss: 2.0510 - val_accuracy: 0.7564\n",
      "Epoch 108/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.9219 - accuracy: 0.9207 - val_loss: 1.9424 - val_accuracy: 0.7564\n",
      "Epoch 109/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 1.0160 - accuracy: 0.9048 - val_loss: 1.9360 - val_accuracy: 0.7756\n",
      "Epoch 110/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.9060 - accuracy: 0.9226 - val_loss: 1.9317 - val_accuracy: 0.7821\n",
      "Epoch 111/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.7851 - accuracy: 0.9160 - val_loss: 1.9394 - val_accuracy: 0.7564\n",
      "Epoch 112/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.7036 - accuracy: 0.9336 - val_loss: 1.9366 - val_accuracy: 0.7821\n",
      "Epoch 113/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.8075 - accuracy: 0.9321 - val_loss: 1.9023 - val_accuracy: 0.7821\n",
      "Epoch 114/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.6632 - accuracy: 0.9312 - val_loss: 1.9159 - val_accuracy: 0.7692\n",
      "Epoch 115/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.8749 - accuracy: 0.9073 - val_loss: 2.0008 - val_accuracy: 0.7821\n",
      "Epoch 116/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.8109 - accuracy: 0.9129 - val_loss: 1.9928 - val_accuracy: 0.7885\n",
      "Epoch 117/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.8470 - accuracy: 0.9132 - val_loss: 1.9985 - val_accuracy: 0.7885\n",
      "Epoch 118/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.9197 - accuracy: 0.9120 - val_loss: 1.9197 - val_accuracy: 0.7821\n",
      "Epoch 119/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.7091 - accuracy: 0.9372 - val_loss: 2.0357 - val_accuracy: 0.7564\n",
      "Epoch 120/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.8106 - accuracy: 0.9379 - val_loss: 1.9148 - val_accuracy: 0.7756\n",
      "Epoch 121/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 1.1251 - accuracy: 0.9162 - val_loss: 1.9267 - val_accuracy: 0.7821\n",
      "Epoch 122/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.8157 - accuracy: 0.9340 - val_loss: 2.0087 - val_accuracy: 0.7756\n",
      "Epoch 123/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.6687 - accuracy: 0.9449 - val_loss: 2.1218 - val_accuracy: 0.7436\n",
      "Epoch 124/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.7417 - accuracy: 0.9413 - val_loss: 1.9317 - val_accuracy: 0.7628\n",
      "Epoch 125/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.7054 - accuracy: 0.9271 - val_loss: 1.9736 - val_accuracy: 0.7564\n",
      "Epoch 126/150\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 1.1174 - accuracy: 0.9181 - val_loss: 1.9398 - val_accuracy: 0.7692\n",
      "Epoch 127/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.7914 - accuracy: 0.9223 - val_loss: 2.0729 - val_accuracy: 0.7885\n",
      "Epoch 128/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.6503 - accuracy: 0.9343 - val_loss: 2.1924 - val_accuracy: 0.7628\n",
      "Epoch 129/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.7387 - accuracy: 0.9417 - val_loss: 2.0312 - val_accuracy: 0.7692\n",
      "Epoch 130/150\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.7341 - accuracy: 0.9343 - val_loss: 2.0931 - val_accuracy: 0.7500\n",
      "Epoch 131/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.6936 - accuracy: 0.9397 - val_loss: 2.0590 - val_accuracy: 0.7436\n",
      "Epoch 132/150\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.7695 - accuracy: 0.9346 - val_loss: 2.0595 - val_accuracy: 0.7308\n",
      "Epoch 133/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.9934 - accuracy: 0.9238 - val_loss: 2.0884 - val_accuracy: 0.7756\n",
      "Epoch 134/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.7659 - accuracy: 0.9322 - val_loss: 2.0328 - val_accuracy: 0.7564\n",
      "Epoch 135/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 1.0360 - accuracy: 0.9223 - val_loss: 2.0471 - val_accuracy: 0.7436\n",
      "Epoch 136/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.7921 - accuracy: 0.9340 - val_loss: 2.0845 - val_accuracy: 0.7949\n",
      "Epoch 137/150\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.7637 - accuracy: 0.9361 - val_loss: 2.1253 - val_accuracy: 0.7564\n",
      "Epoch 138/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.9402 - accuracy: 0.9180 - val_loss: 2.1003 - val_accuracy: 0.7692\n",
      "Epoch 139/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.8856 - accuracy: 0.9327 - val_loss: 2.1150 - val_accuracy: 0.7756\n",
      "Epoch 140/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 1.1028 - accuracy: 0.9151 - val_loss: 2.1527 - val_accuracy: 0.7436\n",
      "Epoch 141/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.7128 - accuracy: 0.9479 - val_loss: 2.0376 - val_accuracy: 0.7756\n",
      "Epoch 142/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.7780 - accuracy: 0.9443 - val_loss: 2.1114 - val_accuracy: 0.7564\n",
      "Epoch 143/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.6157 - accuracy: 0.9492 - val_loss: 2.3146 - val_accuracy: 0.7115\n",
      "Epoch 144/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.7990 - accuracy: 0.9359 - val_loss: 2.3127 - val_accuracy: 0.7436\n",
      "Epoch 145/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 1.1493 - accuracy: 0.9207 - val_loss: 2.2113 - val_accuracy: 0.7756\n",
      "Epoch 146/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.6952 - accuracy: 0.9470 - val_loss: 2.2893 - val_accuracy: 0.7372\n",
      "Epoch 147/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.8066 - accuracy: 0.9389 - val_loss: 2.3069 - val_accuracy: 0.7436\n",
      "Epoch 148/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.7056 - accuracy: 0.9513 - val_loss: 2.3028 - val_accuracy: 0.7308\n",
      "Epoch 149/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.9897 - accuracy: 0.9224 - val_loss: 2.2287 - val_accuracy: 0.7244\n",
      "Epoch 150/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 1.2120 - accuracy: 0.9046 - val_loss: 2.1053 - val_accuracy: 0.7949\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f9036af1690>"
      ]
     },
     "execution_count": 19,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model3=Sequential()\n",
    "model3.add(Dense(12,input_dim=24,activation='relu'))\n",
    "model3.add(Dense(8,activation='relu'))\n",
    "model3.add(Dense(1,activation='relu'))\n",
    "model3.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "model3.fit(x,y,epochs=150, validation_split=0.3,batch_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8e544618",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8e544618",
    "outputId": "a7908551-e8ef-4356-98fe-0d7c5e15741e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17/17 [==============================] - 0s 1ms/step - loss: 1.1991 - accuracy: 0.8897\n",
      "accuracy: 88.97%\n"
     ]
    }
   ],
   "source": [
    "scores3 = model3.evaluate(x, y)\n",
    "print(\"%s: %.2f%%\" % (model3.metrics_names[1], scores3[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29868fb1",
   "metadata": {
    "id": "29868fb1"
   },
   "outputs": [],
   "source": [
    "# hence here we can analyse that the best of all iteration is first one where accuracy of the system came as 92.65%"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "include_colab_link": true,
   "name": "neural networks forestfires.csv.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
