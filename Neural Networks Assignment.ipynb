{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# forest fires.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: keras in c:\\users\\sahil\\anaconda3\\lib\\site-packages (2.4.3)\n",
      "Requirement already satisfied: pyyaml in c:\\users\\sahil\\anaconda3\\lib\\site-packages (from keras) (5.3.1)\n",
      "Requirement already satisfied: scipy>=0.14 in c:\\users\\sahil\\anaconda3\\lib\\site-packages (from keras) (1.5.2)\n",
      "Requirement already satisfied: h5py in c:\\users\\sahil\\anaconda3\\lib\\site-packages (from keras) (3.1.0)\n",
      "Requirement already satisfied: numpy>=1.9.1 in c:\\users\\sahil\\anaconda3\\lib\\site-packages (from keras) (1.19.2)\n",
      "Requirement already satisfied: tensorflow in c:\\users\\sahil\\anaconda3\\lib\\site-packages (2.5.0)\n",
      "Requirement already satisfied: h5py~=3.1.0 in c:\\users\\sahil\\anaconda3\\lib\\site-packages (from tensorflow) (3.1.0)\n",
      "Requirement already satisfied: tensorflow-estimator<2.6.0,>=2.5.0rc0 in c:\\users\\sahil\\anaconda3\\lib\\site-packages (from tensorflow) (2.5.0)\n",
      "Requirement already satisfied: typing-extensions~=3.7.4 in c:\\users\\sahil\\anaconda3\\lib\\site-packages (from tensorflow) (3.7.4.3)\n",
      "Requirement already satisfied: six~=1.15.0 in c:\\users\\sahil\\anaconda3\\lib\\site-packages (from tensorflow) (1.15.0)\n",
      "Requirement already satisfied: termcolor~=1.1.0 in c:\\users\\sahil\\anaconda3\\lib\\site-packages (from tensorflow) (1.1.0)\n",
      "Requirement already satisfied: opt-einsum~=3.3.0 in c:\\users\\sahil\\anaconda3\\lib\\site-packages (from tensorflow) (3.3.0)\n",
      "Requirement already satisfied: tensorboard~=2.5 in c:\\users\\sahil\\anaconda3\\lib\\site-packages (from tensorflow) (2.5.0)\n",
      "Requirement already satisfied: keras-nightly~=2.5.0.dev in c:\\users\\sahil\\anaconda3\\lib\\site-packages (from tensorflow) (2.5.0.dev2021032900)\n",
      "Requirement already satisfied: flatbuffers~=1.12.0 in c:\\users\\sahil\\anaconda3\\lib\\site-packages (from tensorflow) (1.12)\n",
      "Requirement already satisfied: astunparse~=1.6.3 in c:\\users\\sahil\\anaconda3\\lib\\site-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: google-pasta~=0.2 in c:\\users\\sahil\\anaconda3\\lib\\site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: protobuf>=3.9.2 in c:\\users\\sahil\\anaconda3\\lib\\site-packages (from tensorflow) (3.15.5)\n",
      "Requirement already satisfied: absl-py~=0.10 in c:\\users\\sahil\\anaconda3\\lib\\site-packages (from tensorflow) (0.13.0)\n",
      "Requirement already satisfied: wrapt~=1.12.1 in c:\\users\\sahil\\anaconda3\\lib\\site-packages (from tensorflow) (1.12.1)\n",
      "Requirement already satisfied: grpcio~=1.34.0 in c:\\users\\sahil\\anaconda3\\lib\\site-packages (from tensorflow) (1.34.1)\n",
      "Requirement already satisfied: keras-preprocessing~=1.1.2 in c:\\users\\sahil\\anaconda3\\lib\\site-packages (from tensorflow) (1.1.2)\n",
      "Requirement already satisfied: gast==0.4.0 in c:\\users\\sahil\\anaconda3\\lib\\site-packages (from tensorflow) (0.4.0)\n",
      "Requirement already satisfied: numpy~=1.19.2 in c:\\users\\sahil\\anaconda3\\lib\\site-packages (from tensorflow) (1.19.2)\n",
      "Requirement already satisfied: wheel~=0.35 in c:\\users\\sahil\\anaconda3\\lib\\site-packages (from tensorflow) (0.35.1)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in c:\\users\\sahil\\anaconda3\\lib\\site-packages (from tensorboard~=2.5->tensorflow) (50.3.1.post20201107)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in c:\\users\\sahil\\anaconda3\\lib\\site-packages (from tensorboard~=2.5->tensorflow) (0.4.4)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in c:\\users\\sahil\\anaconda3\\lib\\site-packages (from tensorboard~=2.5->tensorflow) (1.0.1)\n",
      "Requirement already satisfied: google-auth<2,>=1.6.3 in c:\\users\\sahil\\anaconda3\\lib\\site-packages (from tensorboard~=2.5->tensorflow) (1.32.0)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\sahil\\anaconda3\\lib\\site-packages (from tensorboard~=2.5->tensorflow) (2.24.0)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in c:\\users\\sahil\\anaconda3\\lib\\site-packages (from tensorboard~=2.5->tensorflow) (1.8.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\sahil\\anaconda3\\lib\\site-packages (from tensorboard~=2.5->tensorflow) (3.3.4)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in c:\\users\\sahil\\anaconda3\\lib\\site-packages (from tensorboard~=2.5->tensorflow) (0.6.1)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in c:\\users\\sahil\\anaconda3\\lib\\site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.5->tensorflow) (1.3.0)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in c:\\users\\sahil\\anaconda3\\lib\\site-packages (from google-auth<2,>=1.6.3->tensorboard~=2.5->tensorflow) (4.2.1)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3.6\" in c:\\users\\sahil\\anaconda3\\lib\\site-packages (from google-auth<2,>=1.6.3->tensorboard~=2.5->tensorflow) (4.7.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\sahil\\anaconda3\\lib\\site-packages (from google-auth<2,>=1.6.3->tensorboard~=2.5->tensorflow) (0.2.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\sahil\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard~=2.5->tensorflow) (2020.6.20)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\users\\sahil\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard~=2.5->tensorflow) (1.25.11)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in c:\\users\\sahil\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard~=2.5->tensorflow) (3.0.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\sahil\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard~=2.5->tensorflow) (2.10)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in c:\\users\\sahil\\anaconda3\\lib\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.5->tensorflow) (3.1.1)\n",
      "Requirement already satisfied: pyasn1>=0.1.3 in c:\\users\\sahil\\anaconda3\\lib\\site-packages (from rsa<5,>=3.1.4; python_version >= \"3.6\"->google-auth<2,>=1.6.3->tensorboard~=2.5->tensorflow) (0.4.8)\n"
     ]
    }
   ],
   "source": [
    "!pip install keras\n",
    "!pip install tensorflow\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>month</th>\n",
       "      <th>day</th>\n",
       "      <th>FFMC</th>\n",
       "      <th>DMC</th>\n",
       "      <th>DC</th>\n",
       "      <th>ISI</th>\n",
       "      <th>temp</th>\n",
       "      <th>RH</th>\n",
       "      <th>wind</th>\n",
       "      <th>rain</th>\n",
       "      <th>...</th>\n",
       "      <th>monthfeb</th>\n",
       "      <th>monthjan</th>\n",
       "      <th>monthjul</th>\n",
       "      <th>monthjun</th>\n",
       "      <th>monthmar</th>\n",
       "      <th>monthmay</th>\n",
       "      <th>monthnov</th>\n",
       "      <th>monthoct</th>\n",
       "      <th>monthsep</th>\n",
       "      <th>size_category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>mar</td>\n",
       "      <td>fri</td>\n",
       "      <td>86.2</td>\n",
       "      <td>26.2</td>\n",
       "      <td>94.3</td>\n",
       "      <td>5.1</td>\n",
       "      <td>8.2</td>\n",
       "      <td>51</td>\n",
       "      <td>6.7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>small</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>oct</td>\n",
       "      <td>tue</td>\n",
       "      <td>90.6</td>\n",
       "      <td>35.4</td>\n",
       "      <td>669.1</td>\n",
       "      <td>6.7</td>\n",
       "      <td>18.0</td>\n",
       "      <td>33</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>small</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>oct</td>\n",
       "      <td>sat</td>\n",
       "      <td>90.6</td>\n",
       "      <td>43.7</td>\n",
       "      <td>686.9</td>\n",
       "      <td>6.7</td>\n",
       "      <td>14.6</td>\n",
       "      <td>33</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>small</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>mar</td>\n",
       "      <td>fri</td>\n",
       "      <td>91.7</td>\n",
       "      <td>33.3</td>\n",
       "      <td>77.5</td>\n",
       "      <td>9.0</td>\n",
       "      <td>8.3</td>\n",
       "      <td>97</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>small</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>mar</td>\n",
       "      <td>sun</td>\n",
       "      <td>89.3</td>\n",
       "      <td>51.3</td>\n",
       "      <td>102.2</td>\n",
       "      <td>9.6</td>\n",
       "      <td>11.4</td>\n",
       "      <td>99</td>\n",
       "      <td>1.8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>small</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>512</th>\n",
       "      <td>aug</td>\n",
       "      <td>sun</td>\n",
       "      <td>81.6</td>\n",
       "      <td>56.7</td>\n",
       "      <td>665.6</td>\n",
       "      <td>1.9</td>\n",
       "      <td>27.8</td>\n",
       "      <td>32</td>\n",
       "      <td>2.7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>large</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>513</th>\n",
       "      <td>aug</td>\n",
       "      <td>sun</td>\n",
       "      <td>81.6</td>\n",
       "      <td>56.7</td>\n",
       "      <td>665.6</td>\n",
       "      <td>1.9</td>\n",
       "      <td>21.9</td>\n",
       "      <td>71</td>\n",
       "      <td>5.8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>large</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>514</th>\n",
       "      <td>aug</td>\n",
       "      <td>sun</td>\n",
       "      <td>81.6</td>\n",
       "      <td>56.7</td>\n",
       "      <td>665.6</td>\n",
       "      <td>1.9</td>\n",
       "      <td>21.2</td>\n",
       "      <td>70</td>\n",
       "      <td>6.7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>large</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>515</th>\n",
       "      <td>aug</td>\n",
       "      <td>sat</td>\n",
       "      <td>94.4</td>\n",
       "      <td>146.0</td>\n",
       "      <td>614.7</td>\n",
       "      <td>11.3</td>\n",
       "      <td>25.6</td>\n",
       "      <td>42</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>small</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>516</th>\n",
       "      <td>nov</td>\n",
       "      <td>tue</td>\n",
       "      <td>79.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>106.7</td>\n",
       "      <td>1.1</td>\n",
       "      <td>11.8</td>\n",
       "      <td>31</td>\n",
       "      <td>4.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>small</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>517 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    month  day  FFMC    DMC     DC   ISI  temp  RH  wind  rain  ...  monthfeb  \\\n",
       "0     mar  fri  86.2   26.2   94.3   5.1   8.2  51   6.7   0.0  ...         0   \n",
       "1     oct  tue  90.6   35.4  669.1   6.7  18.0  33   0.9   0.0  ...         0   \n",
       "2     oct  sat  90.6   43.7  686.9   6.7  14.6  33   1.3   0.0  ...         0   \n",
       "3     mar  fri  91.7   33.3   77.5   9.0   8.3  97   4.0   0.2  ...         0   \n",
       "4     mar  sun  89.3   51.3  102.2   9.6  11.4  99   1.8   0.0  ...         0   \n",
       "..    ...  ...   ...    ...    ...   ...   ...  ..   ...   ...  ...       ...   \n",
       "512   aug  sun  81.6   56.7  665.6   1.9  27.8  32   2.7   0.0  ...         0   \n",
       "513   aug  sun  81.6   56.7  665.6   1.9  21.9  71   5.8   0.0  ...         0   \n",
       "514   aug  sun  81.6   56.7  665.6   1.9  21.2  70   6.7   0.0  ...         0   \n",
       "515   aug  sat  94.4  146.0  614.7  11.3  25.6  42   4.0   0.0  ...         0   \n",
       "516   nov  tue  79.5    3.0  106.7   1.1  11.8  31   4.5   0.0  ...         0   \n",
       "\n",
       "     monthjan  monthjul  monthjun  monthmar  monthmay  monthnov  monthoct  \\\n",
       "0           0         0         0         1         0         0         0   \n",
       "1           0         0         0         0         0         0         1   \n",
       "2           0         0         0         0         0         0         1   \n",
       "3           0         0         0         1         0         0         0   \n",
       "4           0         0         0         1         0         0         0   \n",
       "..        ...       ...       ...       ...       ...       ...       ...   \n",
       "512         0         0         0         0         0         0         0   \n",
       "513         0         0         0         0         0         0         0   \n",
       "514         0         0         0         0         0         0         0   \n",
       "515         0         0         0         0         0         0         0   \n",
       "516         0         0         0         0         0         1         0   \n",
       "\n",
       "     monthsep  size_category  \n",
       "0           0          small  \n",
       "1           0          small  \n",
       "2           0          small  \n",
       "3           0          small  \n",
       "4           0          small  \n",
       "..        ...            ...  \n",
       "512         0          large  \n",
       "513         0          large  \n",
       "514         0          large  \n",
       "515         0          small  \n",
       "516         0          small  \n",
       "\n",
       "[517 rows x 31 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=pd.read_csv('forestfires.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-8.05959472e-01, -1.32332557e+00, -1.83047676e+00, ...,\n",
       "        -4.40225453e-02, -1.72859706e-01, -7.06081245e-01],\n",
       "       [-8.10203395e-03, -1.17954077e+00,  4.88890915e-01, ...,\n",
       "        -4.40225453e-02,  5.78503817e+00, -7.06081245e-01],\n",
       "       [-8.10203395e-03, -1.04982188e+00,  5.60715454e-01, ...,\n",
       "        -4.40225453e-02,  5.78503817e+00, -7.06081245e-01],\n",
       "       ...,\n",
       "       [-1.64008316e+00, -8.46647711e-01,  4.74768113e-01, ...,\n",
       "        -4.40225453e-02, -1.72859706e-01, -7.06081245e-01],\n",
       "       [ 6.80956663e-01,  5.49002541e-01,  2.69382214e-01, ...,\n",
       "        -4.40225453e-02, -1.72859706e-01, -7.06081245e-01],\n",
       "       [-2.02087875e+00, -1.68591332e+00, -1.78044169e+00, ...,\n",
       "         2.27156334e+01, -1.72859706e-01, -7.06081245e-01]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#scaling the numerical data( leaving the target variable )\n",
    "df1=df.iloc[:,2:30]\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc=StandardScaler()\n",
    "df_norm=sc.fit_transform(df1)\n",
    "df_norm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 3.76670947e+00, -1.32025451e+00, -8.43971398e-01, ...,\n",
       "        -6.53345819e-02,  4.98037274e-16, -2.73530281e-16],\n",
       "       [ 3.90786263e-01,  8.31061522e-01, -1.10136513e+00, ...,\n",
       "         3.42618601e-02, -9.55928328e-15,  1.15055466e-15],\n",
       "       [ 6.90415596e-01,  1.17774562e+00, -1.22199841e+00, ...,\n",
       "         2.63235187e-02,  2.58690766e-15, -5.66797432e-17],\n",
       "       ...,\n",
       "       [ 9.21634000e-01, -2.64543072e-01,  2.71921606e+00, ...,\n",
       "        -2.97865814e-01, -1.84247930e-16,  2.36645381e-16],\n",
       "       [-1.62054896e+00, -9.78838231e-01,  3.31987355e-01, ...,\n",
       "         3.91949863e-02, -2.30354869e-16,  2.72058887e-16],\n",
       "       [ 4.07590654e+00, -3.67440726e-01, -2.47151775e-01, ...,\n",
       "        -2.50420726e-02,  5.70142521e-17,  8.50237385e-17]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca=PCA(n_components=28)\n",
    "pca_values=pca.fit_transform(df_norm)\n",
    "pca_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.35522746e-01, 6.85788793e-02, 6.23572652e-02, 5.32713255e-02,\n",
       "       4.75942360e-02, 4.68009902e-02, 4.37490015e-02, 4.28025164e-02,\n",
       "       4.08875728e-02, 4.01633268e-02, 3.92926854e-02, 3.83232321e-02,\n",
       "       3.64221503e-02, 3.63217289e-02, 3.57856782e-02, 3.50087806e-02,\n",
       "       3.35447704e-02, 3.24777366e-02, 3.04490902e-02, 3.00246758e-02,\n",
       "       2.37167400e-02, 2.08329788e-02, 1.18357869e-02, 8.88449559e-03,\n",
       "       4.55347471e-03, 7.98135931e-04, 2.67271490e-32, 1.95971390e-33])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var=pca.explained_variance_ratio_\n",
    "var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.35522746e-01, 6.85788793e-02, 6.23572652e-02, 5.32713255e-02,\n",
       "       4.75942360e-02, 4.68009902e-02, 4.37490015e-02, 4.28025164e-02,\n",
       "       4.08875728e-02, 4.01633268e-02, 3.92926854e-02, 3.83232321e-02,\n",
       "       3.64221503e-02, 3.63217289e-02, 3.57856782e-02, 3.50087806e-02,\n",
       "       3.35447704e-02, 3.24777366e-02, 3.04490902e-02, 3.00246758e-02,\n",
       "       2.37167400e-02, 2.08329788e-02, 1.18357869e-02, 8.88449559e-03,\n",
       "       4.55347471e-03, 7.98135931e-04, 2.67271490e-32, 1.95971390e-33])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var=pca.explained_variance_ratio_\n",
    "var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x19c509b9610>]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(12,8))\n",
    "plt.plot(var,color='red')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pc1</th>\n",
       "      <th>pc2</th>\n",
       "      <th>pc3</th>\n",
       "      <th>pc4</th>\n",
       "      <th>pc5</th>\n",
       "      <th>pc6</th>\n",
       "      <th>pc7</th>\n",
       "      <th>pc8</th>\n",
       "      <th>pc9</th>\n",
       "      <th>pc10</th>\n",
       "      <th>...</th>\n",
       "      <th>pc16</th>\n",
       "      <th>pc17</th>\n",
       "      <th>pc18</th>\n",
       "      <th>pc19</th>\n",
       "      <th>pc20</th>\n",
       "      <th>pc21</th>\n",
       "      <th>pc22</th>\n",
       "      <th>pc23</th>\n",
       "      <th>pc24</th>\n",
       "      <th>size_category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3.766709</td>\n",
       "      <td>-1.320255</td>\n",
       "      <td>-0.843971</td>\n",
       "      <td>-1.994738</td>\n",
       "      <td>-1.453359</td>\n",
       "      <td>0.693985</td>\n",
       "      <td>0.308104</td>\n",
       "      <td>-0.019764</td>\n",
       "      <td>0.010161</td>\n",
       "      <td>-0.437314</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.197543</td>\n",
       "      <td>-0.021839</td>\n",
       "      <td>0.688958</td>\n",
       "      <td>0.563603</td>\n",
       "      <td>-0.439596</td>\n",
       "      <td>-0.926619</td>\n",
       "      <td>-0.405425</td>\n",
       "      <td>-0.118719</td>\n",
       "      <td>-0.017933</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.390786</td>\n",
       "      <td>0.831062</td>\n",
       "      <td>-1.101365</td>\n",
       "      <td>1.400671</td>\n",
       "      <td>2.869388</td>\n",
       "      <td>0.965898</td>\n",
       "      <td>-2.795574</td>\n",
       "      <td>0.041095</td>\n",
       "      <td>-0.548879</td>\n",
       "      <td>0.104500</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.503167</td>\n",
       "      <td>0.499649</td>\n",
       "      <td>0.563706</td>\n",
       "      <td>-0.703319</td>\n",
       "      <td>-1.535718</td>\n",
       "      <td>-0.892995</td>\n",
       "      <td>0.836590</td>\n",
       "      <td>0.204975</td>\n",
       "      <td>0.290771</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.690416</td>\n",
       "      <td>1.177746</td>\n",
       "      <td>-1.221998</td>\n",
       "      <td>2.442038</td>\n",
       "      <td>1.090630</td>\n",
       "      <td>0.390801</td>\n",
       "      <td>-1.586675</td>\n",
       "      <td>-2.159336</td>\n",
       "      <td>-0.090580</td>\n",
       "      <td>0.260888</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.545144</td>\n",
       "      <td>-0.658411</td>\n",
       "      <td>-0.423618</td>\n",
       "      <td>0.860550</td>\n",
       "      <td>-1.195230</td>\n",
       "      <td>-0.297870</td>\n",
       "      <td>0.743648</td>\n",
       "      <td>0.081757</td>\n",
       "      <td>0.345915</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.359951</td>\n",
       "      <td>-1.161443</td>\n",
       "      <td>0.385728</td>\n",
       "      <td>-2.118328</td>\n",
       "      <td>-1.949601</td>\n",
       "      <td>1.027664</td>\n",
       "      <td>-0.179422</td>\n",
       "      <td>-0.250227</td>\n",
       "      <td>-0.620329</td>\n",
       "      <td>-1.343189</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.040887</td>\n",
       "      <td>0.017843</td>\n",
       "      <td>0.332572</td>\n",
       "      <td>1.164745</td>\n",
       "      <td>-1.632741</td>\n",
       "      <td>-0.817618</td>\n",
       "      <td>1.523710</td>\n",
       "      <td>-0.342302</td>\n",
       "      <td>-0.378420</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.974329</td>\n",
       "      <td>-0.842626</td>\n",
       "      <td>1.327788</td>\n",
       "      <td>0.038086</td>\n",
       "      <td>-1.124763</td>\n",
       "      <td>-0.574676</td>\n",
       "      <td>-0.777155</td>\n",
       "      <td>0.303635</td>\n",
       "      <td>0.861126</td>\n",
       "      <td>-2.024719</td>\n",
       "      <td>...</td>\n",
       "      <td>0.844431</td>\n",
       "      <td>1.014944</td>\n",
       "      <td>-0.618231</td>\n",
       "      <td>0.822853</td>\n",
       "      <td>-1.794109</td>\n",
       "      <td>-0.723371</td>\n",
       "      <td>2.020419</td>\n",
       "      <td>-0.545591</td>\n",
       "      <td>0.161735</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>512</th>\n",
       "      <td>-0.087560</td>\n",
       "      <td>0.153964</td>\n",
       "      <td>1.241810</td>\n",
       "      <td>1.536581</td>\n",
       "      <td>0.372425</td>\n",
       "      <td>-1.133422</td>\n",
       "      <td>-0.362287</td>\n",
       "      <td>0.766946</td>\n",
       "      <td>0.818745</td>\n",
       "      <td>-0.289632</td>\n",
       "      <td>...</td>\n",
       "      <td>0.300522</td>\n",
       "      <td>0.513876</td>\n",
       "      <td>0.539642</td>\n",
       "      <td>-0.052958</td>\n",
       "      <td>1.898628</td>\n",
       "      <td>-1.441786</td>\n",
       "      <td>-0.821192</td>\n",
       "      <td>-1.205707</td>\n",
       "      <td>-0.698666</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>513</th>\n",
       "      <td>0.794366</td>\n",
       "      <td>-0.083966</td>\n",
       "      <td>2.670485</td>\n",
       "      <td>0.284995</td>\n",
       "      <td>0.223323</td>\n",
       "      <td>-0.904232</td>\n",
       "      <td>-0.014849</td>\n",
       "      <td>0.107226</td>\n",
       "      <td>1.340049</td>\n",
       "      <td>-0.147246</td>\n",
       "      <td>...</td>\n",
       "      <td>0.342367</td>\n",
       "      <td>0.485571</td>\n",
       "      <td>0.580150</td>\n",
       "      <td>0.384984</td>\n",
       "      <td>0.086251</td>\n",
       "      <td>-0.970693</td>\n",
       "      <td>-1.353365</td>\n",
       "      <td>-1.254890</td>\n",
       "      <td>-1.212175</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>514</th>\n",
       "      <td>0.921634</td>\n",
       "      <td>-0.264543</td>\n",
       "      <td>2.719216</td>\n",
       "      <td>-0.019643</td>\n",
       "      <td>0.242195</td>\n",
       "      <td>-0.966939</td>\n",
       "      <td>-0.118080</td>\n",
       "      <td>0.123010</td>\n",
       "      <td>1.290364</td>\n",
       "      <td>-0.177553</td>\n",
       "      <td>...</td>\n",
       "      <td>0.332816</td>\n",
       "      <td>0.344047</td>\n",
       "      <td>0.122409</td>\n",
       "      <td>0.313948</td>\n",
       "      <td>0.211157</td>\n",
       "      <td>-0.777731</td>\n",
       "      <td>-1.736711</td>\n",
       "      <td>-1.154127</td>\n",
       "      <td>-1.230040</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>515</th>\n",
       "      <td>-1.620549</td>\n",
       "      <td>-0.978838</td>\n",
       "      <td>0.331987</td>\n",
       "      <td>1.256638</td>\n",
       "      <td>-0.408164</td>\n",
       "      <td>0.735698</td>\n",
       "      <td>0.815510</td>\n",
       "      <td>-1.398344</td>\n",
       "      <td>0.076379</td>\n",
       "      <td>-0.005814</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.011739</td>\n",
       "      <td>-1.035533</td>\n",
       "      <td>-0.774382</td>\n",
       "      <td>-0.216315</td>\n",
       "      <td>0.515791</td>\n",
       "      <td>0.080575</td>\n",
       "      <td>-0.055548</td>\n",
       "      <td>-0.067502</td>\n",
       "      <td>-0.311027</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>516</th>\n",
       "      <td>4.075907</td>\n",
       "      <td>-0.367441</td>\n",
       "      <td>-0.247152</td>\n",
       "      <td>0.979966</td>\n",
       "      <td>6.792273</td>\n",
       "      <td>5.943666</td>\n",
       "      <td>-1.639583</td>\n",
       "      <td>8.121827</td>\n",
       "      <td>-0.627980</td>\n",
       "      <td>4.953722</td>\n",
       "      <td>...</td>\n",
       "      <td>10.467443</td>\n",
       "      <td>-7.333036</td>\n",
       "      <td>0.377340</td>\n",
       "      <td>8.870354</td>\n",
       "      <td>-1.074288</td>\n",
       "      <td>2.382433</td>\n",
       "      <td>1.042850</td>\n",
       "      <td>0.296436</td>\n",
       "      <td>0.125099</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>517 rows × 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          pc1       pc2       pc3       pc4       pc5       pc6       pc7  \\\n",
       "0    3.766709 -1.320255 -0.843971 -1.994738 -1.453359  0.693985  0.308104   \n",
       "1    0.390786  0.831062 -1.101365  1.400671  2.869388  0.965898 -2.795574   \n",
       "2    0.690416  1.177746 -1.221998  2.442038  1.090630  0.390801 -1.586675   \n",
       "3    3.359951 -1.161443  0.385728 -2.118328 -1.949601  1.027664 -0.179422   \n",
       "4    2.974329 -0.842626  1.327788  0.038086 -1.124763 -0.574676 -0.777155   \n",
       "..        ...       ...       ...       ...       ...       ...       ...   \n",
       "512 -0.087560  0.153964  1.241810  1.536581  0.372425 -1.133422 -0.362287   \n",
       "513  0.794366 -0.083966  2.670485  0.284995  0.223323 -0.904232 -0.014849   \n",
       "514  0.921634 -0.264543  2.719216 -0.019643  0.242195 -0.966939 -0.118080   \n",
       "515 -1.620549 -0.978838  0.331987  1.256638 -0.408164  0.735698  0.815510   \n",
       "516  4.075907 -0.367441 -0.247152  0.979966  6.792273  5.943666 -1.639583   \n",
       "\n",
       "          pc8       pc9      pc10  ...       pc16      pc17      pc18  \\\n",
       "0   -0.019764  0.010161 -0.437314  ...  -0.197543 -0.021839  0.688958   \n",
       "1    0.041095 -0.548879  0.104500  ...  -2.503167  0.499649  0.563706   \n",
       "2   -2.159336 -0.090580  0.260888  ...  -2.545144 -0.658411 -0.423618   \n",
       "3   -0.250227 -0.620329 -1.343189  ...  -0.040887  0.017843  0.332572   \n",
       "4    0.303635  0.861126 -2.024719  ...   0.844431  1.014944 -0.618231   \n",
       "..        ...       ...       ...  ...        ...       ...       ...   \n",
       "512  0.766946  0.818745 -0.289632  ...   0.300522  0.513876  0.539642   \n",
       "513  0.107226  1.340049 -0.147246  ...   0.342367  0.485571  0.580150   \n",
       "514  0.123010  1.290364 -0.177553  ...   0.332816  0.344047  0.122409   \n",
       "515 -1.398344  0.076379 -0.005814  ...  -0.011739 -1.035533 -0.774382   \n",
       "516  8.121827 -0.627980  4.953722  ...  10.467443 -7.333036  0.377340   \n",
       "\n",
       "         pc19      pc20      pc21      pc22      pc23      pc24  size_category  \n",
       "0    0.563603 -0.439596 -0.926619 -0.405425 -0.118719 -0.017933              0  \n",
       "1   -0.703319 -1.535718 -0.892995  0.836590  0.204975  0.290771              0  \n",
       "2    0.860550 -1.195230 -0.297870  0.743648  0.081757  0.345915              0  \n",
       "3    1.164745 -1.632741 -0.817618  1.523710 -0.342302 -0.378420              0  \n",
       "4    0.822853 -1.794109 -0.723371  2.020419 -0.545591  0.161735              0  \n",
       "..        ...       ...       ...       ...       ...       ...            ...  \n",
       "512 -0.052958  1.898628 -1.441786 -0.821192 -1.205707 -0.698666              1  \n",
       "513  0.384984  0.086251 -0.970693 -1.353365 -1.254890 -1.212175              1  \n",
       "514  0.313948  0.211157 -0.777731 -1.736711 -1.154127 -1.230040              1  \n",
       "515 -0.216315  0.515791  0.080575 -0.055548 -0.067502 -0.311027              0  \n",
       "516  8.870354 -1.074288  2.382433  1.042850  0.296436  0.125099              0  \n",
       "\n",
       "[517 rows x 25 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "finaldf=pd.concat([pd.DataFrame(pca_values[:,0:24],columns=['pc1','pc2','pc3','pc4','pc5','pc6','pc7',\n",
    "                                                             'pc8','pc9','pc10','pc11','pc12','pc13','pc14',\n",
    "                                                             'pc15','pc16','pc17','pc18','pc19','pc20','pc21',\n",
    "                                                             'pc22','pc23','pc24']),\n",
    "                 df[['size_category']]], axis = 1)\n",
    "finaldf.size_category.replace(('large','small'),(1,0),inplace=True)\n",
    "finaldf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split the data into x and y\n",
    "array=finaldf.values\n",
    "x=array[:,0:24]\n",
    "y=array[:,24]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# iteration 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "37/37 [==============================] - 2s 17ms/step - loss: 0.6820 - accuracy: 0.5449 - val_loss: 0.6824 - val_accuracy: 0.6410\n",
      "Epoch 2/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.6088 - accuracy: 0.7484 - val_loss: 0.6787 - val_accuracy: 0.6667\n",
      "Epoch 3/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.6051 - accuracy: 0.7363 - val_loss: 0.6770 - val_accuracy: 0.6410\n",
      "Epoch 4/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.5938 - accuracy: 0.7695 - val_loss: 0.6789 - val_accuracy: 0.6667\n",
      "Epoch 5/150\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.5693 - accuracy: 0.7682 - val_loss: 0.6812 - val_accuracy: 0.6603\n",
      "Epoch 6/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.5531 - accuracy: 0.7537 - val_loss: 0.6814 - val_accuracy: 0.6603\n",
      "Epoch 7/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.5469 - accuracy: 0.7911 - val_loss: 0.6868 - val_accuracy: 0.6603\n",
      "Epoch 8/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.5370 - accuracy: 0.7827 - val_loss: 0.6903 - val_accuracy: 0.6603\n",
      "Epoch 9/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.5310 - accuracy: 0.7825 - val_loss: 0.6941 - val_accuracy: 0.6603\n",
      "Epoch 10/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.4997 - accuracy: 0.8166 - val_loss: 0.6981 - val_accuracy: 0.6667\n",
      "Epoch 11/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.5224 - accuracy: 0.7681 - val_loss: 0.7002 - val_accuracy: 0.6667\n",
      "Epoch 12/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.5306 - accuracy: 0.7595 - val_loss: 0.7010 - val_accuracy: 0.6667\n",
      "Epoch 13/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.5150 - accuracy: 0.7668 - val_loss: 0.7088 - val_accuracy: 0.6731\n",
      "Epoch 14/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.4956 - accuracy: 0.7979 - val_loss: 0.7069 - val_accuracy: 0.6731\n",
      "Epoch 15/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.5334 - accuracy: 0.7675 - val_loss: 0.7080 - val_accuracy: 0.6731\n",
      "Epoch 16/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.4908 - accuracy: 0.7741 - val_loss: 0.7141 - val_accuracy: 0.6731\n",
      "Epoch 17/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.4790 - accuracy: 0.7779 - val_loss: 0.7170 - val_accuracy: 0.6731\n",
      "Epoch 18/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.4872 - accuracy: 0.7797 - val_loss: 0.7210 - val_accuracy: 0.6667\n",
      "Epoch 19/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.5122 - accuracy: 0.7710 - val_loss: 0.7240 - val_accuracy: 0.6731\n",
      "Epoch 20/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.4733 - accuracy: 0.7896 - val_loss: 0.7314 - val_accuracy: 0.6795\n",
      "Epoch 21/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.4825 - accuracy: 0.7769 - val_loss: 0.7264 - val_accuracy: 0.6731\n",
      "Epoch 22/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.4506 - accuracy: 0.7932 - val_loss: 0.7311 - val_accuracy: 0.6731\n",
      "Epoch 23/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.4542 - accuracy: 0.8053 - val_loss: 0.7317 - val_accuracy: 0.6731\n",
      "Epoch 24/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.4700 - accuracy: 0.7766 - val_loss: 0.7359 - val_accuracy: 0.6795\n",
      "Epoch 25/150\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.4641 - accuracy: 0.7873 - val_loss: 0.7430 - val_accuracy: 0.6731\n",
      "Epoch 26/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.4567 - accuracy: 0.7919 - val_loss: 0.7392 - val_accuracy: 0.6731\n",
      "Epoch 27/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.4594 - accuracy: 0.7666 - val_loss: 0.7398 - val_accuracy: 0.6731\n",
      "Epoch 28/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.5004 - accuracy: 0.7444 - val_loss: 0.7382 - val_accuracy: 0.6667\n",
      "Epoch 29/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.4295 - accuracy: 0.7881 - val_loss: 0.7423 - val_accuracy: 0.6795\n",
      "Epoch 30/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.4535 - accuracy: 0.7615 - val_loss: 0.7427 - val_accuracy: 0.6795\n",
      "Epoch 31/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.4609 - accuracy: 0.7685 - val_loss: 0.7446 - val_accuracy: 0.6795\n",
      "Epoch 32/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.4384 - accuracy: 0.7719 - val_loss: 0.7471 - val_accuracy: 0.6795\n",
      "Epoch 33/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.3888 - accuracy: 0.8123 - val_loss: 0.7494 - val_accuracy: 0.6859\n",
      "Epoch 34/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.4405 - accuracy: 0.7873 - val_loss: 0.7394 - val_accuracy: 0.6859\n",
      "Epoch 35/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.4254 - accuracy: 0.7821 - val_loss: 0.7408 - val_accuracy: 0.6859\n",
      "Epoch 36/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.4410 - accuracy: 0.7918 - val_loss: 0.7412 - val_accuracy: 0.6859\n",
      "Epoch 37/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.4161 - accuracy: 0.7834 - val_loss: 0.7424 - val_accuracy: 0.6859\n",
      "Epoch 38/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.3967 - accuracy: 0.7967 - val_loss: 0.7460 - val_accuracy: 0.6859\n",
      "Epoch 39/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.3897 - accuracy: 0.7923 - val_loss: 0.7437 - val_accuracy: 0.6859\n",
      "Epoch 40/150\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.4318 - accuracy: 0.7821 - val_loss: 0.7376 - val_accuracy: 0.6923\n",
      "Epoch 41/150\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.4025 - accuracy: 0.7884 - val_loss: 0.7308 - val_accuracy: 0.6859\n",
      "Epoch 42/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.4062 - accuracy: 0.7970 - val_loss: 0.7325 - val_accuracy: 0.6859\n",
      "Epoch 43/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.3624 - accuracy: 0.8209 - val_loss: 0.7345 - val_accuracy: 0.6987\n",
      "Epoch 44/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.3827 - accuracy: 0.8071 - val_loss: 0.7285 - val_accuracy: 0.7115\n",
      "Epoch 45/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.3719 - accuracy: 0.8311 - val_loss: 0.7317 - val_accuracy: 0.7115\n",
      "Epoch 46/150\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.3840 - accuracy: 0.8147 - val_loss: 0.7266 - val_accuracy: 0.7179\n",
      "Epoch 47/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.3655 - accuracy: 0.7985 - val_loss: 0.7233 - val_accuracy: 0.7244\n",
      "Epoch 48/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.3798 - accuracy: 0.8117 - val_loss: 0.7211 - val_accuracy: 0.7308\n",
      "Epoch 49/150\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.3655 - accuracy: 0.8289 - val_loss: 0.7242 - val_accuracy: 0.7308\n",
      "Epoch 50/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.3457 - accuracy: 0.8381 - val_loss: 0.7215 - val_accuracy: 0.7372\n",
      "Epoch 51/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.2988 - accuracy: 0.8871 - val_loss: 0.7225 - val_accuracy: 0.7500\n",
      "Epoch 52/150\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.3480 - accuracy: 0.8644 - val_loss: 0.7185 - val_accuracy: 0.7500\n",
      "Epoch 53/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.3193 - accuracy: 0.8746 - val_loss: 0.7195 - val_accuracy: 0.7500\n",
      "Epoch 54/150\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.3188 - accuracy: 0.8611 - val_loss: 0.7196 - val_accuracy: 0.7500\n",
      "Epoch 55/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.3446 - accuracy: 0.8357 - val_loss: 0.7202 - val_accuracy: 0.7500\n",
      "Epoch 56/150\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.2982 - accuracy: 0.8789 - val_loss: 0.7232 - val_accuracy: 0.7564\n",
      "Epoch 57/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.3040 - accuracy: 0.8666 - val_loss: 0.7243 - val_accuracy: 0.7500\n",
      "Epoch 58/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.3030 - accuracy: 0.8784 - val_loss: 0.7256 - val_accuracy: 0.7564\n",
      "Epoch 59/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.2889 - accuracy: 0.8776 - val_loss: 0.7246 - val_accuracy: 0.7628\n",
      "Epoch 60/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.3221 - accuracy: 0.8575 - val_loss: 0.7190 - val_accuracy: 0.7564\n",
      "Epoch 61/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.3026 - accuracy: 0.8657 - val_loss: 0.7216 - val_accuracy: 0.7628\n",
      "Epoch 62/150\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.3105 - accuracy: 0.8746 - val_loss: 0.7227 - val_accuracy: 0.7628\n",
      "Epoch 63/150\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.3077 - accuracy: 0.8742 - val_loss: 0.7272 - val_accuracy: 0.7564\n",
      "Epoch 64/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.2707 - accuracy: 0.8885 - val_loss: 0.7258 - val_accuracy: 0.7500\n",
      "Epoch 65/150\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.2513 - accuracy: 0.9343 - val_loss: 0.7306 - val_accuracy: 0.7500\n",
      "Epoch 66/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.2426 - accuracy: 0.9224 - val_loss: 0.7303 - val_accuracy: 0.7628\n",
      "Epoch 67/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.2556 - accuracy: 0.9127 - val_loss: 0.7303 - val_accuracy: 0.7436\n",
      "Epoch 68/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.2496 - accuracy: 0.9249 - val_loss: 0.7322 - val_accuracy: 0.7628\n",
      "Epoch 69/150\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.2546 - accuracy: 0.9145 - val_loss: 0.7304 - val_accuracy: 0.7564\n",
      "Epoch 70/150\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.2407 - accuracy: 0.9101 - val_loss: 0.7355 - val_accuracy: 0.7628\n",
      "Epoch 71/150\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.2335 - accuracy: 0.9192 - val_loss: 0.7289 - val_accuracy: 0.7628\n",
      "Epoch 72/150\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.2441 - accuracy: 0.9140 - val_loss: 0.7314 - val_accuracy: 0.7628\n",
      "Epoch 73/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.2009 - accuracy: 0.9318 - val_loss: 0.7299 - val_accuracy: 0.7692\n",
      "Epoch 74/150\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.2344 - accuracy: 0.9183 - val_loss: 0.7218 - val_accuracy: 0.7628\n",
      "Epoch 75/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.2190 - accuracy: 0.9161 - val_loss: 0.7337 - val_accuracy: 0.7628\n",
      "Epoch 76/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.2046 - accuracy: 0.9334 - val_loss: 0.7272 - val_accuracy: 0.7756\n",
      "Epoch 77/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.2402 - accuracy: 0.9038 - val_loss: 0.7307 - val_accuracy: 0.7821\n",
      "Epoch 78/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.2202 - accuracy: 0.9244 - val_loss: 0.7311 - val_accuracy: 0.7821\n",
      "Epoch 79/150\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.2010 - accuracy: 0.9434 - val_loss: 0.7308 - val_accuracy: 0.7756\n",
      "Epoch 80/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.2138 - accuracy: 0.9229 - val_loss: 0.7309 - val_accuracy: 0.7756\n",
      "Epoch 81/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.1994 - accuracy: 0.9235 - val_loss: 0.7277 - val_accuracy: 0.7885\n",
      "Epoch 82/150\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.2178 - accuracy: 0.9134 - val_loss: 0.7289 - val_accuracy: 0.7885\n",
      "Epoch 83/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.2084 - accuracy: 0.9264 - val_loss: 0.7303 - val_accuracy: 0.7885\n",
      "Epoch 84/150\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.1772 - accuracy: 0.9422 - val_loss: 0.7252 - val_accuracy: 0.7756\n",
      "Epoch 85/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.1864 - accuracy: 0.9458 - val_loss: 0.7302 - val_accuracy: 0.7885\n",
      "Epoch 86/150\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.1764 - accuracy: 0.9385 - val_loss: 0.7321 - val_accuracy: 0.8013\n",
      "Epoch 87/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.1908 - accuracy: 0.9429 - val_loss: 0.7297 - val_accuracy: 0.7949\n",
      "Epoch 88/150\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.1808 - accuracy: 0.9429 - val_loss: 0.7314 - val_accuracy: 0.7949\n",
      "Epoch 89/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.1836 - accuracy: 0.9322 - val_loss: 0.7317 - val_accuracy: 0.8013\n",
      "Epoch 90/150\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.1610 - accuracy: 0.9624 - val_loss: 0.7318 - val_accuracy: 0.7949\n",
      "Epoch 91/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.1814 - accuracy: 0.9328 - val_loss: 0.7384 - val_accuracy: 0.8205\n",
      "Epoch 92/150\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.1472 - accuracy: 0.9473 - val_loss: 0.7390 - val_accuracy: 0.7949\n",
      "Epoch 93/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.1416 - accuracy: 0.9721 - val_loss: 0.7408 - val_accuracy: 0.7821\n",
      "Epoch 94/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.1368 - accuracy: 0.9649 - val_loss: 0.7448 - val_accuracy: 0.8013\n",
      "Epoch 95/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.1731 - accuracy: 0.9372 - val_loss: 0.7458 - val_accuracy: 0.8013\n",
      "Epoch 96/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.1868 - accuracy: 0.9436 - val_loss: 0.7440 - val_accuracy: 0.7949\n",
      "Epoch 97/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.1575 - accuracy: 0.9596 - val_loss: 0.7459 - val_accuracy: 0.8013\n",
      "Epoch 98/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.1413 - accuracy: 0.9676 - val_loss: 0.7485 - val_accuracy: 0.7949\n",
      "Epoch 99/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.1424 - accuracy: 0.9663 - val_loss: 0.7523 - val_accuracy: 0.8013\n",
      "Epoch 100/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.1178 - accuracy: 0.9650 - val_loss: 0.7514 - val_accuracy: 0.7885\n",
      "Epoch 101/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.1363 - accuracy: 0.9587 - val_loss: 0.7539 - val_accuracy: 0.8013\n",
      "Epoch 102/150\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 0.1331 - accuracy: 0.9616 - val_loss: 0.7612 - val_accuracy: 0.8013\n",
      "Epoch 103/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.1369 - accuracy: 0.9532 - val_loss: 0.7603 - val_accuracy: 0.8077\n",
      "Epoch 104/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.1257 - accuracy: 0.9677 - val_loss: 0.7658 - val_accuracy: 0.8013\n",
      "Epoch 105/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.1451 - accuracy: 0.9512 - val_loss: 0.7715 - val_accuracy: 0.8077\n",
      "Epoch 106/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.1318 - accuracy: 0.9550 - val_loss: 0.7730 - val_accuracy: 0.8013\n",
      "Epoch 107/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.1477 - accuracy: 0.9556 - val_loss: 0.7777 - val_accuracy: 0.8077\n",
      "Epoch 108/150\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.0990 - accuracy: 0.9817 - val_loss: 0.7751 - val_accuracy: 0.7885\n",
      "Epoch 109/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.1203 - accuracy: 0.9571 - val_loss: 0.7890 - val_accuracy: 0.7949\n",
      "Epoch 110/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.1099 - accuracy: 0.9792 - val_loss: 0.7873 - val_accuracy: 0.8013\n",
      "Epoch 111/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.0972 - accuracy: 0.9763 - val_loss: 0.7902 - val_accuracy: 0.8013\n",
      "Epoch 112/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.1288 - accuracy: 0.9525 - val_loss: 0.7960 - val_accuracy: 0.8141\n",
      "Epoch 113/150\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.1058 - accuracy: 0.9782 - val_loss: 0.8021 - val_accuracy: 0.8013\n",
      "Epoch 114/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.1257 - accuracy: 0.9621 - val_loss: 0.8044 - val_accuracy: 0.8013\n",
      "Epoch 115/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.0809 - accuracy: 0.9813 - val_loss: 0.8048 - val_accuracy: 0.8077\n",
      "Epoch 116/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.1000 - accuracy: 0.9772 - val_loss: 0.8124 - val_accuracy: 0.8077\n",
      "Epoch 117/150\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.1171 - accuracy: 0.9642 - val_loss: 0.8179 - val_accuracy: 0.8205\n",
      "Epoch 118/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.1016 - accuracy: 0.9711 - val_loss: 0.8213 - val_accuracy: 0.8141\n",
      "Epoch 119/150\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.0930 - accuracy: 0.9814 - val_loss: 0.8286 - val_accuracy: 0.8205\n",
      "Epoch 120/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.1131 - accuracy: 0.9675 - val_loss: 0.8267 - val_accuracy: 0.8205\n",
      "Epoch 121/150\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.0740 - accuracy: 0.9856 - val_loss: 0.8323 - val_accuracy: 0.8141\n",
      "Epoch 122/150\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.1115 - accuracy: 0.9619 - val_loss: 0.8342 - val_accuracy: 0.8205\n",
      "Epoch 123/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.0991 - accuracy: 0.9696 - val_loss: 0.8434 - val_accuracy: 0.8205\n",
      "Epoch 124/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.0901 - accuracy: 0.9715 - val_loss: 0.8474 - val_accuracy: 0.8141\n",
      "Epoch 125/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.0971 - accuracy: 0.9687 - val_loss: 0.8486 - val_accuracy: 0.8141\n",
      "Epoch 126/150\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.1010 - accuracy: 0.9693 - val_loss: 0.8510 - val_accuracy: 0.8141\n",
      "Epoch 127/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.0958 - accuracy: 0.9681 - val_loss: 0.8620 - val_accuracy: 0.8141\n",
      "Epoch 128/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.0753 - accuracy: 0.9856 - val_loss: 0.8609 - val_accuracy: 0.8077\n",
      "Epoch 129/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.0649 - accuracy: 0.9829 - val_loss: 0.8671 - val_accuracy: 0.8141\n",
      "Epoch 130/150\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.0879 - accuracy: 0.9712 - val_loss: 0.8736 - val_accuracy: 0.8205\n",
      "Epoch 131/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.0724 - accuracy: 0.9807 - val_loss: 0.8791 - val_accuracy: 0.8141\n",
      "Epoch 132/150\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.0903 - accuracy: 0.9664 - val_loss: 0.8858 - val_accuracy: 0.8141\n",
      "Epoch 133/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.0872 - accuracy: 0.9668 - val_loss: 0.8872 - val_accuracy: 0.8205\n",
      "Epoch 134/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.0802 - accuracy: 0.9794 - val_loss: 0.8961 - val_accuracy: 0.8141\n",
      "Epoch 135/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.0850 - accuracy: 0.9739 - val_loss: 0.8981 - val_accuracy: 0.8141\n",
      "Epoch 136/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.0685 - accuracy: 0.9819 - val_loss: 0.9006 - val_accuracy: 0.8141\n",
      "Epoch 137/150\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.0751 - accuracy: 0.9745 - val_loss: 0.9114 - val_accuracy: 0.8141\n",
      "Epoch 138/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.0798 - accuracy: 0.9750 - val_loss: 0.9106 - val_accuracy: 0.8141\n",
      "Epoch 139/150\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.0639 - accuracy: 0.9814 - val_loss: 0.9229 - val_accuracy: 0.8141\n",
      "Epoch 140/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.0679 - accuracy: 0.9810 - val_loss: 0.9304 - val_accuracy: 0.8077\n",
      "Epoch 141/150\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.0668 - accuracy: 0.9805 - val_loss: 0.9370 - val_accuracy: 0.8141\n",
      "Epoch 142/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.0685 - accuracy: 0.9830 - val_loss: 0.9428 - val_accuracy: 0.8077\n",
      "Epoch 143/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.0760 - accuracy: 0.9673 - val_loss: 0.9493 - val_accuracy: 0.8013\n",
      "Epoch 144/150\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.0589 - accuracy: 0.9854 - val_loss: 0.9520 - val_accuracy: 0.8141\n",
      "Epoch 145/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.0518 - accuracy: 0.9906 - val_loss: 0.9591 - val_accuracy: 0.7949\n",
      "Epoch 146/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.0595 - accuracy: 0.9818 - val_loss: 0.9709 - val_accuracy: 0.7949\n",
      "Epoch 147/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.0645 - accuracy: 0.9792 - val_loss: 0.9720 - val_accuracy: 0.8205\n",
      "Epoch 148/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.0540 - accuracy: 0.9889 - val_loss: 0.9810 - val_accuracy: 0.7949\n",
      "Epoch 149/150\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.0665 - accuracy: 0.9748 - val_loss: 0.9782 - val_accuracy: 0.8205\n",
      "Epoch 150/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.0591 - accuracy: 0.9835 - val_loss: 0.9870 - val_accuracy: 0.8141\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x19c51394250>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create model\n",
    "model=Sequential()\n",
    "model.add(Dense(12,input_dim=24,activation='relu'))\n",
    "model.add(Dense(8,activation='relu'))\n",
    "model.add(Dense(1,activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "model.fit(x,y, validation_split=0.3,epochs=150,batch_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17/17 [==============================] - 0s 2ms/step - loss: 0.3335 - accuracy: 0.9362\n"
     ]
    }
   ],
   "source": [
    "#accuracy of model\n",
    "scores=model.evaluate(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 93.62%\n"
     ]
    }
   ],
   "source": [
    "print(\"%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# iteration 2¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "25/25 [==============================] - 2s 19ms/step - loss: 0.5921 - accuracy: 0.7266 - val_loss: 0.6636 - val_accuracy: 0.6731\n",
      "Epoch 2/100\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.5327 - accuracy: 0.7699 - val_loss: 0.6679 - val_accuracy: 0.6731\n",
      "Epoch 3/100\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.5587 - accuracy: 0.7468 - val_loss: 0.6627 - val_accuracy: 0.6731\n",
      "Epoch 4/100\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.5272 - accuracy: 0.7686 - val_loss: 0.6620 - val_accuracy: 0.6731\n",
      "Epoch 5/100\n",
      "25/25 [==============================] - 0s 6ms/step - loss: 0.5530 - accuracy: 0.7384 - val_loss: 0.6622 - val_accuracy: 0.6731\n",
      "Epoch 6/100\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.5200 - accuracy: 0.7599 - val_loss: 0.6653 - val_accuracy: 0.6731\n",
      "Epoch 7/100\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.5067 - accuracy: 0.7796 - val_loss: 0.6542 - val_accuracy: 0.6731\n",
      "Epoch 8/100\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.5099 - accuracy: 0.7692 - val_loss: 0.6460 - val_accuracy: 0.6795\n",
      "Epoch 9/100\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.5238 - accuracy: 0.7538 - val_loss: 0.6630 - val_accuracy: 0.6795\n",
      "Epoch 10/100\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.5196 - accuracy: 0.7529 - val_loss: 0.6464 - val_accuracy: 0.6795\n",
      "Epoch 11/100\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.5322 - accuracy: 0.7386 - val_loss: 0.6450 - val_accuracy: 0.6795\n",
      "Epoch 12/100\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.5275 - accuracy: 0.7414 - val_loss: 0.6773 - val_accuracy: 0.6795\n",
      "Epoch 13/100\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.4771 - accuracy: 0.7873 - val_loss: 0.7448 - val_accuracy: 0.6795\n",
      "Epoch 14/100\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.5118 - accuracy: 0.7547 - val_loss: 0.7402 - val_accuracy: 0.6795\n",
      "Epoch 15/100\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.4836 - accuracy: 0.7765 - val_loss: 0.7477 - val_accuracy: 0.6795\n",
      "Epoch 16/100\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.5361 - accuracy: 0.7288 - val_loss: 0.7404 - val_accuracy: 0.6795\n",
      "Epoch 17/100\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.4771 - accuracy: 0.7773 - val_loss: 0.7488 - val_accuracy: 0.6795\n",
      "Epoch 18/100\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.4808 - accuracy: 0.7647 - val_loss: 0.7533 - val_accuracy: 0.6795\n",
      "Epoch 19/100\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.4724 - accuracy: 0.7792 - val_loss: 0.7441 - val_accuracy: 0.6795\n",
      "Epoch 20/100\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.5116 - accuracy: 0.7488 - val_loss: 0.7563 - val_accuracy: 0.6795\n",
      "Epoch 21/100\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.5182 - accuracy: 0.7616 - val_loss: 0.7523 - val_accuracy: 0.6795\n",
      "Epoch 22/100\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.4768 - accuracy: 0.7800 - val_loss: 0.7760 - val_accuracy: 0.6795\n",
      "Epoch 23/100\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.4502 - accuracy: 0.8027 - val_loss: 0.7558 - val_accuracy: 0.6859\n",
      "Epoch 24/100\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.4760 - accuracy: 0.7732 - val_loss: 0.7577 - val_accuracy: 0.6923\n",
      "Epoch 25/100\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.4501 - accuracy: 0.8079 - val_loss: 0.9773 - val_accuracy: 0.6923\n",
      "Epoch 26/100\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.4728 - accuracy: 0.7852 - val_loss: 0.8415 - val_accuracy: 0.6923\n",
      "Epoch 27/100\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.4535 - accuracy: 0.8112 - val_loss: 0.9065 - val_accuracy: 0.6987\n",
      "Epoch 28/100\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.4408 - accuracy: 0.8007 - val_loss: 0.9629 - val_accuracy: 0.6987\n",
      "Epoch 29/100\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.4737 - accuracy: 0.7739 - val_loss: 0.9644 - val_accuracy: 0.6987\n",
      "Epoch 30/100\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.4235 - accuracy: 0.8134 - val_loss: 0.9597 - val_accuracy: 0.6987\n",
      "Epoch 31/100\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.4541 - accuracy: 0.7892 - val_loss: 0.8716 - val_accuracy: 0.7051\n",
      "Epoch 32/100\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.4408 - accuracy: 0.8092 - val_loss: 0.8889 - val_accuracy: 0.7051\n",
      "Epoch 33/100\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.4488 - accuracy: 0.7967 - val_loss: 0.9463 - val_accuracy: 0.7051\n",
      "Epoch 34/100\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.4487 - accuracy: 0.7966 - val_loss: 0.9902 - val_accuracy: 0.7051\n",
      "Epoch 35/100\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.4229 - accuracy: 0.8173 - val_loss: 0.9518 - val_accuracy: 0.7051\n",
      "Epoch 36/100\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.4107 - accuracy: 0.8036 - val_loss: 0.9537 - val_accuracy: 0.7051\n",
      "Epoch 37/100\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.4583 - accuracy: 0.7789 - val_loss: 0.9559 - val_accuracy: 0.7051\n",
      "Epoch 38/100\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.4202 - accuracy: 0.8043 - val_loss: 0.9482 - val_accuracy: 0.7051\n",
      "Epoch 39/100\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.4194 - accuracy: 0.7952 - val_loss: 0.9497 - val_accuracy: 0.7051\n",
      "Epoch 40/100\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.4501 - accuracy: 0.7756 - val_loss: 0.9492 - val_accuracy: 0.7051\n",
      "Epoch 41/100\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.4235 - accuracy: 0.7933 - val_loss: 0.9315 - val_accuracy: 0.7051\n",
      "Epoch 42/100\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.3764 - accuracy: 0.8432 - val_loss: 0.9160 - val_accuracy: 0.7244\n",
      "Epoch 43/100\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.4354 - accuracy: 0.8030 - val_loss: 0.9248 - val_accuracy: 0.7179\n",
      "Epoch 44/100\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.4177 - accuracy: 0.7809 - val_loss: 0.9977 - val_accuracy: 0.7051\n",
      "Epoch 45/100\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.4164 - accuracy: 0.8052 - val_loss: 0.9931 - val_accuracy: 0.7115\n",
      "Epoch 46/100\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.4005 - accuracy: 0.8083 - val_loss: 0.9187 - val_accuracy: 0.7179\n",
      "Epoch 47/100\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.3698 - accuracy: 0.8246 - val_loss: 0.9903 - val_accuracy: 0.7115\n",
      "Epoch 48/100\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.4313 - accuracy: 0.7817 - val_loss: 0.9848 - val_accuracy: 0.7115\n",
      "Epoch 49/100\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.3739 - accuracy: 0.8144 - val_loss: 0.9078 - val_accuracy: 0.7500\n",
      "Epoch 50/100\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.3773 - accuracy: 0.8379 - val_loss: 0.9868 - val_accuracy: 0.7308\n",
      "Epoch 51/100\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.3775 - accuracy: 0.8159 - val_loss: 0.9079 - val_accuracy: 0.7564\n",
      "Epoch 52/100\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.3411 - accuracy: 0.8350 - val_loss: 0.9108 - val_accuracy: 0.7628\n",
      "Epoch 53/100\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.3912 - accuracy: 0.8168 - val_loss: 0.9089 - val_accuracy: 0.7564\n",
      "Epoch 54/100\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.3580 - accuracy: 0.8272 - val_loss: 0.9785 - val_accuracy: 0.7692\n",
      "Epoch 55/100\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.3873 - accuracy: 0.8074 - val_loss: 0.9756 - val_accuracy: 0.7628\n",
      "Epoch 56/100\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.4093 - accuracy: 0.7847 - val_loss: 0.9697 - val_accuracy: 0.7692\n",
      "Epoch 57/100\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.3420 - accuracy: 0.8366 - val_loss: 0.9637 - val_accuracy: 0.7564\n",
      "Epoch 58/100\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.3700 - accuracy: 0.8085 - val_loss: 0.9619 - val_accuracy: 0.7756\n",
      "Epoch 59/100\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.3458 - accuracy: 0.8267 - val_loss: 0.9596 - val_accuracy: 0.7692\n",
      "Epoch 60/100\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.3413 - accuracy: 0.8301 - val_loss: 0.8942 - val_accuracy: 0.7692\n",
      "Epoch 61/100\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.3357 - accuracy: 0.8303 - val_loss: 0.8870 - val_accuracy: 0.7692\n",
      "Epoch 62/100\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.3465 - accuracy: 0.8249 - val_loss: 0.9546 - val_accuracy: 0.7692\n",
      "Epoch 63/100\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.3175 - accuracy: 0.8489 - val_loss: 0.9498 - val_accuracy: 0.7756\n",
      "Epoch 64/100\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.3400 - accuracy: 0.8383 - val_loss: 0.9461 - val_accuracy: 0.7692\n",
      "Epoch 65/100\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.3400 - accuracy: 0.8143 - val_loss: 0.9448 - val_accuracy: 0.7692\n",
      "Epoch 66/100\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.3465 - accuracy: 0.8019 - val_loss: 0.9423 - val_accuracy: 0.7692\n",
      "Epoch 67/100\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.3117 - accuracy: 0.8359 - val_loss: 0.9399 - val_accuracy: 0.7628\n",
      "Epoch 68/100\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.3513 - accuracy: 0.8166 - val_loss: 0.9362 - val_accuracy: 0.7692\n",
      "Epoch 69/100\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.3112 - accuracy: 0.8493 - val_loss: 0.9334 - val_accuracy: 0.7628\n",
      "Epoch 70/100\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.3193 - accuracy: 0.8348 - val_loss: 0.9286 - val_accuracy: 0.7692\n",
      "Epoch 71/100\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.3169 - accuracy: 0.8291 - val_loss: 0.9229 - val_accuracy: 0.7692\n",
      "Epoch 72/100\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.3339 - accuracy: 0.8296 - val_loss: 0.9231 - val_accuracy: 0.7756\n",
      "Epoch 73/100\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.3114 - accuracy: 0.8383 - val_loss: 0.9180 - val_accuracy: 0.7756\n",
      "Epoch 74/100\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.2853 - accuracy: 0.8666 - val_loss: 0.9161 - val_accuracy: 0.7821\n",
      "Epoch 75/100\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.2966 - accuracy: 0.8495 - val_loss: 0.9133 - val_accuracy: 0.7756\n",
      "Epoch 76/100\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.3516 - accuracy: 0.8090 - val_loss: 0.9083 - val_accuracy: 0.7821\n",
      "Epoch 77/100\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.3040 - accuracy: 0.8471 - val_loss: 0.9067 - val_accuracy: 0.7692\n",
      "Epoch 78/100\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.2972 - accuracy: 0.8497 - val_loss: 0.8412 - val_accuracy: 0.7756\n",
      "Epoch 79/100\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.2806 - accuracy: 0.8574 - val_loss: 0.9032 - val_accuracy: 0.7756\n",
      "Epoch 80/100\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.3234 - accuracy: 0.8507 - val_loss: 0.8230 - val_accuracy: 0.7692\n",
      "Epoch 81/100\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.3169 - accuracy: 0.8336 - val_loss: 0.8268 - val_accuracy: 0.7885\n",
      "Epoch 82/100\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.2676 - accuracy: 0.8756 - val_loss: 0.8274 - val_accuracy: 0.7821\n",
      "Epoch 83/100\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.2732 - accuracy: 0.8658 - val_loss: 0.8301 - val_accuracy: 0.7949\n",
      "Epoch 84/100\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.3071 - accuracy: 0.8445 - val_loss: 0.8885 - val_accuracy: 0.7756\n",
      "Epoch 85/100\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.2752 - accuracy: 0.8627 - val_loss: 0.8827 - val_accuracy: 0.7756\n",
      "Epoch 86/100\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.2946 - accuracy: 0.8446 - val_loss: 0.8829 - val_accuracy: 0.7949\n",
      "Epoch 87/100\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.3223 - accuracy: 0.8293 - val_loss: 0.8699 - val_accuracy: 0.7821\n",
      "Epoch 88/100\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.2721 - accuracy: 0.8581 - val_loss: 0.8679 - val_accuracy: 0.7821\n",
      "Epoch 89/100\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.2948 - accuracy: 0.8411 - val_loss: 0.8644 - val_accuracy: 0.7821\n",
      "Epoch 90/100\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.2601 - accuracy: 0.8624 - val_loss: 0.8701 - val_accuracy: 0.8077\n",
      "Epoch 91/100\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.2588 - accuracy: 0.8907 - val_loss: 0.8614 - val_accuracy: 0.8141\n",
      "Epoch 92/100\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.3179 - accuracy: 0.8261 - val_loss: 0.8583 - val_accuracy: 0.8333\n",
      "Epoch 93/100\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.2606 - accuracy: 0.8603 - val_loss: 0.8461 - val_accuracy: 0.8141\n",
      "Epoch 94/100\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.3023 - accuracy: 0.8299 - val_loss: 0.8451 - val_accuracy: 0.8333\n",
      "Epoch 95/100\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.2837 - accuracy: 0.8490 - val_loss: 0.8412 - val_accuracy: 0.8141\n",
      "Epoch 96/100\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.2499 - accuracy: 0.8728 - val_loss: 0.8436 - val_accuracy: 0.8462\n",
      "Epoch 97/100\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.2739 - accuracy: 0.8546 - val_loss: 0.7772 - val_accuracy: 0.8526\n",
      "Epoch 98/100\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.2270 - accuracy: 0.8853 - val_loss: 0.8347 - val_accuracy: 0.8462\n",
      "Epoch 99/100\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.2478 - accuracy: 0.8665 - val_loss: 0.8305 - val_accuracy: 0.8526\n",
      "Epoch 100/100\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.2442 - accuracy: 0.8688 - val_loss: 0.8267 - val_accuracy: 0.8526\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x19c54ca6220>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1=Sequential()\n",
    "model1.add(Dense(12,input_dim=24,activation='sigmoid'))\n",
    "model1.add(Dense(8,activation='sigmoid'))\n",
    "model1.add(Dense(1,activation='relu'))\n",
    "model1.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "model1.fit(x, y, validation_split=0.3, epochs=100, batch_size=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17/17 [==============================] - 0s 2ms/step - loss: 0.4196 - accuracy: 0.8665\n",
      "accuracy: 86.65%\n"
     ]
    }
   ],
   "source": [
    "#model accuracy\n",
    "scores1=model1.evaluate(x,y)\n",
    "print(\"%s: %.2f%%\" % (model1.metrics_names[1], scores1[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# iteration 3¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "25/25 [==============================] - 2s 17ms/step - loss: 2.8283 - accuracy: 0.7567 - val_loss: 3.9024 - val_accuracy: 0.6474\n",
      "Epoch 2/100\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 2.8919 - accuracy: 0.7536 - val_loss: 3.8223 - val_accuracy: 0.6538\n",
      "Epoch 3/100\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 2.6189 - accuracy: 0.7580 - val_loss: 3.5952 - val_accuracy: 0.6538\n",
      "Epoch 4/100\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 2.8685 - accuracy: 0.7399 - val_loss: 3.5546 - val_accuracy: 0.6538\n",
      "Epoch 5/100\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 2.2117 - accuracy: 0.7662 - val_loss: 3.5504 - val_accuracy: 0.6474\n",
      "Epoch 6/100\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 2.4181 - accuracy: 0.7531 - val_loss: 3.5649 - val_accuracy: 0.6474\n",
      "Epoch 7/100\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 2.5065 - accuracy: 0.7414 - val_loss: 3.7108 - val_accuracy: 0.6474\n",
      "Epoch 8/100\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 2.4696 - accuracy: 0.7674 - val_loss: 3.6504 - val_accuracy: 0.6474\n",
      "Epoch 9/100\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 2.4146 - accuracy: 0.7442 - val_loss: 3.6506 - val_accuracy: 0.6474\n",
      "Epoch 10/100\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 2.2205 - accuracy: 0.7908 - val_loss: 3.6739 - val_accuracy: 0.6474\n",
      "Epoch 11/100\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 2.2498 - accuracy: 0.7631 - val_loss: 3.6634 - val_accuracy: 0.6474\n",
      "Epoch 12/100\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 2.3518 - accuracy: 0.7636 - val_loss: 3.6538 - val_accuracy: 0.6538\n",
      "Epoch 13/100\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 1.8711 - accuracy: 0.7981 - val_loss: 3.7202 - val_accuracy: 0.6538\n",
      "Epoch 14/100\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 2.5095 - accuracy: 0.7628 - val_loss: 3.7265 - val_accuracy: 0.6538\n",
      "Epoch 15/100\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 1.9932 - accuracy: 0.7786 - val_loss: 3.6529 - val_accuracy: 0.6538\n",
      "Epoch 16/100\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 1.9800 - accuracy: 0.7894 - val_loss: 3.7265 - val_accuracy: 0.6538\n",
      "Epoch 17/100\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 2.1048 - accuracy: 0.7761 - val_loss: 3.7964 - val_accuracy: 0.6538\n",
      "Epoch 18/100\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 1.9262 - accuracy: 0.7895 - val_loss: 3.7989 - val_accuracy: 0.6538\n",
      "Epoch 19/100\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 2.1118 - accuracy: 0.7755 - val_loss: 3.8074 - val_accuracy: 0.6538\n",
      "Epoch 20/100\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 2.1496 - accuracy: 0.7903 - val_loss: 3.8658 - val_accuracy: 0.6538\n",
      "Epoch 21/100\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 2.2293 - accuracy: 0.7551 - val_loss: 3.8653 - val_accuracy: 0.6538\n",
      "Epoch 22/100\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 2.5385 - accuracy: 0.7420 - val_loss: 3.8657 - val_accuracy: 0.6538\n",
      "Epoch 23/100\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 2.3253 - accuracy: 0.7428 - val_loss: 3.8283 - val_accuracy: 0.6474\n",
      "Epoch 24/100\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 2.0278 - accuracy: 0.7782 - val_loss: 3.8664 - val_accuracy: 0.6538\n",
      "Epoch 25/100\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 2.6030 - accuracy: 0.7527 - val_loss: 3.8660 - val_accuracy: 0.6538\n",
      "Epoch 26/100\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 2.2495 - accuracy: 0.7964 - val_loss: 3.7664 - val_accuracy: 0.6538\n",
      "Epoch 27/100\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 2.2106 - accuracy: 0.7674 - val_loss: 3.7992 - val_accuracy: 0.6474\n",
      "Epoch 28/100\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 2.3393 - accuracy: 0.7919 - val_loss: 3.7399 - val_accuracy: 0.6538\n",
      "Epoch 29/100\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 2.0234 - accuracy: 0.7910 - val_loss: 3.6271 - val_accuracy: 0.6538\n",
      "Epoch 30/100\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 2.4126 - accuracy: 0.7669 - val_loss: 3.6702 - val_accuracy: 0.6603\n",
      "Epoch 31/100\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 2.2687 - accuracy: 0.7772 - val_loss: 3.6648 - val_accuracy: 0.6603\n",
      "Epoch 32/100\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 2.0142 - accuracy: 0.7822 - val_loss: 3.6582 - val_accuracy: 0.6603\n",
      "Epoch 33/100\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 2.5324 - accuracy: 0.7475 - val_loss: 3.5980 - val_accuracy: 0.6667\n",
      "Epoch 34/100\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 2.2839 - accuracy: 0.7578 - val_loss: 3.5444 - val_accuracy: 0.6667\n",
      "Epoch 35/100\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 2.2944 - accuracy: 0.7530 - val_loss: 3.4326 - val_accuracy: 0.6667\n",
      "Epoch 36/100\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 2.1140 - accuracy: 0.7863 - val_loss: 3.4695 - val_accuracy: 0.6667\n",
      "Epoch 37/100\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 2.5368 - accuracy: 0.7241 - val_loss: 3.5714 - val_accuracy: 0.6667\n",
      "Epoch 38/100\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 2.2317 - accuracy: 0.7899 - val_loss: 3.5700 - val_accuracy: 0.6667\n",
      "Epoch 39/100\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 2.4027 - accuracy: 0.7624 - val_loss: 3.4469 - val_accuracy: 0.6667\n",
      "Epoch 40/100\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 2.1388 - accuracy: 0.7910 - val_loss: 3.3590 - val_accuracy: 0.6667\n",
      "Epoch 41/100\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 2.2082 - accuracy: 0.7848 - val_loss: 3.3531 - val_accuracy: 0.6667\n",
      "Epoch 42/100\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 1.9918 - accuracy: 0.7888 - val_loss: 3.4239 - val_accuracy: 0.6667\n",
      "Epoch 43/100\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 2.4378 - accuracy: 0.7576 - val_loss: 3.4188 - val_accuracy: 0.6667\n",
      "Epoch 44/100\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 2.0236 - accuracy: 0.8002 - val_loss: 3.4119 - val_accuracy: 0.6667\n",
      "Epoch 45/100\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 2.0945 - accuracy: 0.7964 - val_loss: 3.4133 - val_accuracy: 0.6603\n",
      "Epoch 46/100\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 2.7274 - accuracy: 0.7678 - val_loss: 3.4241 - val_accuracy: 0.6538\n",
      "Epoch 47/100\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 2.1379 - accuracy: 0.7924 - val_loss: 3.4888 - val_accuracy: 0.6603\n",
      "Epoch 48/100\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 2.3386 - accuracy: 0.7807 - val_loss: 3.3971 - val_accuracy: 0.6538\n",
      "Epoch 49/100\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 2.6235 - accuracy: 0.7811 - val_loss: 3.2920 - val_accuracy: 0.6474\n",
      "Epoch 50/100\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 2.1210 - accuracy: 0.7698 - val_loss: 3.0425 - val_accuracy: 0.6410\n",
      "Epoch 51/100\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 2.0057 - accuracy: 0.7935 - val_loss: 3.0170 - val_accuracy: 0.6474\n",
      "Epoch 52/100\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 1.6059 - accuracy: 0.7937 - val_loss: 3.1389 - val_accuracy: 0.6474\n",
      "Epoch 53/100\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 1.4895 - accuracy: 0.8112 - val_loss: 3.2149 - val_accuracy: 0.6474\n",
      "Epoch 54/100\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 1.6477 - accuracy: 0.8043 - val_loss: 3.2184 - val_accuracy: 0.6474\n",
      "Epoch 55/100\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 2.0885 - accuracy: 0.7792 - val_loss: 3.2204 - val_accuracy: 0.6474\n",
      "Epoch 56/100\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 1.6409 - accuracy: 0.8258 - val_loss: 3.2208 - val_accuracy: 0.6410\n",
      "Epoch 57/100\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 1.7601 - accuracy: 0.7749 - val_loss: 3.2180 - val_accuracy: 0.6410\n",
      "Epoch 58/100\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 1.4889 - accuracy: 0.8180 - val_loss: 3.2235 - val_accuracy: 0.6410\n",
      "Epoch 59/100\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 1.5547 - accuracy: 0.8109 - val_loss: 3.2224 - val_accuracy: 0.6474\n",
      "Epoch 60/100\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 1.5832 - accuracy: 0.7965 - val_loss: 2.8277 - val_accuracy: 0.6410\n",
      "Epoch 61/100\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 1.8481 - accuracy: 0.7895 - val_loss: 2.8897 - val_accuracy: 0.6410\n",
      "Epoch 62/100\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 1.8984 - accuracy: 0.7843 - val_loss: 2.8438 - val_accuracy: 0.6474\n",
      "Epoch 63/100\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 1.5863 - accuracy: 0.8063 - val_loss: 2.8569 - val_accuracy: 0.6603\n",
      "Epoch 64/100\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 1.6852 - accuracy: 0.8000 - val_loss: 2.9727 - val_accuracy: 0.6538\n",
      "Epoch 65/100\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 1.8178 - accuracy: 0.7923 - val_loss: 2.9844 - val_accuracy: 0.6603\n",
      "Epoch 66/100\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 1.6380 - accuracy: 0.7905 - val_loss: 2.9188 - val_accuracy: 0.6603\n",
      "Epoch 67/100\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 1.3022 - accuracy: 0.8201 - val_loss: 2.9972 - val_accuracy: 0.6603\n",
      "Epoch 68/100\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 1.7515 - accuracy: 0.7776 - val_loss: 2.9989 - val_accuracy: 0.6603\n",
      "Epoch 69/100\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 1.6020 - accuracy: 0.8117 - val_loss: 3.0648 - val_accuracy: 0.6603\n",
      "Epoch 70/100\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 1.7976 - accuracy: 0.8066 - val_loss: 3.1276 - val_accuracy: 0.6603\n",
      "Epoch 71/100\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 1.6463 - accuracy: 0.7903 - val_loss: 3.1446 - val_accuracy: 0.6603\n",
      "Epoch 72/100\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 2.2162 - accuracy: 0.7647 - val_loss: 3.1669 - val_accuracy: 0.6603\n",
      "Epoch 73/100\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 1.8384 - accuracy: 0.7910 - val_loss: 3.2094 - val_accuracy: 0.6667\n",
      "Epoch 74/100\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 2.0144 - accuracy: 0.7726 - val_loss: 3.2072 - val_accuracy: 0.6667\n",
      "Epoch 75/100\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 1.6798 - accuracy: 0.7913 - val_loss: 3.2715 - val_accuracy: 0.6667\n",
      "Epoch 76/100\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 1.5029 - accuracy: 0.8096 - val_loss: 3.2033 - val_accuracy: 0.6667\n",
      "Epoch 77/100\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 1.4653 - accuracy: 0.8106 - val_loss: 3.1911 - val_accuracy: 0.6667\n",
      "Epoch 78/100\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 1.4989 - accuracy: 0.7991 - val_loss: 3.1907 - val_accuracy: 0.6667\n",
      "Epoch 79/100\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 1.9050 - accuracy: 0.7889 - val_loss: 3.1912 - val_accuracy: 0.6667\n",
      "Epoch 80/100\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 1.5197 - accuracy: 0.8208 - val_loss: 3.1925 - val_accuracy: 0.6667\n",
      "Epoch 81/100\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 1.7131 - accuracy: 0.7990 - val_loss: 3.1222 - val_accuracy: 0.6667\n",
      "Epoch 82/100\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 1.7379 - accuracy: 0.8021 - val_loss: 3.0358 - val_accuracy: 0.6667\n",
      "Epoch 83/100\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 1.6708 - accuracy: 0.8175 - val_loss: 3.0374 - val_accuracy: 0.6667\n",
      "Epoch 84/100\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 1.9315 - accuracy: 0.7759 - val_loss: 3.0952 - val_accuracy: 0.6667\n",
      "Epoch 85/100\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 1.4106 - accuracy: 0.8091 - val_loss: 3.0898 - val_accuracy: 0.6667\n",
      "Epoch 86/100\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 1.8860 - accuracy: 0.8061 - val_loss: 3.0837 - val_accuracy: 0.6667\n",
      "Epoch 87/100\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 1.7278 - accuracy: 0.7873 - val_loss: 3.0924 - val_accuracy: 0.6667\n",
      "Epoch 88/100\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 1.8042 - accuracy: 0.8288 - val_loss: 3.0852 - val_accuracy: 0.6667\n",
      "Epoch 89/100\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 1.4890 - accuracy: 0.8124 - val_loss: 3.0782 - val_accuracy: 0.6603\n",
      "Epoch 90/100\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 1.6362 - accuracy: 0.8358 - val_loss: 3.0902 - val_accuracy: 0.6603\n",
      "Epoch 91/100\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 1.5739 - accuracy: 0.8198 - val_loss: 3.0858 - val_accuracy: 0.6603\n",
      "Epoch 92/100\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 1.4476 - accuracy: 0.8296 - val_loss: 3.0829 - val_accuracy: 0.6667\n",
      "Epoch 93/100\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 1.9888 - accuracy: 0.8118 - val_loss: 3.0753 - val_accuracy: 0.6603\n",
      "Epoch 94/100\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 1.4605 - accuracy: 0.8319 - val_loss: 3.0694 - val_accuracy: 0.6667\n",
      "Epoch 95/100\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 1.6760 - accuracy: 0.8045 - val_loss: 3.0696 - val_accuracy: 0.6667\n",
      "Epoch 96/100\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 1.5887 - accuracy: 0.8468 - val_loss: 3.0693 - val_accuracy: 0.6667\n",
      "Epoch 97/100\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 1.4566 - accuracy: 0.8395 - val_loss: 3.0009 - val_accuracy: 0.6667\n",
      "Epoch 98/100\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 1.5515 - accuracy: 0.8319 - val_loss: 3.0100 - val_accuracy: 0.6731\n",
      "Epoch 99/100\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 1.4407 - accuracy: 0.8563 - val_loss: 2.9181 - val_accuracy: 0.6731\n",
      "Epoch 100/100\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 1.8333 - accuracy: 0.8123 - val_loss: 2.9989 - val_accuracy: 0.6731\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x19c56108670>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2=Sequential()\n",
    "model2.add(Dense(12,input_dim=24,activation='relu'))\n",
    "model2.add(Dense(8,activation='relu'))\n",
    "model2.add(Dense(1,activation='relu'))\n",
    "model2.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "model2.fit(x,y,epochs=100, validation_split=0.3,batch_size=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17/17 [==============================] - 0s 2ms/step - loss: 2.0514 - accuracy: 0.7872\n",
      "accuracy: 78.72%\n"
     ]
    }
   ],
   "source": [
    "#model accuracy\n",
    "scores2=model2.evaluate(x,y)\n",
    "print(\"%s: %.2f%%\" % (model2.metrics_names[1], scores2[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# iteration 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "37/37 [==============================] - 2s 12ms/step - loss: 3.5796 - accuracy: 0.7629 - val_loss: 4.7912 - val_accuracy: 0.6667\n",
      "Epoch 2/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 3.7158 - accuracy: 0.7498 - val_loss: 4.7960 - val_accuracy: 0.6667\n",
      "Epoch 3/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 3.2119 - accuracy: 0.7781 - val_loss: 4.7985 - val_accuracy: 0.6667\n",
      "Epoch 4/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 3.9571 - accuracy: 0.7328 - val_loss: 4.7946 - val_accuracy: 0.6667\n",
      "Epoch 5/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 3.6990 - accuracy: 0.7487 - val_loss: 4.7983 - val_accuracy: 0.6667\n",
      "Epoch 6/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 3.8625 - accuracy: 0.7418 - val_loss: 4.7981 - val_accuracy: 0.6667\n",
      "Epoch 7/150\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 3.9519 - accuracy: 0.7416 - val_loss: 4.7934 - val_accuracy: 0.6667\n",
      "Epoch 8/150\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 3.2976 - accuracy: 0.7830 - val_loss: 4.7907 - val_accuracy: 0.6667\n",
      "Epoch 9/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 4.0530 - accuracy: 0.7317 - val_loss: 4.7933 - val_accuracy: 0.6667\n",
      "Epoch 10/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 3.6972 - accuracy: 0.7604 - val_loss: 4.7910 - val_accuracy: 0.6667\n",
      "Epoch 11/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 4.0271 - accuracy: 0.7390 - val_loss: 4.7887 - val_accuracy: 0.6667\n",
      "Epoch 12/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 3.8222 - accuracy: 0.7523 - val_loss: 4.7873 - val_accuracy: 0.6667\n",
      "Epoch 13/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 3.5296 - accuracy: 0.7712 - val_loss: 4.7862 - val_accuracy: 0.6667\n",
      "Epoch 14/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 3.9561 - accuracy: 0.7416 - val_loss: 4.7906 - val_accuracy: 0.6731\n",
      "Epoch 15/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 3.5904 - accuracy: 0.7599 - val_loss: 4.7100 - val_accuracy: 0.6538\n",
      "Epoch 16/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 3.4423 - accuracy: 0.7596 - val_loss: 4.7103 - val_accuracy: 0.6538\n",
      "Epoch 17/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 3.3855 - accuracy: 0.7649 - val_loss: 4.7099 - val_accuracy: 0.6538\n",
      "Epoch 18/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 3.7401 - accuracy: 0.7429 - val_loss: 4.7105 - val_accuracy: 0.6538\n",
      "Epoch 19/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 3.1565 - accuracy: 0.7722 - val_loss: 4.7135 - val_accuracy: 0.6538\n",
      "Epoch 20/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 3.5943 - accuracy: 0.7506 - val_loss: 4.7179 - val_accuracy: 0.6538\n",
      "Epoch 21/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 3.5732 - accuracy: 0.7635 - val_loss: 4.7214 - val_accuracy: 0.6538\n",
      "Epoch 22/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 3.6137 - accuracy: 0.7541 - val_loss: 4.7287 - val_accuracy: 0.6667\n",
      "Epoch 23/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 3.4644 - accuracy: 0.7561 - val_loss: 4.7196 - val_accuracy: 0.6667\n",
      "Epoch 24/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 3.9399 - accuracy: 0.7365 - val_loss: 4.7173 - val_accuracy: 0.6603\n",
      "Epoch 25/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 3.7215 - accuracy: 0.7370 - val_loss: 4.7173 - val_accuracy: 0.6603\n",
      "Epoch 26/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 2.4056 - accuracy: 0.8160 - val_loss: 4.7167 - val_accuracy: 0.6603\n",
      "Epoch 27/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 3.3271 - accuracy: 0.7779 - val_loss: 4.7152 - val_accuracy: 0.6603\n",
      "Epoch 28/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 3.7334 - accuracy: 0.7412 - val_loss: 4.7155 - val_accuracy: 0.6667\n",
      "Epoch 29/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 3.3757 - accuracy: 0.7674 - val_loss: 4.7144 - val_accuracy: 0.6603\n",
      "Epoch 30/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 3.6920 - accuracy: 0.7477 - val_loss: 4.7141 - val_accuracy: 0.6603\n",
      "Epoch 31/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 3.3563 - accuracy: 0.7632 - val_loss: 4.7132 - val_accuracy: 0.6603\n",
      "Epoch 32/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 3.4695 - accuracy: 0.7647 - val_loss: 4.7124 - val_accuracy: 0.6603\n",
      "Epoch 33/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 3.4658 - accuracy: 0.7671 - val_loss: 4.7133 - val_accuracy: 0.6603\n",
      "Epoch 34/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 3.8494 - accuracy: 0.7476 - val_loss: 4.7125 - val_accuracy: 0.6603\n",
      "Epoch 35/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 3.5882 - accuracy: 0.7633 - val_loss: 4.7125 - val_accuracy: 0.6603\n",
      "Epoch 36/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 3.4148 - accuracy: 0.7750 - val_loss: 4.7134 - val_accuracy: 0.6603\n",
      "Epoch 37/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 3.3828 - accuracy: 0.7782 - val_loss: 4.7137 - val_accuracy: 0.6603\n",
      "Epoch 38/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 3.4111 - accuracy: 0.7716 - val_loss: 4.7138 - val_accuracy: 0.6667\n",
      "Epoch 39/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 3.7884 - accuracy: 0.7482 - val_loss: 4.7114 - val_accuracy: 0.6603\n",
      "Epoch 40/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 3.6037 - accuracy: 0.7602 - val_loss: 4.7109 - val_accuracy: 0.6603\n",
      "Epoch 41/150\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 3.5452 - accuracy: 0.7665 - val_loss: 4.7104 - val_accuracy: 0.6603\n",
      "Epoch 42/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 3.7259 - accuracy: 0.7554 - val_loss: 4.7109 - val_accuracy: 0.6603\n",
      "Epoch 43/150\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 3.3770 - accuracy: 0.7654 - val_loss: 4.7114 - val_accuracy: 0.6667\n",
      "Epoch 44/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 3.8572 - accuracy: 0.7398 - val_loss: 4.7142 - val_accuracy: 0.6603\n",
      "Epoch 45/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 3.2930 - accuracy: 0.7779 - val_loss: 4.7161 - val_accuracy: 0.6603\n",
      "Epoch 46/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 3.4689 - accuracy: 0.7718 - val_loss: 4.6236 - val_accuracy: 0.6667\n",
      "Epoch 47/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 3.1496 - accuracy: 0.7906 - val_loss: 4.5313 - val_accuracy: 0.6859\n",
      "Epoch 48/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 2.6465 - accuracy: 0.7954 - val_loss: 3.9204 - val_accuracy: 0.6731\n",
      "Epoch 49/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 2.6882 - accuracy: 0.7558 - val_loss: 3.3173 - val_accuracy: 0.6731\n",
      "Epoch 50/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 2.7301 - accuracy: 0.7740 - val_loss: 3.2482 - val_accuracy: 0.6731\n",
      "Epoch 51/150\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 2.1094 - accuracy: 0.7825 - val_loss: 3.3098 - val_accuracy: 0.6731\n",
      "Epoch 52/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 2.1631 - accuracy: 0.8110 - val_loss: 3.2052 - val_accuracy: 0.6795\n",
      "Epoch 53/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 2.6319 - accuracy: 0.7863 - val_loss: 3.1223 - val_accuracy: 0.6795\n",
      "Epoch 54/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 2.0796 - accuracy: 0.8248 - val_loss: 3.1043 - val_accuracy: 0.6923\n",
      "Epoch 55/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 2.0042 - accuracy: 0.8308 - val_loss: 3.1145 - val_accuracy: 0.6795\n",
      "Epoch 56/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 2.5878 - accuracy: 0.7770 - val_loss: 3.0864 - val_accuracy: 0.6923\n",
      "Epoch 57/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 2.3795 - accuracy: 0.7677 - val_loss: 3.0348 - val_accuracy: 0.6795\n",
      "Epoch 58/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 2.3682 - accuracy: 0.7855 - val_loss: 3.0493 - val_accuracy: 0.6859\n",
      "Epoch 59/150\n",
      "37/37 [==============================] - ETA: 0s - loss: 1.7649 - accuracy: 0.90 - 0s 2ms/step - loss: 2.0704 - accuracy: 0.8127 - val_loss: 3.1755 - val_accuracy: 0.6859\n",
      "Epoch 60/150\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 2.0119 - accuracy: 0.8077 - val_loss: 3.1682 - val_accuracy: 0.6859\n",
      "Epoch 61/150\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 2.5766 - accuracy: 0.7765 - val_loss: 3.1032 - val_accuracy: 0.6859\n",
      "Epoch 62/150\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 2.0274 - accuracy: 0.8244 - val_loss: 3.0924 - val_accuracy: 0.6859\n",
      "Epoch 63/150\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 2.0841 - accuracy: 0.8170 - val_loss: 3.0773 - val_accuracy: 0.6987\n",
      "Epoch 64/150\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 1.7495 - accuracy: 0.8455 - val_loss: 3.2330 - val_accuracy: 0.6923\n",
      "Epoch 65/150\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 1.8962 - accuracy: 0.8315 - val_loss: 3.1666 - val_accuracy: 0.6923\n",
      "Epoch 66/150\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 1.9995 - accuracy: 0.8318 - val_loss: 3.2306 - val_accuracy: 0.6923\n",
      "Epoch 67/150\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 2.1278 - accuracy: 0.8247 - val_loss: 3.2297 - val_accuracy: 0.6923\n",
      "Epoch 68/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 2.4725 - accuracy: 0.7727 - val_loss: 3.2277 - val_accuracy: 0.6923\n",
      "Epoch 69/150\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 2.2449 - accuracy: 0.7766 - val_loss: 3.2250 - val_accuracy: 0.6987\n",
      "Epoch 70/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 2.7096 - accuracy: 0.7781 - val_loss: 3.1759 - val_accuracy: 0.6987\n",
      "Epoch 71/150\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 1.8560 - accuracy: 0.8232 - val_loss: 3.1639 - val_accuracy: 0.6987\n",
      "Epoch 72/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 1.9717 - accuracy: 0.8249 - val_loss: 3.0816 - val_accuracy: 0.7051\n",
      "Epoch 73/150\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 2.0455 - accuracy: 0.8345 - val_loss: 3.0779 - val_accuracy: 0.7051\n",
      "Epoch 74/150\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 1.7421 - accuracy: 0.8400 - val_loss: 3.0871 - val_accuracy: 0.7051\n",
      "Epoch 75/150\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 2.1687 - accuracy: 0.8126 - val_loss: 3.0788 - val_accuracy: 0.7051\n",
      "Epoch 76/150\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 2.0106 - accuracy: 0.8324 - val_loss: 3.0078 - val_accuracy: 0.7051\n",
      "Epoch 77/150\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 1.7490 - accuracy: 0.8429 - val_loss: 3.0803 - val_accuracy: 0.7244\n",
      "Epoch 78/150\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 1.7934 - accuracy: 0.8333 - val_loss: 2.9743 - val_accuracy: 0.7244\n",
      "Epoch 79/150\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 1.9114 - accuracy: 0.8321 - val_loss: 2.8382 - val_accuracy: 0.7244\n",
      "Epoch 80/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 1.9767 - accuracy: 0.8269 - val_loss: 2.9573 - val_accuracy: 0.7372\n",
      "Epoch 81/150\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 2.1883 - accuracy: 0.8287 - val_loss: 2.8982 - val_accuracy: 0.7372\n",
      "Epoch 82/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 2.2827 - accuracy: 0.8029 - val_loss: 2.8624 - val_accuracy: 0.7436\n",
      "Epoch 83/150\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 2.2769 - accuracy: 0.8283 - val_loss: 2.8581 - val_accuracy: 0.7436\n",
      "Epoch 84/150\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 2.0310 - accuracy: 0.8319 - val_loss: 2.7834 - val_accuracy: 0.7372\n",
      "Epoch 85/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 2.3039 - accuracy: 0.8128 - val_loss: 2.7816 - val_accuracy: 0.7372\n",
      "Epoch 86/150\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 2.2322 - accuracy: 0.8112 - val_loss: 2.7813 - val_accuracy: 0.7372\n",
      "Epoch 87/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 2.6381 - accuracy: 0.7794 - val_loss: 2.7813 - val_accuracy: 0.7372\n",
      "Epoch 88/150\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 1.8839 - accuracy: 0.8477 - val_loss: 2.7203 - val_accuracy: 0.7372\n",
      "Epoch 89/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 2.2739 - accuracy: 0.8087 - val_loss: 2.7114 - val_accuracy: 0.7372\n",
      "Epoch 90/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 2.0650 - accuracy: 0.8327 - val_loss: 2.7083 - val_accuracy: 0.7372\n",
      "Epoch 91/150\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 2.1491 - accuracy: 0.8060 - val_loss: 2.7058 - val_accuracy: 0.7372\n",
      "Epoch 92/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 2.2287 - accuracy: 0.8132 - val_loss: 2.7176 - val_accuracy: 0.7372\n",
      "Epoch 93/150\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 2.0079 - accuracy: 0.8218 - val_loss: 2.8042 - val_accuracy: 0.7308\n",
      "Epoch 94/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 2.2808 - accuracy: 0.8063 - val_loss: 2.8686 - val_accuracy: 0.7308\n",
      "Epoch 95/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 1.9776 - accuracy: 0.8096 - val_loss: 2.8674 - val_accuracy: 0.7308\n",
      "Epoch 96/150\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 1.8099 - accuracy: 0.8404 - val_loss: 2.8666 - val_accuracy: 0.7372\n",
      "Epoch 97/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 2.2819 - accuracy: 0.7966 - val_loss: 2.7272 - val_accuracy: 0.7372\n",
      "Epoch 98/150\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 1.9699 - accuracy: 0.8313 - val_loss: 2.7882 - val_accuracy: 0.7372\n",
      "Epoch 99/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 2.1667 - accuracy: 0.8409 - val_loss: 2.7909 - val_accuracy: 0.7308\n",
      "Epoch 100/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 2.0694 - accuracy: 0.8277 - val_loss: 2.7868 - val_accuracy: 0.7372\n",
      "Epoch 101/150\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 1.9015 - accuracy: 0.8476 - val_loss: 2.7879 - val_accuracy: 0.7372\n",
      "Epoch 102/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 1.9638 - accuracy: 0.8369 - val_loss: 2.7839 - val_accuracy: 0.7372\n",
      "Epoch 103/150\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 2.1020 - accuracy: 0.8151 - val_loss: 2.7867 - val_accuracy: 0.7372\n",
      "Epoch 104/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 2.0560 - accuracy: 0.8407 - val_loss: 2.7849 - val_accuracy: 0.7372\n",
      "Epoch 105/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 2.7284 - accuracy: 0.7942 - val_loss: 2.7851 - val_accuracy: 0.7372\n",
      "Epoch 106/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 2.1912 - accuracy: 0.8406 - val_loss: 2.7854 - val_accuracy: 0.7436\n",
      "Epoch 107/150\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 2.1805 - accuracy: 0.8235 - val_loss: 2.7858 - val_accuracy: 0.7436\n",
      "Epoch 108/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 2.3243 - accuracy: 0.7980 - val_loss: 2.7836 - val_accuracy: 0.7436\n",
      "Epoch 109/150\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 2.2816 - accuracy: 0.8145 - val_loss: 2.7851 - val_accuracy: 0.7436\n",
      "Epoch 110/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 2.1284 - accuracy: 0.8279 - val_loss: 2.7847 - val_accuracy: 0.7372\n",
      "Epoch 111/150\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 1.6004 - accuracy: 0.8653 - val_loss: 2.7939 - val_accuracy: 0.7308\n",
      "Epoch 112/150\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 2.1001 - accuracy: 0.8159 - val_loss: 2.7135 - val_accuracy: 0.7308\n",
      "Epoch 113/150\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 1.7516 - accuracy: 0.8394 - val_loss: 2.7108 - val_accuracy: 0.7308\n",
      "Epoch 114/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 1.7900 - accuracy: 0.8436 - val_loss: 2.7108 - val_accuracy: 0.7308\n",
      "Epoch 115/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 2.1785 - accuracy: 0.8152 - val_loss: 2.7094 - val_accuracy: 0.7308\n",
      "Epoch 116/150\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 1.8489 - accuracy: 0.8366 - val_loss: 2.7058 - val_accuracy: 0.7372\n",
      "Epoch 117/150\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 1.6409 - accuracy: 0.8666 - val_loss: 2.7073 - val_accuracy: 0.7372\n",
      "Epoch 118/150\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 2.1009 - accuracy: 0.8252 - val_loss: 2.7065 - val_accuracy: 0.7372\n",
      "Epoch 119/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 2.4897 - accuracy: 0.8083 - val_loss: 2.7062 - val_accuracy: 0.7372\n",
      "Epoch 120/150\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 1.9411 - accuracy: 0.8189 - val_loss: 2.7038 - val_accuracy: 0.7372\n",
      "Epoch 121/150\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 1.6599 - accuracy: 0.8458 - val_loss: 2.7049 - val_accuracy: 0.7372\n",
      "Epoch 122/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 2.2466 - accuracy: 0.7916 - val_loss: 2.7076 - val_accuracy: 0.7372\n",
      "Epoch 123/150\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 2.0497 - accuracy: 0.8435 - val_loss: 2.7103 - val_accuracy: 0.7372\n",
      "Epoch 124/150\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 2.0438 - accuracy: 0.8256 - val_loss: 2.7093 - val_accuracy: 0.7372\n",
      "Epoch 125/150\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 2.0384 - accuracy: 0.8204 - val_loss: 2.7042 - val_accuracy: 0.7372\n",
      "Epoch 126/150\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 2.3609 - accuracy: 0.7889 - val_loss: 2.7071 - val_accuracy: 0.7372\n",
      "Epoch 127/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 1.5757 - accuracy: 0.8443 - val_loss: 2.7071 - val_accuracy: 0.7372\n",
      "Epoch 128/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 2.5794 - accuracy: 0.7865 - val_loss: 2.7089 - val_accuracy: 0.7372\n",
      "Epoch 129/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 2.1410 - accuracy: 0.8217 - val_loss: 2.7085 - val_accuracy: 0.7372\n",
      "Epoch 130/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 1.5515 - accuracy: 0.8615 - val_loss: 2.7037 - val_accuracy: 0.7372\n",
      "Epoch 131/150\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 1.9221 - accuracy: 0.8304 - val_loss: 2.7055 - val_accuracy: 0.7372\n",
      "Epoch 132/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 1.6192 - accuracy: 0.8676 - val_loss: 2.7066 - val_accuracy: 0.7372\n",
      "Epoch 133/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 2.4254 - accuracy: 0.8067 - val_loss: 2.7057 - val_accuracy: 0.7372\n",
      "Epoch 134/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 1.6841 - accuracy: 0.8450 - val_loss: 2.7078 - val_accuracy: 0.7372\n",
      "Epoch 135/150\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 1.8803 - accuracy: 0.8207 - val_loss: 2.7100 - val_accuracy: 0.7372\n",
      "Epoch 136/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 1.8425 - accuracy: 0.8516 - val_loss: 2.7052 - val_accuracy: 0.7372\n",
      "Epoch 137/150\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 2.0373 - accuracy: 0.8341 - val_loss: 2.7085 - val_accuracy: 0.7372\n",
      "Epoch 138/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 1.9588 - accuracy: 0.8310 - val_loss: 2.7085 - val_accuracy: 0.7372\n",
      "Epoch 139/150\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 2.0197 - accuracy: 0.8405 - val_loss: 2.7079 - val_accuracy: 0.7372\n",
      "Epoch 140/150\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 1.7551 - accuracy: 0.8469 - val_loss: 2.7102 - val_accuracy: 0.7372\n",
      "Epoch 141/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 2.0633 - accuracy: 0.8111 - val_loss: 2.7105 - val_accuracy: 0.7372\n",
      "Epoch 142/150\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 1.9535 - accuracy: 0.8387 - val_loss: 2.7155 - val_accuracy: 0.7372\n",
      "Epoch 143/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 1.6030 - accuracy: 0.8552 - val_loss: 2.7132 - val_accuracy: 0.7372\n",
      "Epoch 144/150\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 1.8663 - accuracy: 0.8402 - val_loss: 2.7143 - val_accuracy: 0.7372\n",
      "Epoch 145/150\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 1.7483 - accuracy: 0.8329 - val_loss: 2.7587 - val_accuracy: 0.7372\n",
      "Epoch 146/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 1.6029 - accuracy: 0.8503 - val_loss: 2.7537 - val_accuracy: 0.7372\n",
      "Epoch 147/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 1.8987 - accuracy: 0.8361 - val_loss: 2.7592 - val_accuracy: 0.7372\n",
      "Epoch 148/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 2.2302 - accuracy: 0.8138 - val_loss: 2.7601 - val_accuracy: 0.7372\n",
      "Epoch 149/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 1.7534 - accuracy: 0.8607 - val_loss: 2.7563 - val_accuracy: 0.7372\n",
      "Epoch 150/150\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 1.9383 - accuracy: 0.8461 - val_loss: 2.7582 - val_accuracy: 0.7372\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x19c5657b940>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model3=Sequential()\n",
    "model3.add(Dense(12,input_dim=24,activation='relu'))\n",
    "model3.add(Dense(8,activation='relu'))\n",
    "model3.add(Dense(1,activation='relu'))\n",
    "model3.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "model3.fit(x,y,epochs=150, validation_split=0.3,batch_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17/17 [==============================] - 0s 1ms/step - loss: 2.1458 - accuracy: 0.8104\n",
      "accuracy: 81.04%\n"
     ]
    }
   ],
   "source": [
    "scores3 = model3.evaluate(x, y)\n",
    "print(\"%s: %.2f%%\" % (model3.metrics_names[1], scores3[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hence here we can analyse that the best of all iteration is first one where accuracy of the system came as 92.65%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# gas_turbines.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "#Plot Tools\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "#Model Building\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import sklearn\n",
    "import keras\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "from keras.models import Sequential\n",
    "from keras.layers import InputLayer,Dense\n",
    "import tensorflow as tf\n",
    "#Model Validation\n",
    "from sklearn.model_selection import cross_val_score, KFold, train_test_split\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AT</th>\n",
       "      <th>AP</th>\n",
       "      <th>AH</th>\n",
       "      <th>AFDP</th>\n",
       "      <th>GTEP</th>\n",
       "      <th>TIT</th>\n",
       "      <th>TAT</th>\n",
       "      <th>TEY</th>\n",
       "      <th>CDP</th>\n",
       "      <th>CO</th>\n",
       "      <th>NOX</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6.8594</td>\n",
       "      <td>1007.9</td>\n",
       "      <td>96.799</td>\n",
       "      <td>3.5000</td>\n",
       "      <td>19.663</td>\n",
       "      <td>1059.2</td>\n",
       "      <td>550.00</td>\n",
       "      <td>114.70</td>\n",
       "      <td>10.605</td>\n",
       "      <td>3.1547</td>\n",
       "      <td>82.722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6.7850</td>\n",
       "      <td>1008.4</td>\n",
       "      <td>97.118</td>\n",
       "      <td>3.4998</td>\n",
       "      <td>19.728</td>\n",
       "      <td>1059.3</td>\n",
       "      <td>550.00</td>\n",
       "      <td>114.72</td>\n",
       "      <td>10.598</td>\n",
       "      <td>3.2363</td>\n",
       "      <td>82.776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6.8977</td>\n",
       "      <td>1008.8</td>\n",
       "      <td>95.939</td>\n",
       "      <td>3.4824</td>\n",
       "      <td>19.779</td>\n",
       "      <td>1059.4</td>\n",
       "      <td>549.87</td>\n",
       "      <td>114.71</td>\n",
       "      <td>10.601</td>\n",
       "      <td>3.2012</td>\n",
       "      <td>82.468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7.0569</td>\n",
       "      <td>1009.2</td>\n",
       "      <td>95.249</td>\n",
       "      <td>3.4805</td>\n",
       "      <td>19.792</td>\n",
       "      <td>1059.6</td>\n",
       "      <td>549.99</td>\n",
       "      <td>114.72</td>\n",
       "      <td>10.606</td>\n",
       "      <td>3.1923</td>\n",
       "      <td>82.670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7.3978</td>\n",
       "      <td>1009.7</td>\n",
       "      <td>95.150</td>\n",
       "      <td>3.4976</td>\n",
       "      <td>19.765</td>\n",
       "      <td>1059.7</td>\n",
       "      <td>549.98</td>\n",
       "      <td>114.72</td>\n",
       "      <td>10.612</td>\n",
       "      <td>3.2484</td>\n",
       "      <td>82.311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15034</th>\n",
       "      <td>9.0301</td>\n",
       "      <td>1005.6</td>\n",
       "      <td>98.460</td>\n",
       "      <td>3.5421</td>\n",
       "      <td>19.164</td>\n",
       "      <td>1049.7</td>\n",
       "      <td>546.21</td>\n",
       "      <td>111.61</td>\n",
       "      <td>10.400</td>\n",
       "      <td>4.5186</td>\n",
       "      <td>79.559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15035</th>\n",
       "      <td>7.8879</td>\n",
       "      <td>1005.9</td>\n",
       "      <td>99.093</td>\n",
       "      <td>3.5059</td>\n",
       "      <td>19.414</td>\n",
       "      <td>1046.3</td>\n",
       "      <td>543.22</td>\n",
       "      <td>111.78</td>\n",
       "      <td>10.433</td>\n",
       "      <td>4.8470</td>\n",
       "      <td>79.917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15036</th>\n",
       "      <td>7.2647</td>\n",
       "      <td>1006.3</td>\n",
       "      <td>99.496</td>\n",
       "      <td>3.4770</td>\n",
       "      <td>19.530</td>\n",
       "      <td>1037.7</td>\n",
       "      <td>537.32</td>\n",
       "      <td>110.19</td>\n",
       "      <td>10.483</td>\n",
       "      <td>7.9632</td>\n",
       "      <td>90.912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15037</th>\n",
       "      <td>7.0060</td>\n",
       "      <td>1006.8</td>\n",
       "      <td>99.008</td>\n",
       "      <td>3.4486</td>\n",
       "      <td>19.377</td>\n",
       "      <td>1043.2</td>\n",
       "      <td>541.24</td>\n",
       "      <td>110.74</td>\n",
       "      <td>10.533</td>\n",
       "      <td>6.2494</td>\n",
       "      <td>93.227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15038</th>\n",
       "      <td>6.9279</td>\n",
       "      <td>1007.2</td>\n",
       "      <td>97.533</td>\n",
       "      <td>3.4275</td>\n",
       "      <td>19.306</td>\n",
       "      <td>1049.9</td>\n",
       "      <td>545.85</td>\n",
       "      <td>111.58</td>\n",
       "      <td>10.583</td>\n",
       "      <td>4.9816</td>\n",
       "      <td>92.498</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>15039 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           AT      AP      AH    AFDP    GTEP     TIT     TAT     TEY     CDP  \\\n",
       "0      6.8594  1007.9  96.799  3.5000  19.663  1059.2  550.00  114.70  10.605   \n",
       "1      6.7850  1008.4  97.118  3.4998  19.728  1059.3  550.00  114.72  10.598   \n",
       "2      6.8977  1008.8  95.939  3.4824  19.779  1059.4  549.87  114.71  10.601   \n",
       "3      7.0569  1009.2  95.249  3.4805  19.792  1059.6  549.99  114.72  10.606   \n",
       "4      7.3978  1009.7  95.150  3.4976  19.765  1059.7  549.98  114.72  10.612   \n",
       "...       ...     ...     ...     ...     ...     ...     ...     ...     ...   \n",
       "15034  9.0301  1005.6  98.460  3.5421  19.164  1049.7  546.21  111.61  10.400   \n",
       "15035  7.8879  1005.9  99.093  3.5059  19.414  1046.3  543.22  111.78  10.433   \n",
       "15036  7.2647  1006.3  99.496  3.4770  19.530  1037.7  537.32  110.19  10.483   \n",
       "15037  7.0060  1006.8  99.008  3.4486  19.377  1043.2  541.24  110.74  10.533   \n",
       "15038  6.9279  1007.2  97.533  3.4275  19.306  1049.9  545.85  111.58  10.583   \n",
       "\n",
       "           CO     NOX  \n",
       "0      3.1547  82.722  \n",
       "1      3.2363  82.776  \n",
       "2      3.2012  82.468  \n",
       "3      3.1923  82.670  \n",
       "4      3.2484  82.311  \n",
       "...       ...     ...  \n",
       "15034  4.5186  79.559  \n",
       "15035  4.8470  79.917  \n",
       "15036  7.9632  90.912  \n",
       "15037  6.2494  93.227  \n",
       "15038  4.9816  92.498  \n",
       "\n",
       "[15039 rows x 11 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data=pd.read_csv('gas_turbines.csv')\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 15039 entries, 0 to 15038\n",
      "Data columns (total 11 columns):\n",
      " #   Column  Non-Null Count  Dtype  \n",
      "---  ------  --------------  -----  \n",
      " 0   AT      15039 non-null  float64\n",
      " 1   AP      15039 non-null  float64\n",
      " 2   AH      15039 non-null  float64\n",
      " 3   AFDP    15039 non-null  float64\n",
      " 4   GTEP    15039 non-null  float64\n",
      " 5   TIT     15039 non-null  float64\n",
      " 6   TAT     15039 non-null  float64\n",
      " 7   TEY     15039 non-null  float64\n",
      " 8   CDP     15039 non-null  float64\n",
      " 9   CO      15039 non-null  float64\n",
      " 10  NOX     15039 non-null  float64\n",
      "dtypes: float64(11)\n",
      "memory usage: 1.3 MB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AT</th>\n",
       "      <th>AP</th>\n",
       "      <th>AH</th>\n",
       "      <th>AFDP</th>\n",
       "      <th>GTEP</th>\n",
       "      <th>TIT</th>\n",
       "      <th>TAT</th>\n",
       "      <th>TEY</th>\n",
       "      <th>CDP</th>\n",
       "      <th>CO</th>\n",
       "      <th>NOX</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>15039.000000</td>\n",
       "      <td>15039.00000</td>\n",
       "      <td>15039.000000</td>\n",
       "      <td>15039.000000</td>\n",
       "      <td>15039.000000</td>\n",
       "      <td>15039.000000</td>\n",
       "      <td>15039.000000</td>\n",
       "      <td>15039.000000</td>\n",
       "      <td>15039.000000</td>\n",
       "      <td>15039.000000</td>\n",
       "      <td>15039.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>17.764381</td>\n",
       "      <td>1013.19924</td>\n",
       "      <td>79.124174</td>\n",
       "      <td>4.200294</td>\n",
       "      <td>25.419061</td>\n",
       "      <td>1083.798770</td>\n",
       "      <td>545.396183</td>\n",
       "      <td>134.188464</td>\n",
       "      <td>12.102353</td>\n",
       "      <td>1.972499</td>\n",
       "      <td>68.190934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>7.574323</td>\n",
       "      <td>6.41076</td>\n",
       "      <td>13.793439</td>\n",
       "      <td>0.760197</td>\n",
       "      <td>4.173916</td>\n",
       "      <td>16.527806</td>\n",
       "      <td>7.866803</td>\n",
       "      <td>15.829717</td>\n",
       "      <td>1.103196</td>\n",
       "      <td>2.222206</td>\n",
       "      <td>10.470586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.522300</td>\n",
       "      <td>985.85000</td>\n",
       "      <td>30.344000</td>\n",
       "      <td>2.087400</td>\n",
       "      <td>17.878000</td>\n",
       "      <td>1000.800000</td>\n",
       "      <td>512.450000</td>\n",
       "      <td>100.170000</td>\n",
       "      <td>9.904400</td>\n",
       "      <td>0.000388</td>\n",
       "      <td>27.765000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>11.408000</td>\n",
       "      <td>1008.90000</td>\n",
       "      <td>69.750000</td>\n",
       "      <td>3.723900</td>\n",
       "      <td>23.294000</td>\n",
       "      <td>1079.600000</td>\n",
       "      <td>542.170000</td>\n",
       "      <td>127.985000</td>\n",
       "      <td>11.622000</td>\n",
       "      <td>0.858055</td>\n",
       "      <td>61.303500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>18.186000</td>\n",
       "      <td>1012.80000</td>\n",
       "      <td>82.266000</td>\n",
       "      <td>4.186200</td>\n",
       "      <td>25.082000</td>\n",
       "      <td>1088.700000</td>\n",
       "      <td>549.890000</td>\n",
       "      <td>133.780000</td>\n",
       "      <td>12.025000</td>\n",
       "      <td>1.390200</td>\n",
       "      <td>66.601000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>23.862500</td>\n",
       "      <td>1016.90000</td>\n",
       "      <td>90.043500</td>\n",
       "      <td>4.550900</td>\n",
       "      <td>27.184000</td>\n",
       "      <td>1096.000000</td>\n",
       "      <td>550.060000</td>\n",
       "      <td>140.895000</td>\n",
       "      <td>12.578000</td>\n",
       "      <td>2.160400</td>\n",
       "      <td>73.935500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>34.929000</td>\n",
       "      <td>1034.20000</td>\n",
       "      <td>100.200000</td>\n",
       "      <td>7.610600</td>\n",
       "      <td>37.402000</td>\n",
       "      <td>1100.800000</td>\n",
       "      <td>550.610000</td>\n",
       "      <td>174.610000</td>\n",
       "      <td>15.081000</td>\n",
       "      <td>44.103000</td>\n",
       "      <td>119.890000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 AT           AP            AH          AFDP          GTEP  \\\n",
       "count  15039.000000  15039.00000  15039.000000  15039.000000  15039.000000   \n",
       "mean      17.764381   1013.19924     79.124174      4.200294     25.419061   \n",
       "std        7.574323      6.41076     13.793439      0.760197      4.173916   \n",
       "min        0.522300    985.85000     30.344000      2.087400     17.878000   \n",
       "25%       11.408000   1008.90000     69.750000      3.723900     23.294000   \n",
       "50%       18.186000   1012.80000     82.266000      4.186200     25.082000   \n",
       "75%       23.862500   1016.90000     90.043500      4.550900     27.184000   \n",
       "max       34.929000   1034.20000    100.200000      7.610600     37.402000   \n",
       "\n",
       "                TIT           TAT           TEY           CDP            CO  \\\n",
       "count  15039.000000  15039.000000  15039.000000  15039.000000  15039.000000   \n",
       "mean    1083.798770    545.396183    134.188464     12.102353      1.972499   \n",
       "std       16.527806      7.866803     15.829717      1.103196      2.222206   \n",
       "min     1000.800000    512.450000    100.170000      9.904400      0.000388   \n",
       "25%     1079.600000    542.170000    127.985000     11.622000      0.858055   \n",
       "50%     1088.700000    549.890000    133.780000     12.025000      1.390200   \n",
       "75%     1096.000000    550.060000    140.895000     12.578000      2.160400   \n",
       "max     1100.800000    550.610000    174.610000     15.081000     44.103000   \n",
       "\n",
       "                NOX  \n",
       "count  15039.000000  \n",
       "mean      68.190934  \n",
       "std       10.470586  \n",
       "min       27.765000  \n",
       "25%       61.303500  \n",
       "50%       66.601000  \n",
       "75%       73.935500  \n",
       "max      119.890000  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.loc[:,['AT', 'AP', 'AH', 'AFDP', 'GTEP', 'TIT', 'TAT', 'CDP', 'CO','NOX']]\n",
    "y= data.loc[:,['TEY']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "y = scaler.fit_transform(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def baseline_model():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(10, input_dim=10, activation='tanh'))\n",
    "    model.add(Dense(1))\n",
    "    model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results: -0.21 (0.14) MSE\n"
     ]
    }
   ],
   "source": [
    "estimator = KerasRegressor(build_fn=baseline_model, nb_epoch=50, batch_size=100, verbose=False)\n",
    "kfold = KFold(n_splits=10)\n",
    "results = cross_val_score(estimator, X, y, cv=kfold)\n",
    "print(\"Results: %.2f (%.2f) MSE\" % (results.mean(), results.std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator.fit(X, y)\n",
    "prediction = estimator.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([122.48248, 122.25984, 122.1758 , ..., 116.89626, 118.73088,\n",
       "       119.73915], dtype=float32)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a=scaler.inverse_transform(prediction)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[114.7 ],\n",
       "       [114.72],\n",
       "       [114.71],\n",
       "       ...,\n",
       "       [110.19],\n",
       "       [110.74],\n",
       "       [111.58]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b=scaler.inverse_transform(y)\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13.727745526500795"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_squared_error(b,a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator.fit(X_train, y_train)\n",
    "prediction = estimator.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.49071008,  1.2907953 ,  0.15385179, ..., -1.4284554 ,\n",
       "        0.10786632,  1.0205607 ], dtype=float32)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "c=scaler.inverse_transform(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "d=scaler.inverse_transform(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25.04630708361525"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_squared_error(d,c)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
